{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/netapp2/Store_uni/home/usc/ci/avs/personal/aprendizaje/p4/venvP4/lib/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold, cross_val_predict\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n",
    "from sklearn.linear_model import ElasticNet, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, StackingRegressor, VotingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "from xgboost import XGBRegressor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "from xgboost import XGBRegressor\n",
    "import lightgbm as lgb\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import time\n",
    "import torch\n",
    "from torch import nn\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold, cross_val_predict\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n",
    "from sklearn.linear_model import ElasticNet, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, StackingRegressor, VotingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "from xgboost import XGBRegressor\n",
    "import lightgbm as lgb\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import warnings\n",
    "\n",
    "# Create results directory\n",
    "os.makedirs('results_stacking', exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "# PyTorch MLP model definition\n",
    "class MLPRegressorTorch(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(MLPRegressorTorch, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(0.25),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "\n",
    "# Function to load data\n",
    "def load_data(train_path, test_path):\n",
    "    \"\"\"\n",
    "    Load training and test data\n",
    "    \"\"\"\n",
    "    train_df = pd.read_csv(train_path)\n",
    "    test_df = pd.read_csv(test_path)\n",
    "    \n",
    "    # For training data\n",
    "    X_train = train_df.drop(['prezo_euros', 'id'], axis=1, errors='ignore')\n",
    "    y_train = train_df['prezo_euros']\n",
    "    \n",
    "    # For test data\n",
    "    if 'Unnamed: 0' in test_df.columns:\n",
    "        X_test = test_df.drop(['id', 'Unnamed: 0'], axis=1, errors='ignore')\n",
    "    else:\n",
    "        X_test = test_df.drop(['id'], axis=1, errors='ignore')\n",
    "    test_ids = test_df['id']\n",
    "    \n",
    "    print(f\"Training data shape: {X_train.shape}\")\n",
    "    print(f\"Test data shape: {X_test.shape}\")\n",
    "    return X_train, y_train, X_test, test_ids\n",
    "\n",
    "# Function to identify categorical features\n",
    "def get_categorical_features(df):\n",
    "    \"\"\"\n",
    "    Identify categorical features in the dataset\n",
    "    \"\"\"\n",
    "    categorical_features = []\n",
    "    for col in df.columns:\n",
    "        if (df[col].dtype == 'object' or\n",
    "            col in ['tipo_edificacion', 'calidade_materiais',\n",
    "                   'cor_favorita_propietario', 'acceso_transporte_publico',\n",
    "                   'orientacion', 'eficiencia_enerxetica'] or\n",
    "            'tipo_' in col or 'color_' in col):\n",
    "            categorical_features.append(col)\n",
    "    print(f\"Categorical features: {categorical_features}\")\n",
    "    return categorical_features\n",
    "\n",
    "# Function to load pretrained models\n",
    "def load_models(model_paths):\n",
    "    \"\"\"\n",
    "    Load pretrained models from specified paths\n",
    "    \"\"\"\n",
    "    models = {}\n",
    "    \n",
    "     # Load PyTorch MLP model\n",
    "    if 'torch_mlp' in model_paths and os.path.exists(model_paths['torch_mlp']):\n",
    "        print(\"Loading PyTorch MLP model...\")\n",
    "        # Load model state dictionary and scaler\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        scaler_path = model_paths['torch_mlp_scaler'] if 'torch_mlp_scaler' in model_paths else 'models/mlp_scaler.pkl'\n",
    "        \n",
    "        if os.path.exists(scaler_path):\n",
    "            scaler = joblib.load(scaler_path)\n",
    "            # We need to know the input dimension to create the model\n",
    "            input_dim = scaler.n_features_in_ if hasattr(scaler, 'n_features_in_') else scaler.feature_names_in_.shape[0]\n",
    "            \n",
    "            # Create model instance\n",
    "            torch_model = MLPRegressorTorch(input_dim).to(device)\n",
    "            torch_model.load_state_dict(torch.load(model_paths['torch_mlp'], map_location=device))\n",
    "            torch_model.eval()\n",
    "            \n",
    "            # Store both model and scaler together\n",
    "            models['torch_mlp'] = {\n",
    "                'model': torch_model,\n",
    "                'scaler': scaler,\n",
    "                'device': device\n",
    "            }\n",
    "        else:\n",
    "            print(f\"Warning: Scaler not found at {scaler_path}, PyTorch model will not be loaded\")\n",
    "            \n",
    "    # Load CatBoost model\n",
    "    if 'catboost' in model_paths and os.path.exists(model_paths['catboost']):\n",
    "        print(\"Loading CatBoost model...\")\n",
    "        models['catboost'] = CatBoostRegressor()\n",
    "        models['catboost'].load_model(model_paths['catboost'])\n",
    "    \n",
    "    # Load XGBoost model\n",
    "    if 'xgboost' in model_paths and os.path.exists(model_paths['xgboost']):\n",
    "        print(\"Loading XGBoost model...\")\n",
    "        models['xgboost'] = XGBRegressor()\n",
    "        models['xgboost'].load_model(model_paths['xgboost'])\n",
    "    \n",
    "    # Load LightGBM model\n",
    "    if 'lightgbm' in model_paths and os.path.exists(model_paths['lightgbm']):\n",
    "        print(\"Loading LightGBM model...\")\n",
    "        models['lightgbm'] = lgb.Booster(model_file=model_paths['lightgbm'])\n",
    "    \n",
    "    # Load MLP model (which includes the scaler)\n",
    "    if 'mlp' in model_paths and os.path.exists(model_paths['mlp']):\n",
    "        print(\"Loading MLP model...\")\n",
    "        models['mlp'] = joblib.load(model_paths['mlp'])\n",
    "    \n",
    "    \n",
    "\n",
    "    print(f\"Successfully loaded {len(models)} models\")\n",
    "    return models\n",
    "\n",
    "# Function to make predictions with loaded models\n",
    "def predict_with_models(models, X):\n",
    "    \"\"\"\n",
    "    Make predictions using all loaded models\n",
    "    \"\"\"\n",
    "    predictions = {}\n",
    "    \n",
    "    if 'catboost' in models:\n",
    "        print(\"Predicting with CatBoost...\")\n",
    "        predictions['catboost'] = models['catboost'].predict(X)\n",
    "    \n",
    "    if 'xgboost' in models:\n",
    "        print(\"Predicting with XGBoost...\")\n",
    "        predictions['xgboost'] = models['xgboost'].predict(X)\n",
    "    \n",
    "    if 'lightgbm' in models:\n",
    "        print(\"Predicting with LightGBM...\")\n",
    "        predictions['lightgbm'] = models['lightgbm'].predict(X)\n",
    "    \n",
    "    if 'mlp' in models:\n",
    "        print(\"Predicting with MLP...\")\n",
    "        # Extract model and scaler from the MLP model object\n",
    "        scaler = models['mlp']['scaler']\n",
    "        mlp_model = models['mlp']['model']\n",
    "        # Apply scaling before prediction\n",
    "        X_scaled = scaler.transform(X)\n",
    "        predictions['mlp'] = mlp_model.predict(X_scaled)\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# Function to generate level 1 features\n",
    "def generate_level1_features(models, X_train, X_test, y_train, categorical_features=None):\n",
    "    \"\"\"\n",
    "    Generate level 1 training features for meta-model using cross-validation\n",
    "    \"\"\"\n",
    "    print(\"Generating level 1 features for stacking...\")\n",
    "    \n",
    "    # For training set, use k-fold cross-validation to avoid data leakage\n",
    "    k = 10\n",
    "    train_preds_df = pd.DataFrame(index=range(X_train.shape[0]))\n",
    "    test_preds_all = {}\n",
    "    \n",
    "    # Create folds\n",
    "    n_samples = X_train.shape[0]\n",
    "    fold_size = n_samples // k\n",
    "    indices = np.arange(n_samples)\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    for fold in range(k):\n",
    "        print(f\"Processing fold {fold+1}/{k}\")\n",
    "        \n",
    "        # Get indices for this fold\n",
    "        start_idx = fold * fold_size\n",
    "        end_idx = (fold + 1) * fold_size if fold < k - 1 else n_samples\n",
    "        val_indices = indices[start_idx:end_idx]\n",
    "        train_indices = np.setdiff1d(indices, val_indices)\n",
    "        \n",
    "        # Split data for this fold\n",
    "        X_train_fold = X_train.iloc[train_indices]\n",
    "        y_train_fold = y_train.iloc[train_indices]\n",
    "        X_val_fold = X_train.iloc[val_indices]\n",
    "        \n",
    "        # Generate predictions for this fold\n",
    "        fold_preds = pd.DataFrame(index=val_indices)\n",
    "        test_fold_preds = {}\n",
    "        \n",
    "        # Train and predict with CatBoost\n",
    "        if 'catboost' in models:\n",
    "            print(\"Training fold with CatBoost...\")\n",
    "            if categorical_features:\n",
    "                train_pool = Pool(X_train_fold, y_train_fold, cat_features=categorical_features)\n",
    "                val_pool = Pool(X_val_fold, cat_features=categorical_features)\n",
    "                test_pool = Pool(X_test, cat_features=categorical_features)\n",
    "                \n",
    "                temp_model = CatBoostRegressor()\n",
    "                temp_model.set_params(**models['catboost'].get_params())\n",
    "                temp_model.fit(train_pool, verbose=False)\n",
    "                fold_preds['catboost'] = temp_model.predict(val_pool)\n",
    "                test_fold_preds['catboost'] = temp_model.predict(test_pool)\n",
    "            else:\n",
    "                temp_model = CatBoostRegressor()\n",
    "                temp_model.set_params(**models['catboost'].get_params())\n",
    "                temp_model.fit(X_train_fold, y_train_fold, verbose=False)\n",
    "                fold_preds['catboost'] = temp_model.predict(X_val_fold)\n",
    "                test_fold_preds['catboost'] = temp_model.predict(X_test)\n",
    "        \n",
    "        # Train and predict with XGBoost\n",
    "        if 'xgboost' in models:\n",
    "            print(\"Training fold with XGBoost...\")\n",
    "            temp_model = XGBRegressor()\n",
    "            temp_model.set_params(**models['xgboost'].get_params())\n",
    "            temp_model.fit(X_train_fold, y_train_fold, verbose=False)\n",
    "            fold_preds['xgboost'] = temp_model.predict(X_val_fold)\n",
    "            test_fold_preds['xgboost'] = temp_model.predict(X_test)\n",
    "        \n",
    "        # Train and predict with LightGBM\n",
    "        if 'lightgbm' in models:\n",
    "            print(\"Training fold with LightGBM...\")\n",
    "            train_lgb = lgb.Dataset(X_train_fold, label=y_train_fold)\n",
    "            params = models['lightgbm'].params if hasattr(models['lightgbm'], 'params') else {}\n",
    "            temp_model = lgb.train(params, train_lgb, num_boost_round=100)\n",
    "            fold_preds['lightgbm'] = temp_model.predict(X_val_fold)\n",
    "            test_fold_preds['lightgbm'] = temp_model.predict(X_test)\n",
    "        \n",
    "        # Train and predict with MLP\n",
    "        if 'mlp' in models:\n",
    "            print(\"Training fold with MLP...\")\n",
    "            # Extract parameters from loaded model\n",
    "            mlp_params = models['mlp']['model'].get_params()\n",
    "            \n",
    "            # Create and fit scaler\n",
    "            scaler = StandardScaler()\n",
    "            X_train_fold_scaled = scaler.fit_transform(X_train_fold)\n",
    "            X_val_fold_scaled = scaler.transform(X_val_fold)\n",
    "            X_test_scaled = scaler.transform(X_test)\n",
    "            \n",
    "            # Create and train MLP model\n",
    "            temp_model = MLPRegressor(**mlp_params)\n",
    "            temp_model.fit(X_train_fold_scaled, y_train_fold)\n",
    "            \n",
    "            fold_preds['mlp'] = temp_model.predict(X_val_fold_scaled)\n",
    "            test_fold_preds['mlp'] = temp_model.predict(X_test_scaled)\n",
    "\n",
    "        if 'torch_mlp' in models:\n",
    "            print(\"Training fold with PyTorch MLP...\")\n",
    "            # Get parameters from the loaded model\n",
    "            torch_mlp_info = models['torch_mlp']\n",
    "            device = torch_mlp_info['device']\n",
    "            \n",
    "            # Create and fit scaler\n",
    "            scaler = StandardScaler()\n",
    "            X_train_fold_scaled = scaler.fit_transform(X_train_fold)\n",
    "            X_val_fold_scaled = scaler.transform(X_val_fold)\n",
    "            X_test_scaled = scaler.transform(X_test)\n",
    "            \n",
    "            # Create a new model with the same architecture\n",
    "            input_dim = X_train_fold_scaled.shape[1]\n",
    "            temp_model = MLPRegressorTorch(input_dim).to(device)\n",
    "            \n",
    "            # Convert to PyTorch tensors\n",
    "            X_train_tensor = torch.tensor(X_train_fold_scaled, dtype=torch.float32).to(device)\n",
    "            y_train_tensor = torch.tensor(y_train_fold.values.reshape(-1, 1), dtype=torch.float32).to(device)\n",
    "            X_val_tensor = torch.tensor(X_val_fold_scaled, dtype=torch.float32).to(device)\n",
    "            X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32).to(device)\n",
    "            \n",
    "            # Train the PyTorch model\n",
    "            criterion = nn.L1Loss()\n",
    "            optimizer = torch.optim.Adam(temp_model.parameters(), lr=0.001)\n",
    "            batch_size = 64\n",
    "            epochs = 100\n",
    "            \n",
    "            for epoch in range(epochs):\n",
    "                # Create mini-batches\n",
    "                perm = torch.randperm(X_train_tensor.size(0))\n",
    "                for start in range(0, X_train_tensor.size(0), batch_size):\n",
    "                    batch_indices = perm[start:start + batch_size]\n",
    "                    batch_X = X_train_tensor[batch_indices]\n",
    "                    batch_y = y_train_tensor[batch_indices]\n",
    "                    \n",
    "                    # Forward and backward pass\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = temp_model(batch_X)\n",
    "                    loss = criterion(outputs, batch_y)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "            \n",
    "            # Get predictions\n",
    "            temp_model.eval()\n",
    "            with torch.no_grad():\n",
    "                fold_preds['torch_mlp'] = temp_model(X_val_tensor).cpu().numpy().flatten()\n",
    "                test_fold_preds['torch_mlp'] = temp_model(X_test_tensor).cpu().numpy().flatten()\n",
    "        \n",
    "        # Add fold predictions to overall predictions DataFrame\n",
    "        for model_name in fold_preds.columns:\n",
    "            train_preds_df.loc[val_indices, model_name] = fold_preds[model_name].values\n",
    "            \n",
    "            if model_name not in test_preds_all:\n",
    "                test_preds_all[model_name] = []\n",
    "            test_preds_all[model_name].append(test_fold_preds[model_name])\n",
    "    \n",
    "    # Create test predictions by averaging fold predictions\n",
    "    test_preds_df = pd.DataFrame()\n",
    "    for model_name, preds_list in test_preds_all.items():\n",
    "        test_preds_df[model_name] = np.mean(preds_list, axis=0)\n",
    "    \n",
    "    print(f\"Generated level 1 features with shape: {train_preds_df.shape} (train), {test_preds_df.shape} (test)\")\n",
    "    return train_preds_df, test_preds_df\n",
    "\n",
    "# Function to train meta-model\n",
    "def train_meta_model(level1_train, y_train, level1_test):\n",
    "    \"\"\"\n",
    "    Train meta-model on level 1 features\n",
    "    \"\"\"\n",
    "    print(\"Training meta-model...\")\n",
    "    \n",
    "    # Configure and train meta-model\n",
    "    meta_model = GradientBoostingRegressor(\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=3,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Split level 1 data for validation\n",
    "    X_train, X_val, y_train_split, y_val = train_test_split(\n",
    "        level1_train, y_train, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Train meta-model\n",
    "    meta_model.fit(X_train, y_train_split)\n",
    "    \n",
    "    # Evaluate on validation set\n",
    "    val_preds = meta_model.predict(X_val)\n",
    "    val_mae = mean_absolute_error(y_val, val_preds)\n",
    "    val_rmse = np.sqrt(mean_squared_error(y_val, val_preds))\n",
    "    val_r2 = r2_score(y_val, val_preds)\n",
    "    \n",
    "    print(f\"Meta-model validation metrics:\")\n",
    "    print(f\"  MAE: {val_mae:.2f}\")\n",
    "    print(f\"  RMSE: {val_rmse:.2f}\")\n",
    "    print(f\"  R²: {val_r2:.4f}\")\n",
    "    \n",
    "    # Retrain on full dataset\n",
    "    meta_model.fit(level1_train, y_train)\n",
    "    \n",
    "    # Generate predictions for test data\n",
    "    test_preds = meta_model.predict(level1_test)\n",
    "    \n",
    "    return meta_model, test_preds\n",
    "\n",
    "# Function to create submission file\n",
    "def create_submission(test_ids, predictions, output_file):\n",
    "    \"\"\"\n",
    "    Create submission file with predictions\n",
    "    \"\"\"\n",
    "    submission_df = pd.DataFrame({\n",
    "        'id': test_ids,\n",
    "        'prezo_euros': predictions\n",
    "    })\n",
    "    \n",
    "    submission_df.to_csv(output_file, index=False)\n",
    "    print(f\"Submission saved to {output_file}\")\n",
    "    return submission_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (20000, 45)\n",
      "Test data shape: (10000, 45)\n",
      "Categorical features: ['tipo_Apartamento', 'tipo_Casa', 'tipo_Chalet adosado', 'color_Amarelo', 'color_Azul', 'color_Branco', 'color_Negro', 'color_Verde', 'color_Vermello', 'tipo_Apartamento.1', 'tipo_Casa.1', 'tipo_Chalet adosado.1', 'color_Amarelo.1', 'color_Azul.1', 'color_Branco.1', 'color_Negro.1', 'color_Verde.1', 'color_Vermello.1']\n",
      "Loading PyTorch MLP model...\n",
      "Loading CatBoost model...\n",
      "Loading XGBoost model...\n",
      "Successfully loaded 3 models\n",
      "Generating level 1 features for stacking...\n",
      "Processing fold 1/10\n",
      "Training fold with CatBoost...\n",
      "Training fold with XGBoost...\n",
      "Training fold with PyTorch MLP...\n",
      "Processing fold 2/10\n",
      "Training fold with CatBoost...\n",
      "Training fold with XGBoost...\n",
      "Training fold with PyTorch MLP...\n",
      "Processing fold 3/10\n",
      "Training fold with CatBoost...\n",
      "Training fold with XGBoost...\n",
      "Training fold with PyTorch MLP...\n",
      "Processing fold 4/10\n",
      "Training fold with CatBoost...\n",
      "Training fold with XGBoost...\n",
      "Training fold with PyTorch MLP...\n",
      "Processing fold 5/10\n",
      "Training fold with CatBoost...\n",
      "Training fold with XGBoost...\n",
      "Training fold with PyTorch MLP...\n",
      "Processing fold 6/10\n",
      "Training fold with CatBoost...\n",
      "Training fold with XGBoost...\n",
      "Training fold with PyTorch MLP...\n",
      "Processing fold 7/10\n",
      "Training fold with CatBoost...\n",
      "Training fold with XGBoost...\n",
      "Training fold with PyTorch MLP...\n",
      "Processing fold 8/10\n",
      "Training fold with CatBoost...\n",
      "Training fold with XGBoost...\n",
      "Training fold with PyTorch MLP...\n",
      "Processing fold 9/10\n",
      "Training fold with CatBoost...\n",
      "Training fold with XGBoost...\n",
      "Training fold with PyTorch MLP...\n",
      "Processing fold 10/10\n",
      "Training fold with CatBoost...\n",
      "Training fold with XGBoost...\n",
      "Training fold with PyTorch MLP...\n",
      "Generated level 1 features with shape: (20000, 3) (train), (10000, 3) (test)\n",
      "Training meta-model...\n",
      "Meta-model validation metrics:\n",
      "  MAE: 29662.62\n",
      "  RMSE: 41571.06\n",
      "  R²: 0.9381\n",
      "Submission saved to submissions_final_stacking_new.csv\n",
      "Meta-model saved to results_stacking/meta_model.pkl\n",
      "Total execution time: 1099.53 seconds\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Load data\n",
    "train_path = 'train_processed.csv'\n",
    "test_path = 'test_processed.csv'\n",
    "X_train, y_train, X_test, test_ids = load_data(train_path, test_path)\n",
    "\n",
    "# Get categorical features\n",
    "categorical_features = get_categorical_features(X_train)\n",
    "\n",
    "# Define paths to pretrained models\n",
    "model_paths = {\n",
    "    'torch_mlp': 'models_stacking/mlp_torch_model.pt',\n",
    "    'catboost': 'models_stacking/stacking_catboost_model.cbm',\n",
    "    'xgboost': 'models_stacking/stacking_xgboost_model.json',\n",
    "    #'lightgbm': 'models_stacking/stacking_lightgbm_model.txt',\n",
    "    #'mlp': 'models_stacking/stacking_mlp_model.pkl'\n",
    "}\n",
    "\n",
    "# Load models\n",
    "models = load_models(model_paths)\n",
    "\n",
    "# Generate level 1 features\n",
    "level1_train, level1_test = generate_level1_features(\n",
    "    models, X_train, X_test, y_train, categorical_features\n",
    ")\n",
    "\n",
    "# Train meta-model\n",
    "meta_model, test_preds = train_meta_model(level1_train, y_train, level1_test)\n",
    "\n",
    "# Create submission file\n",
    "create_submission(test_ids, test_preds, 'submissions_final_stacking_new.csv')\n",
    "\n",
    "# Save meta-model\n",
    "joblib.dump(meta_model, 'results_stacking/meta_model.pkl')\n",
    "print(\"Meta-model saved to results_stacking/meta_model.pkl\")\n",
    "\n",
    "# End timing\n",
    "end_time = time.time()\n",
    "print(f\"Total execution time: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0523ecfdfd03da9535a2cd394fa2b2a2368df119d71b1e2a5e4a2b8711053260"
  },
  "kernelspec": {
   "display_name": "Python 3.7.8 64-bit ('venvP4': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
