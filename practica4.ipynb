{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORTS Y CONFIGURACIÓN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, OrdinalEncoder\n",
    "from sklearn.impute import KNNImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n",
    "import os\n",
    "from scipy import stats\n",
    "import missingno as msno  # Para visualizar datos faltantes\n",
    "import geopandas as gpd  # Para análisis geoespacial\n",
    "import time\n",
    "from xgboost import XGBRegressor\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "import lightgbm as lgb\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import joblib\n",
    "\n",
    "\n",
    "\n",
    "# Definir una semilla para reproducibilidad\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# Configuraciones para visualización\n",
    "#plt.style.use('seaborn-v0_8-whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "sns.set_palette(\"viridis\")\n",
    "plt.rcParams['axes.titlesize'] = 16\n",
    "plt.rcParams['axes.labelsize'] = 14\n",
    "plt.rcParams['xtick.labelsize'] = 12\n",
    "plt.rcParams['ytick.labelsize'] = 12\n",
    "\n",
    "# Configurar opciones de pandas\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.float_format', '{:.2f}'.format)\n",
    "pd.set_option('display.width', 200)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ANÁLISIS DE CARACTERÍSTICAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 1.1 CARGA DE DATOS\n",
    "# =============================================================================\n",
    "\n",
    "# Cargar los conjuntos de datos\n",
    "train_data = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('test.csv')\n",
    "\n",
    "print(f\"Dimensiones del conjunto de entrenamiento: {train_data.shape}\")\n",
    "print(f\"Dimensiones del conjunto de prueba: {test_data.shape}\")\n",
    "\n",
    "# Verificar las primeras filas del conjunto de entrenamiento\n",
    "print(\"\\nPrimeras filas del conjunto de entrenamiento:\")\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 2.1 EXPLORACIÓN INICIAL | NULLS y TIPOS\n",
    "# =============================================================================\n",
    "\n",
    "# Información general sobre los tipos de datos\n",
    "print(\"\\nTipos de datos en el conjunto de entrenamiento:\")\n",
    "train_data.info()\n",
    "\n",
    "# Estadísticas descriptivas para variables numéricas\n",
    "print(\"\\nEstadísticas descriptivas para variables numéricas:\")\n",
    "train_data.describe()\n",
    "\n",
    "# Estadísticas descriptivas para variables categóricas\n",
    "print(\"\\nEstadísticas descriptivas para variables categóricas:\")\n",
    "train_data.describe(include=['object'])\n",
    "\n",
    "display(train_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 2.2 VERIFICACIÓN DE DUPLICADOS\n",
    "# =============================================================================\n",
    "\n",
    "# Verificar filas duplicadas en el conjunto de entrenamiento\n",
    "duplicados_train = train_data.duplicated().sum()\n",
    "print(f\"\\nCantidad de filas duplicadas en el conjunto de entrenamiento: {duplicados_train}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 3. ANÁLISIS DE VALORES FALTANTES\n",
    "# =============================================================================\n",
    "\n",
    "# Calcular la cantidad de valores faltantes por columna\n",
    "missing_values = train_data.isnull().sum().sort_values(ascending=False)\n",
    "missing_percent = (train_data.isnull().sum() / train_data.shape[0] * 100).sort_values(ascending=False)\n",
    "\n",
    "missing_df = pd.DataFrame({'Missing Values': missing_values, \n",
    "                          'Percent Missing': missing_percent})\n",
    "print(\"\\nAnálisis de valores faltantes:\")\n",
    "print(missing_df[missing_df['Missing Values'] > 0])\n",
    "\n",
    "df = train_data\n",
    "# Cálculo de nulos y tipos\n",
    "null_counts = df.isnull().sum()\n",
    "dtypes = df.dtypes\n",
    "\n",
    "# Preparar la figura\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "bars = ax.barh(null_counts.index, null_counts.values, edgecolor='black')\n",
    "\n",
    "# Anotar el tipo de dato junto a cada barra\n",
    "for bar, dtype in zip(bars, dtypes):\n",
    "    ax.text(bar.get_width() + max(null_counts.values)*0.01,  # un poquito a la derecha\n",
    "            bar.get_y() + bar.get_height() / 2,\n",
    "            str(dtype),\n",
    "            va='center', fontsize= 9)\n",
    "\n",
    "# Etiquetas y estilo\n",
    "ax.set_xlabel('Número de valores nulos')\n",
    "ax.set_title('Valores nulos y tipo de dato por columna')\n",
    "ax.invert_yaxis()  # Opcional: para que la primera columna quede arriba\n",
    "plt.tight_layout()\n",
    "\n",
    "# Guardar o mostrar\n",
    "plt.savefig('figura_nulos_tipos.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 4. ANÁLISIS DE LA VARIABLE OBJETIVO\n",
    "# =============================================================================\n",
    "\n",
    "# Distribución del precio\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.histplot(train_data['prezo_euros'], kde=True)\n",
    "plt.title('Distribución del Precio de las Viviendas', fontsize=16)\n",
    "plt.xlabel('Precio (¬)', fontsize=14)\n",
    "plt.ylabel('Frecuencia', fontsize=14)\n",
    "plt.tight_layout()\n",
    "#plt.savefig('precio_distribucion.png')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Estadísticas básicas del precio\n",
    "precio_stats = train_data['prezo_euros'].describe()\n",
    "print(\"\\nEstadísticas de la variable objetivo (prezo_euros):\")\n",
    "print(precio_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 5. ANÁLISIS DE VARIABLES NUMÉRICAS\n",
    "# =============================================================================\n",
    "\n",
    "# Identificar variables numéricas (excluyendo el ID y la variable objetivo)\n",
    "numeric_features = train_data.select_dtypes(include=['int64', 'float64']).columns\n",
    "numeric_features = [f for f in numeric_features if f not in ['id', 'prezo_euros']]\n",
    "\n",
    "# Histogramas para variables numéricas\n",
    "plt.figure(figsize=(18, 15))\n",
    "for i, feature in enumerate(numeric_features, 1):\n",
    "    plt.subplot(4, 4, i)\n",
    "    sns.histplot(train_data[feature], kde=True)\n",
    "    plt.title(f'Distribución de {feature}')\n",
    "    plt.tight_layout()\n",
    "#plt.savefig('numeric_features_distribution.png')\n",
    "plt.show()\n",
    "\n",
    "# Columnas a analizar\n",
    "cols = [\n",
    "    'distancia_centro_km',\n",
    "    'distancia_escola_km',\n",
    "    'superficie_interior_m2',\n",
    "    'superficie_exterior_m2'\n",
    "]\n",
    "\n",
    "# Cálculo del número de filas con valor 0 en cada columna\n",
    "zero_counts = train_data[cols].apply(lambda s: (s == 0).sum())\n",
    "\n",
    "# Mostrar resultados\n",
    "for col, count in zero_counts.items():\n",
    "    print(f\"{col}: {count} filas con valor 0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Definir umbrales por columna\n",
    "thresholds = {\n",
    "    'distancia_centro_km': 0.5,\n",
    "    'distancia_escola_km': 0.5,\n",
    "    'superficie_interior_m2': 50,\n",
    "    'superficie_exterior_m2': 50\n",
    "}\n",
    "\n",
    "# Crear figura con subplots 2x2\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for ax, (col, umbral) in zip(axes, thresholds.items()):\n",
    "    # Tomar datos, eliminar nulls y valores por encima del umbral\n",
    "    data = train_data[col].dropna()\n",
    "    data = data[data <= umbral]\n",
    "    \n",
    "    # Histograma\n",
    "    sns.histplot(data, kde=True, ax=ax)\n",
    "    ax.set_title(f'{col} - {umbral}', fontsize=14)\n",
    "    ax.set_xlabel(col, fontsize=12)\n",
    "    ax.set_ylabel('Frecuencia', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "# plt.savefig('numeric_features_thresholded.png', dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Boxplots para variables numéricas\n",
    "plt.figure(figsize=(18, 15))\n",
    "for i, feature in enumerate(numeric_features, 1):\n",
    "    plt.subplot(4, 4, i)\n",
    "    sns.boxplot(y=train_data[feature])\n",
    "    plt.title(f'Boxplot de {feature}')\n",
    "    plt.tight_layout()\n",
    "#plt.savefig('numeric_features_boxplot.png')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scatter plots con la variable objetivo\n",
    "plt.figure(figsize=(18, 15))\n",
    "for i, feature in enumerate(numeric_features, 1):\n",
    "    plt.subplot(4, 4, i)\n",
    "    sns.scatterplot(x=train_data[feature], y=train_data['prezo_euros'])\n",
    "    plt.title(f'{feature} vs Precio')\n",
    "    plt.tight_layout()\n",
    "#plt.savefig('numeric_features_vs_precio.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 6. ANÁLISIS DE VARIABLES CATEGÓRICAS\n",
    "# =============================================================================\n",
    "\n",
    "# Identificar variables categóricas\n",
    "categorical_features = train_data.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Bar plots para variables categóricas\n",
    "plt.figure(figsize=(18, 15))\n",
    "for i, feature in enumerate(categorical_features, 1):\n",
    "    plt.subplot(3, 2, i)\n",
    "    train_data[feature].value_counts().plot(kind='bar')\n",
    "    plt.title(f'Distribución de {feature}')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "#plt.savefig('categorical_features_distribution.png')\n",
    "plt.show()\n",
    "\n",
    "# Boxplots de precio por categoría\n",
    "plt.figure(figsize=(18, 15))\n",
    "for i, feature in enumerate(categorical_features, 1):\n",
    "    plt.subplot(3, 2, i)\n",
    "    sns.boxplot(x=train_data[feature], y=train_data['prezo_euros'])\n",
    "    plt.title(f'Precio por {feature}')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "#plt.savefig('precio_vs_categorical_features.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 7.1 MATRIZ DE CORRELACIÓN VARIABLES NUMERICAS\n",
    "# =============================================================================\n",
    "\n",
    "# Calcular matriz de correlación para variables numéricas\n",
    "correlation_matrix = train_data[numeric_features + ['prezo_euros']].corr()\n",
    "\n",
    "# Visualizar matriz de correlación\n",
    "plt.figure(figsize=(14, 10))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt='.2f', linewidths=0.5)\n",
    "plt.title('Matriz de Correlación de Variables Numéricas', fontsize=16)\n",
    "plt.tight_layout()\n",
    "#plt.savefig('correlation_matrix.png')\n",
    "plt.show()\n",
    "\n",
    "# Top correlaciones con la variable objetivo\n",
    "precio_correlations = correlation_matrix['prezo_euros'].sort_values(ascending=False)\n",
    "print(\"\\nCorrelaciones con la variable objetivo (prezo_euros):\")\n",
    "print(precio_correlations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 7.2 CORRELACIÓN ENTRE VARIABLES CATEGÓRICAS Y PRECIO (·² - Eta Squared)\n",
    "# =============================================================================\n",
    "\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "# Función para calcular eta squared (proporción de varianza explicada)\n",
    "def calcular_eta_squared(cat_col, target_col='prezo_euros'):\n",
    "    modelo = ols(f\"{target_col} ~ C({cat_col})\", data=train_data).fit()\n",
    "    anova_tabla = sm.stats.anova_lm(modelo, typ=2)\n",
    "    eta_sq = anova_tabla['sum_sq'][0] / anova_tabla['sum_sq'].sum()\n",
    "    return eta_sq\n",
    "\n",
    "# Calcular ·² para cada variable categórica\n",
    "eta_squared_resultados = {}\n",
    "\n",
    "for col in categorical_features:\n",
    "    try:\n",
    "        eta_squared_resultados[col] = calcular_eta_squared(col)\n",
    "    except:\n",
    "        eta_squared_resultados[col] = np.nan\n",
    "\n",
    "# Convertir resultados a DataFrame ordenado\n",
    "eta_df = pd.DataFrame.from_dict(eta_squared_resultados, orient='index', columns=['Eta Squared'])\n",
    "eta_df.sort_values(by='Eta Squared', ascending=False, inplace=True)\n",
    "\n",
    "# Visualización\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=eta_df['Eta Squared'], y=eta_df.index, palette='viridis')\n",
    "plt.title(\"Correlación de Variables Categóricas con el Precio (eta squared)\", fontsize=16)\n",
    "plt.xlabel(\"Eta Squared (proporción de varianza explicada)\")\n",
    "plt.ylabel(\"Variable categórica\")\n",
    "plt.tight_layout()\n",
    "# plt.savefig('categorical_eta_squared_vs_precio.png', dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# Mostrar valores numéricos\n",
    "print(\"\\nCorrelación de variables categóricas con 'prezo_euros' (eta squared):\")\n",
    "print(eta_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 8. ANÁLISIS GEOESPACIAL\n",
    "# =============================================================================\n",
    "\n",
    "# Visualizar distribución geográfica de las viviendas\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.scatterplot(x='lonxitude', y='latitude', hue='prezo_euros', \n",
    "                size='superficie_interior_m2', data=train_data,\n",
    "                palette='viridis', sizes=(20, 200), alpha=0.6)\n",
    "plt.title('Distribución Geográfica de Viviendas', fontsize=16)\n",
    "plt.xlabel('Longitud', fontsize=14)\n",
    "plt.ylabel('Latitud', fontsize=14)\n",
    "plt.legend(title='Precio (¬)', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "#plt.savefig('distribucion_geografica.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 10. CREACIÓN DE CARACTERÍSTICAS ADICIONALES\n",
    "# =============================================================================\n",
    "\n",
    "# Añadir edad de la vivienda (año actual - año construcción)\n",
    "current_year = 2025  # Año actual del problema\n",
    "train_data['edad_vivienda'] = current_year - train_data['ano_construccion']\n",
    "\n",
    "# Ratio superficie por habitación\n",
    "train_data['superficie_por_habitacion'] = train_data['superficie_interior_m2'] / train_data['numero_habitacions']\n",
    "\n",
    "# Relación entre edad de la vivienda y precio\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.scatterplot(x='edad_vivienda', y='prezo_euros', \n",
    "                hue='calidade_materiais', data=train_data, alpha=0.7)\n",
    "plt.title('Relación entre Edad de la Vivienda y Precio', fontsize=16)\n",
    "plt.xlabel('Edad de la Vivienda (años)', fontsize=14)\n",
    "plt.ylabel('Precio (¬)', fontsize=14)\n",
    "plt.legend(title='Calidad de Materiales', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "#plt.savefig('edad_vs_precio.png')\n",
    "plt.show()\n",
    "\n",
    "# Relación entre superficie por habitación y precio\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.scatterplot(x='superficie_por_habitacion', y='prezo_euros', \n",
    "                hue='tipo_edificacion', data=train_data, alpha=0.7)\n",
    "plt.title('Relación entre Superficie por Habitación y Precio', fontsize=16)\n",
    "plt.xlabel('Superficie por Habitación (m²)', fontsize=14)\n",
    "plt.ylabel('Precio (¬)', fontsize=14)\n",
    "plt.legend(title='Tipo de Edificación', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "#plt.savefig('superficie_por_habitacion_vs_precio.png')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 11. ANÁLISIS DE VALORES ATÍPICOS (OUTLIERS)\n",
    "# =============================================================================\n",
    "\n",
    "# Detección de outliers en la variable objetivo\n",
    "Q1 = train_data['prezo_euros'].quantile(0.25)\n",
    "Q3 = train_data['prezo_euros'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "\n",
    "outliers = train_data[(train_data['prezo_euros'] < lower_bound) | \n",
    "                      (train_data['prezo_euros'] > upper_bound)]\n",
    "\n",
    "print(f\"\\nNúmero de posibles outliers en precio: {len(outliers)}\")\n",
    "print(f\"Porcentaje de outliers: {len(outliers) / len(train_data) * 100:.2f}%\")\n",
    "\n",
    "# Visualización de outliers en el precio\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.boxplot(x=train_data['prezo_euros'])\n",
    "plt.title('Detección de Outliers en Precio', fontsize=16)\n",
    "plt.xlabel('Precio (¬)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "#plt.savefig('precio_outliers.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 12. CONCLUSIONES Y RESUMEN\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\n=== RESUMEN DEL ANÁLISIS EXPLORATORIO DE DATOS ===\")\n",
    "print(f\"1. Tamaño del conjunto de entrenamiento: {train_data.shape[0]} filas, {train_data.shape[1]} columnas\")\n",
    "print(f\"2. Valores faltantes: {missing_df[missing_df['Missing Values'] > 0].shape[0]} columnas con valores faltantes\")\n",
    "print(f\"3. Rango de precios: {train_data['prezo_euros'].min():.2f}¬ - {train_data['prezo_euros'].max():.2f}¬\")\n",
    "print(f\"4. Precio medio: {train_data['prezo_euros'].mean():.2f}¬\")\n",
    "print(f\"5. Variables con mayor correlación con el precio:\")\n",
    "for feature, corr in precio_correlations[:5].items():\n",
    "    if feature != 'prezo_euros':\n",
    "        print(f\"   - {feature}: {corr:.4f}\")\n",
    "\n",
    "# Posibles pasos a seguir basados en el análisis\n",
    "print(\"\\nPasos a seguir basados en el análisis exploratorio:\")\n",
    "print(\"1. Imputar valores faltantes para superficie_interior_m2, superficie_exterior_m2 y distancia_centro_km\")\n",
    "print(\"2. Codificar variables categóricas (tipo_edificacion, calidade_materiais, etc.)\")\n",
    "print(\"3. Crear características adicionales (ratios, interacciones, features geoespaciales)\")\n",
    "print(\"4. Considerar transformación logarítmica de la variable objetivo para normalizar su distribución\")\n",
    "print(\"5. Evaluar el impacto de outliers en el modelado\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PREPROCESADO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Procesando datos desde cero...\n",
      "Dimensiones del conjunto de entrenamiento: (20000, 20)\n",
      "Dimensiones del conjunto de prueba: (10000, 20)\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 1. CARGA DE DATOS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"Procesando datos desde cero...\")\n",
    "    \n",
    "# Cargar los conjuntos de datos originales\n",
    "train_data = pd.read_csv('train.csv')\n",
    "test_data = pd.read_csv('test.csv')\n",
    "\n",
    "print(f\"Dimensiones del conjunto de entrenamiento: {train_data.shape}\")\n",
    "print(f\"Dimensiones del conjunto de prueba: {test_data.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 2. FUNCIÓN AUXILIAR PARA PREPROCESAMIENTO\n",
    "# =============================================================================\n",
    "\n",
    "def preprocess_data(df, is_train=True):\n",
    "\n",
    "    # Crear una copia para no modificar el original\n",
    "    data = df.copy()\n",
    "    \n",
    "    # =============================================================================\n",
    "    # 2.1 LIMPIEZA DE DATOS\n",
    "    # =============================================================================\n",
    "    \n",
    "    print(\"Iniciando limpieza de datos...\")\n",
    "    \n",
    "    # Eliminar duplicados en el conjunto de entrenamiento\n",
    "    if is_train:\n",
    "        duplicates = data.duplicated()\n",
    "        if duplicates.sum() > 0:\n",
    "            print(f\"Eliminando {duplicates.sum()} filas duplicadas...\")\n",
    "            data = data.drop_duplicates().reset_index(drop=True)\n",
    "        \n",
    "        \"\"\"\n",
    "        # PEQUEÑA PRUEBA: Filtrar registros con precio de 50000 euros\n",
    "        price_50k_count = (data['prezo_euros'] == 50000).sum()\n",
    "        if price_50k_count > 0:\n",
    "            print(f\"Eliminando {price_50k_count} registros con precio exacto de 50000 euros...\")\n",
    "            data = data[data['prezo_euros'] != 50000].reset_index(drop=True)\n",
    "            print(f\"Registros restantes después del filtrado: {len(data)}\")\n",
    "        \"\"\"\n",
    "\n",
    "    \n",
    "    # =============================================================================\n",
    "    # 2.2 MANEJO DE VALORES FALTANTES\n",
    "    # =============================================================================\n",
    "    \n",
    "    print(\"Manejando valores faltantes...\")\n",
    "    \n",
    "    # Chequear valores faltantes antes del preprocesado\n",
    "    missing_before = data.isnull().sum()\n",
    "    print(\"Valores faltantes antes del preprocesado:\")\n",
    "    print(missing_before[missing_before > 0])\n",
    "    \n",
    "    # Imputar valores faltantes usando KNN para variables numéricas principales\n",
    "    numeric_features = [\n",
    "        'superficie_interior_m2', \n",
    "        'superficie_exterior_m2', \n",
    "        'distancia_centro_km', \n",
    "        'distancia_escola_km', \n",
    "        'indice_criminalidade'\n",
    "    ]\n",
    "    \n",
    "    # Seleccionar columnas auxiliares para KNN (que tengan pocos o ningún NA)\n",
    "    aux_features = [\n",
    "        'numero_habitacions', \n",
    "        'numero_banos', \n",
    "        'ano_construccion',\n",
    "        'lonxitude', \n",
    "        'latitude', \n",
    "        'temperatura_media_mes_construccion',\n",
    "        'numero_arboles_xardin'\n",
    "    ]\n",
    "    \n",
    "    # Combinar características para imputer\n",
    "    imputer_features = numeric_features + aux_features\n",
    "    \n",
    "    # Crear una copia temporal para la imputación\n",
    "    imputer_data = data[imputer_features].copy()\n",
    "    \n",
    "    # Inicializar KNN Imputer\n",
    "    imputer = KNNImputer(n_neighbors=5, weights='distance')\n",
    "    \n",
    "    # Aplicar imputación\n",
    "    imputed_values = imputer.fit_transform(imputer_data)\n",
    "    \n",
    "    # Reemplazar valores en el DataFrame original solo para las columnas con NAs\n",
    "    for i, col in enumerate(numeric_features):\n",
    "        # Solo imputar si hay valores faltantes en la columna\n",
    "        if data[col].isnull().sum() > 0:\n",
    "            data[col] = imputed_values[:, i]\n",
    "    \n",
    "    # =============================================================================\n",
    "    # 2.3 INGENIERÍA DE CARACTERÍSTICAS\n",
    "    # =============================================================================\n",
    "    \n",
    "    print(\"Creando nuevas características...\")\n",
    "    \n",
    "    # Año actual para cálculos de edad\n",
    "    current_year = 2025\n",
    "    \n",
    "    # Crear características básicas\n",
    "    data['edad_vivienda'] = current_year - data['ano_construccion']\n",
    "    data['superficie_por_habitacion'] = data['superficie_interior_m2'] / data['numero_habitacions']\n",
    "    data['superficie_total'] = data['superficie_interior_m2'] + data['superficie_exterior_m2']\n",
    "    data['ratio_interior_exterior'] = data['superficie_interior_m2'] / (data['superficie_exterior_m2'] + 1)  # Evitar división por cero\n",
    "    data['densidad_banos'] = data['numero_banos'] / data['superficie_interior_m2']\n",
    "    data['densidad_habitaciones'] = data['numero_habitacions'] / data['superficie_interior_m2']\n",
    "    \n",
    "    # Características de localización\n",
    "    # Calculamos la distancia euclidiana desde algunos puntos de referencia en Galicia\n",
    "    # Centro aproximado de A Coruña\n",
    "    data['dist_coruna'] = np.sqrt((data['lonxitude'] - (-8.4))**2 + (data['latitude'] - 43.37)**2)\n",
    "    # Centro aproximado de Vigo\n",
    "    data['dist_vigo'] = np.sqrt((data['lonxitude'] - (-8.72))**2 + (data['latitude'] - 42.23)**2)\n",
    "    # Centro aproximado de Santiago\n",
    "    data['dist_santiago'] = np.sqrt((data['lonxitude'] - (-8.54))**2 + (data['latitude'] - 42.88)**2)\n",
    "    \n",
    "    # Características interactivas\n",
    "    data['calidad_edad'] = data['edad_vivienda'] * data.apply(\n",
    "        lambda x: {'Alta': 3, 'Media': 2, 'Baixa': 1}[x['calidade_materiais']] \n",
    "        if pd.notna(x['calidade_materiais']) else 2, axis=1)\n",
    "    \n",
    "    data['banos_por_habitacion'] = data['numero_banos'] / data['numero_habitacions']\n",
    "    \n",
    "    # Características climáticas\n",
    "    # Codificar la orientación según exposición solar (Sur es mejor)\n",
    "    orientacion_map = {\n",
    "        'Sur': 4,\n",
    "        'Este': 3,\n",
    "        'Oeste': 2,\n",
    "        'Norte': 1\n",
    "    }\n",
    "    data['orientacion_valor'] = data['orientacion'].map(orientacion_map)\n",
    "    \n",
    "    # Codificar eficiencia energética\n",
    "    eficiencia_map = {\n",
    "        'A': 7,\n",
    "        'B': 6,\n",
    "        'C': 5,\n",
    "        'D': 4,\n",
    "        'E': 3,\n",
    "        'F': 2,\n",
    "        'G': 1\n",
    "    }\n",
    "    data['eficiencia_valor'] = data['eficiencia_enerxetica'].map(eficiencia_map)\n",
    "    \n",
    "    # Codificar calidad de materiales\n",
    "    calidad_map = {\n",
    "        'Alta': 3,\n",
    "        'Media': 2,\n",
    "        'Baixa': 1\n",
    "    }\n",
    "    data['calidade_valor'] = data['calidade_materiais'].map(calidad_map)\n",
    "    \n",
    "    # Codificar acceso a transporte público\n",
    "    transporte_map = {\n",
    "        'Bo': 3,\n",
    "        'Regular': 2,\n",
    "        'Malo': 1\n",
    "    }\n",
    "    data['transporte_valor'] = data['acceso_transporte_publico'].map(transporte_map)\n",
    "    \n",
    "    # Características de tipo de vivienda\n",
    "    # One-hot encoding para tipo de edificación\n",
    "    tipo_edificacion_dummies = pd.get_dummies(data['tipo_edificacion'], prefix='tipo')\n",
    "    data = pd.concat([data, tipo_edificacion_dummies], axis=1)\n",
    "    \n",
    "    # One-hot encoding para color favorito del propietario\n",
    "    color_dummies = pd.get_dummies(data['cor_favorita_propietario'], prefix='color')\n",
    "    data = pd.concat([data, color_dummies], axis=1)\n",
    "            \n",
    "    # =============================================================================\n",
    "    # 2.4 MANEJO DE OUTLIERS (Solo para entrenamiento)\n",
    "    # =============================================================================\n",
    "    \n",
    "    if is_train:\n",
    "        print(\"Analizando outliers...\")\n",
    "        \n",
    "        # Detectar outliers en el precio\n",
    "        Q1 = data['prezo_euros'].quantile(0.25)\n",
    "        Q3 = data['prezo_euros'].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        \n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        \n",
    "        outliers = data[(data['prezo_euros'] < lower_bound) | (data['prezo_euros'] > upper_bound)]\n",
    "        \n",
    "        print(f\"Detectados {len(outliers)} outliers en el precio ({len(outliers)/len(data)*100:.2f}%)\")\n",
    "        \n",
    "        # Decidimos mantener los outliers \n",
    "        #data['is_outlier'] = (data['prezo_euros'] < lower_bound) | (data['prezo_euros'] > upper_bound)\n",
    "        \n",
    "        # Transformar la variable objetivo para normalización (log transform)\n",
    "        #data['log_prezo'] = np.log1p(data['prezo_euros'])\n",
    "            \n",
    "    # =============================================================================\n",
    "    # 2.5 NORMALIZACIÓN Y ESCALADO\n",
    "    # =============================================================================\n",
    "    \n",
    "    print(\"Escalando características numéricas...\")\n",
    "    \n",
    "    # Lista de características numéricas para escalar\n",
    "    numeric_features_to_scale = [\n",
    "        'superficie_interior_m2', \n",
    "        'superficie_exterior_m2',\n",
    "        'distancia_centro_km', \n",
    "        'distancia_escola_km',\n",
    "        'indice_criminalidade',\n",
    "        'superficie_por_habitacion',\n",
    "        'superficie_total',\n",
    "        'ratio_interior_exterior',\n",
    "        'densidad_banos',\n",
    "        'densidad_habitaciones',\n",
    "        'dist_coruna',\n",
    "        'dist_vigo',\n",
    "        'dist_santiago',\n",
    "        'calidad_edad'\n",
    "    ]\n",
    "    \n",
    "    # No escalamos variables como número de habitaciones, baños, etc. que tienen sentido como están\n",
    "    \n",
    "    if is_train:\n",
    "        # Inicializar el scaler\n",
    "        scaler = StandardScaler()\n",
    "        # Ajustar el scaler solo en los datos de entrenamiento\n",
    "        scaler.fit(data[numeric_features_to_scale])\n",
    "        # Guardar el scaler para uso futuro\n",
    "        import joblib\n",
    "        joblib.dump(scaler, 'scaler.pkl')\n",
    "    else:\n",
    "        # Cargar el scaler previamente ajustado\n",
    "        import joblib\n",
    "        try:\n",
    "            scaler = joblib.load('scaler.pkl')\n",
    "        except:\n",
    "            print(\"ADVERTENCIA: No se encontró el scaler. Los datos de prueba no se escalarán correctamente.\")\n",
    "            return data\n",
    "    \n",
    "    # Aplicar la transformación\n",
    "    data[numeric_features_to_scale] = scaler.transform(data[numeric_features_to_scale])\n",
    "    \n",
    "    # =============================================================================\n",
    "    # 2.6 LIMPIAR COLUMNAS INNECESARIAS\n",
    "    # =============================================================================\n",
    "    \n",
    "    print(\"Limpiando columnas innecesarias...\")\n",
    "    \n",
    "    # Eliminar columnas originales que ya han sido procesadas o no son necesarias\n",
    "    columns_to_drop = [\n",
    "        # No eliminar 'id' ya que se necesita para la presentación\n",
    "        'tipo_edificacion', \n",
    "        'calidade_materiais', \n",
    "        'cor_favorita_propietario', \n",
    "        'acceso_transporte_publico', \n",
    "        'orientacion', \n",
    "        'eficiencia_enerxetica'\n",
    "    ]\n",
    "    \n",
    "    data = data.drop(columns=columns_to_drop, errors='ignore')\n",
    "    \n",
    "    # Verificar valores faltantes después del preprocesado\n",
    "    missing_after = data.isnull().sum()\n",
    "    print(\"Valores faltantes después del preprocesado:\")\n",
    "    print(missing_after[missing_after > 0])\n",
    "    \n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 3. APLICAR PREPROCESAMIENTO\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\nPreprocesando conjunto de entrenamiento...\")\n",
    "train_processed = preprocess_data(train_data, is_train=True)\n",
    "\n",
    "print(\"\\nPreprocesando conjunto de prueba...\")\n",
    "test_processed = preprocess_data(test_data, is_train=False)\n",
    "\n",
    "# =============================================================================\n",
    "# 4. GUARDAR DATOS PROCESADOS\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\nGuardando datos procesados...\")\n",
    "train_processed.to_csv('train_processed_presentacion.csv', index=False)\n",
    "test_processed.to_csv('test_processed_presentacion.csv', index=False)\n",
    "\n",
    "print(\"Datos procesados guardados en 'train_processed_presentacion.csv' y 'test_processed_presentacion.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 5. VISUALIZACIÓN DE RESULTADOS DEL PREPROCESAMIENTO\n",
    "# =============================================================================\n",
    "\n",
    "print(\"\\nMostrando información de los datos procesados:\")\n",
    "print(f\"Dimensiones del conjunto de entrenamiento procesado: {train_data.shape}\")\n",
    "print(f\"Dimensiones del conjunto de prueba procesado: {test_data.shape}\")\n",
    "\n",
    "# Mostrar las primeras filas\n",
    "print(\"\\nPrimeras filas del conjunto de entrenamiento procesado:\")\n",
    "print(train_data.head())\n",
    "\n",
    "# Mostrar las columnas disponibles\n",
    "print(\"\\nColumnas disponibles en el conjunto de entrenamiento procesado:\")\n",
    "print(train_data.columns.tolist())\n",
    "\"\"\"\n",
    "\n",
    "# Verificar nuevas características creadas\n",
    "new_features = ['superficie_por_habitacion', 'edad_vivienda', 'superficie_total', \n",
    "                'ratio_interior_exterior', 'dist_coruna', 'dist_vigo', 'dist_santiago']\n",
    "print(\"\\nEstadísticas de las nuevas características:\")\n",
    "print(train_data[new_features].describe())\n",
    "\n",
    "print(\"\\nPreprocesamiento completado exitosamente.\")\n",
    "\"\"\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular la correlación con la variable objetivo\n",
    "correlaciones = train_processed.corr()['prezo_euros'].sort_values(ascending=False)\n",
    "\n",
    "# Mostrar solo las correlaciones más altas (por ejemplo, top 15)\n",
    "top_corr = correlaciones.drop('prezo_euros').head(15)\n",
    "\n",
    "# Graficar\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=top_corr.values, y=top_corr.index)\n",
    "plt.title('Correlación de características con el precio')\n",
    "plt.xlabel('Correlación')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# CORRELACIÓN DE NUEVAS CARACTERÍSTICAS CON EL PRECIO (GRÁFICO DE BARRAS)\n",
    "# =============================================================================\n",
    "\n",
    "# Lista de nuevas características creadas\n",
    "nuevas_caracteristicas = [\n",
    "    'edad_vivienda',\n",
    "    'superficie_por_habitacion',\n",
    "    'superficie_total',\n",
    "    'ratio_interior_exterior',\n",
    "    'densidad_banos',\n",
    "    'densidad_habitaciones',\n",
    "    'dist_coruna',\n",
    "    'dist_vigo',\n",
    "    'dist_santiago',\n",
    "    'calidad_edad',\n",
    "    'banos_por_habitacion',\n",
    "    'orientacion_valor',\n",
    "    'eficiencia_valor',\n",
    "    'calidade_valor',\n",
    "    'transporte_valor'\n",
    "]\n",
    "\n",
    "# Calcular correlación con el precio\n",
    "correlaciones = train_processed[nuevas_caracteristicas + ['prezo_euros']].corr()['prezo_euros']\n",
    "correlaciones = correlaciones.drop('prezo_euros').sort_values(ascending=False)\n",
    "\n",
    "# Gráfico de barras horizontales\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=correlaciones.values, y=correlaciones.index)\n",
    "plt.title('Correlación de nuevas características con el precio')\n",
    "plt.xlabel('Correlación con prezo_euros')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODELOS BASE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CATBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuración para visualización\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "sns.set_palette(\"viridis\")\n",
    "os.makedirs('results_initial_hyper', exist_ok=True)\n",
    "\n",
    "# Función para cargar y preparar los datos\n",
    "def load_data(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Verificar y gestionar valores faltantes\n",
    "    print(f\"Valores faltantes en el dataset: {df.isnull().sum().sum()}\")\n",
    "    \n",
    "    # Identificar columnas categóricas automáticamente (si es necesario)\n",
    "    categorical_features = []\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'object' or col in ['tipo_edificacion', 'calidade_materiais', \n",
    "                                               'cor_favorita_propietario', 'acceso_transporte_publico',\n",
    "                                               'orientacion', 'eficiencia_enerxetica']:\n",
    "            categorical_features.append(col)\n",
    "    \n",
    "    print(f\"Características categóricas detectadas: {categorical_features}\")\n",
    "    \n",
    "    # Separamos features y target\n",
    "    X = df.drop(['prezo_euros', 'id'], axis=1, errors='ignore')\n",
    "    y = df['prezo_euros']\n",
    "    \n",
    "    print(f\"Forma del dataset: {df.shape}\")\n",
    "    print(f\"Features incluidas: {X.columns.tolist()}\")\n",
    "    \n",
    "    return X, y, categorical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FASE 1: Entrenamiento base y análisis\n",
    "def train_initial_model(X, y, cat_features=None, random_state=42):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.1, random_state=random_state\n",
    "    )\n",
    "    \n",
    "    initial_params = {\n",
    "        'iterations': 10000,\n",
    "        'learning_rate': 0.05,\n",
    "        'depth': 3,\n",
    "        'loss_function': 'MAE',\n",
    "        'eval_metric': 'MAE',\n",
    "        'random_seed': random_state,\n",
    "        'verbose': 200\n",
    "    }\n",
    "    \n",
    "    print(\"\\n=== Entrenando modelo inicial para análisis ===\")\n",
    "    \n",
    "    # Crear pool de datos para CatBoost\n",
    "    train_pool = Pool(X_train, y_train, cat_features=cat_features)\n",
    "    test_pool = Pool(X_test, y_test, cat_features=cat_features)\n",
    "    \n",
    "    # Entrenar modelo\n",
    "    model = CatBoostRegressor(**initial_params)\n",
    "    model.fit(train_pool, eval_set=test_pool, use_best_model=True)\n",
    "    \n",
    "    # Evaluar modelo\n",
    "    y_pred = model.predict(X_test)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"Métricas del modelo inicial:\")\n",
    "    print(f\"  MAE: {mae:.2f}\")\n",
    "    print(f\"  RMSE: {rmse:.2f}\")\n",
    "    print(f\"  R2: {r2:.4f}\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Analizar importancia de características\n",
    "    feature_importance = model.get_feature_importance()\n",
    "    feature_names = X.columns\n",
    "    importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importance})\n",
    "    importance_df = importance_df.sort_values('Importance', ascending=False)\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(x='Importance', y='Feature', data=importance_df.head(20))\n",
    "    plt.title('Top 20 características más importantes', fontsize=15)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('results_initial_hyper/initial_feature_importance.png')\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    print(\"\\nAnálisis de características completado - Ver 'results_initial_hyper/initial_feature_importance.png'\")\n",
    "    \n",
    "    # Devolver todo lo necesario para optimización\n",
    "    return model, X_train, X_test, y_train, y_test, cat_features, importance_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FASE 2: Optimización con Grid Search (solo sobre los parámetros iniciales)\n",
    "def optimize_with_grid_search(X_train, y_train, X_test, y_test, cat_features=None, random_state=42):\n",
    "    print(\"\\n=== Optimización con Grid Search ===\")\n",
    "    \n",
    "    # Grid de parámetros a evaluar (solo los parámetros iniciales)\n",
    "    param_grid = {\n",
    "        'iterations': [5000],\n",
    "        'learning_rate': [0.1,0.08, 0.05, 0.03, 0.01],\n",
    "        'depth': [1,2,3,4,5,6,7],\n",
    "    }\n",
    "    \n",
    "    # Crear pool para entrenamiento\n",
    "    train_pool = Pool(X_train, y_train, cat_features=cat_features)\n",
    "    \n",
    "    # Configurar modelo base para GridSearch\n",
    "    base_model = CatBoostRegressor(\n",
    "        loss_function='MAE',\n",
    "        eval_metric='MAE',\n",
    "        random_seed=random_state,\n",
    "        verbose=100\n",
    "    )\n",
    "    \n",
    "    # Configurar GridSearchCV\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=base_model,\n",
    "        param_grid=param_grid,\n",
    "        cv=3,  # Usar validación cruzada con 3 folds\n",
    "        scoring='neg_mean_absolute_error',\n",
    "        n_jobs=-1,  # Usar todos los núcleos disponibles\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Entrenar Grid Search\n",
    "    start_time = time.time()\n",
    "    grid_search.fit(X_train, y_train, cat_features=cat_features)\n",
    "    gs_time = time.time() - start_time\n",
    "    \n",
    "    # Mejor valor de MAE encontrado\n",
    "    print(f\"Tiempo de Grid Search: {gs_time:.2f} segundos\")\n",
    "    print(f\"Mejor MAE encontrado: {-grid_search.best_score_:.2f}\")\n",
    "    print(f\"Mejores parámetros: {grid_search.best_params_}\")\n",
    "    \n",
    "    # Evaluar mejor modelo en conjunto de prueba\n",
    "    best_gs_model = grid_search.best_estimator_\n",
    "    y_pred = best_gs_model.predict(X_test)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    \n",
    "    print(f\"MAE en conjunto de prueba: {mae:.2f}\")\n",
    "    \n",
    "    # Guardar modelo optimizado en formato CatBoost .cbm\n",
    "    best_gs_model.save_model('results_initial_hyper/best_model_long.cbm')\n",
    "    print(\"\\nModelo guardado como 'results_initial_hyper/best_model.cbm'\")\n",
    "    \n",
    "    # Guardar resultados\n",
    "    gs_results = pd.DataFrame(grid_search.cv_results_)\n",
    "    gs_results.to_csv('results_initial_hyper/grid_search_results.csv', index=False)\n",
    "    \n",
    "    return best_gs_model, grid_search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fase de ejecución del script\n",
    "if __name__ == \"__main__\":\n",
    "    # Cargar datos\n",
    "    X, y, cat_features = load_data('train_processed.csv')\n",
    "    \n",
    "    # Entrenar el modelo inicial\n",
    "    model, X_train, X_test, y_train, y_test, cat_features, importance_df = train_initial_model(X, y, cat_features)\n",
    "    \n",
    "    # Guardar modelo optimizado en formato CatBoost .cbm\n",
    "    model.save_model('results_initial_hyper/best_model_initial.cbm')\n",
    "    print(\"\\nModelo guardado como 'results_initial_hyper/best_model.cbm'\")\n",
    "\n",
    "    # Evaluar mejor modelo en conjunto de prueba\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    \n",
    "    print(f\"MAE en conjunto de prueba: {mae:.2f}\")\n",
    "    \n",
    "    # Guardar resultados\n",
    "    #gs_results = pd.DataFrame(model.cv_results_)\n",
    "    #gs_results.to_csv('results_initial_hyper/grid_search_results.csv', index=False)\n",
    "    \n",
    "    # Optimización de hiperparámetros con GridSearch\n",
    "    #best_model, best_params = optimize_with_grid_search(X_train, y_train, X_test, y_test, cat_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directory for results\n",
    "os.makedirs('models', exist_ok=True)\n",
    "os.makedirs('results_xgboost', exist_ok=True)\n",
    "\n",
    "# Set visualization parameters\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "sns.set_palette(\"viridis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load and prepare data\n",
    "def load_data(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(f\"Valores faltantes en el dataset: {df.isnull().sum().sum()}\")\n",
    "    \n",
    "    # Identify categorical features\n",
    "    categorical_features = []\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'object' or col in ['tipo_edificacion', 'calidade_materiais', \n",
    "                                               'cor_favorita_propietario', 'acceso_transporte_publico',\n",
    "                                               'orientacion', 'eficiencia_enerxetica'] or 'tipo_' in col or 'color_' in col:\n",
    "            categorical_features.append(col)\n",
    "    \n",
    "    print(f\"Características categóricas detectadas: {categorical_features}\")\n",
    "    \n",
    "    # Separate features and target\n",
    "    X = df.drop(['prezo_euros', 'id'], axis=1, errors='ignore')\n",
    "    y = df['prezo_euros']\n",
    "    \n",
    "    print(f\"Forma del dataset: {df.shape}\")\n",
    "    print(f\"Features incluidas: {X.columns.tolist()}\")\n",
    "    \n",
    "    return X, y, categorical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train XGBoost model\n",
    "def train_xgboost_model(X, y, random_state=42):\n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.1, random_state=random_state\n",
    "    )\n",
    "    \n",
    "    print(\"\\n=== Entrenando modelo XGBoost ===\")\n",
    "    \n",
    "    # Initial hyperparameters\n",
    "    param_grid = {\n",
    "        'n_estimators': [100, 200, 500],\n",
    "        'learning_rate': [0.01, 0.05, 0.1],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'min_child_weight': [1, 3, 5],\n",
    "        'subsample': [0.8, 1.0],\n",
    "        'colsample_bytree': [0.8, 1.0],\n",
    "        'reg_alpha': [0, 0.1, 0.5],\n",
    "        'reg_lambda': [1, 1.5, 2],\n",
    "        'objective': ['reg:squarederror']\n",
    "    }\n",
    "    \n",
    "    # Simplified version for faster execution\n",
    "    simple_param_grid = {\n",
    "        'n_estimators': [200, 500],\n",
    "        'learning_rate': [0.05, 0.1],\n",
    "        'max_depth': [3, 5],\n",
    "        'subsample': [0.8],\n",
    "        'colsample_bytree': [0.8],\n",
    "        'objective': ['reg:squarederror']\n",
    "    }\n",
    "    \n",
    "    # Create model\n",
    "    model = XGBRegressor(random_state=random_state)\n",
    "    \n",
    "    # Grid search with cross-validation\n",
    "    print(\"Realizando búsqueda de hiperparámetros con GridSearchCV...\")\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=model,\n",
    "        param_grid=param_grid,\n",
    "        cv=5,\n",
    "        scoring='neg_mean_absolute_error',\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Fit grid search\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    # Best parameters\n",
    "    print(f\"Mejores hiperparámetros: {grid_search.best_params_}\")\n",
    "    \n",
    "    # Train final model\n",
    "    best_model = XGBRegressor(**grid_search.best_params_, random_state=random_state)\n",
    "    best_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate model\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"Métricas del modelo XGBoost:\")\n",
    "    print(f\"  MAE: {mae:.2f}\")\n",
    "    print(f\"  RMSE: {rmse:.2f}\")\n",
    "    print(f\"  R2: {r2:.4f}\")\n",
    "    \n",
    "    # Feature importance\n",
    "    feature_importance = best_model.feature_importances_\n",
    "    feature_names = X.columns\n",
    "    importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importance})\n",
    "    importance_df = importance_df.sort_values('Importance', ascending=False)\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(x='Importance', y='Feature', data=importance_df.head(20))\n",
    "    plt.title('Top 20 características más importantes (XGBoost)', fontsize=15)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('results_xgboost/xgboost_feature_importance.png')\n",
    "    \n",
    "    print(\"\\nAnálisis de características completado - Ver 'results_xgboost/xgboost_feature_importance.png'\")\n",
    "    \n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Load data\n",
    "    X, y, cat_features = load_data('train_processed.csv')\n",
    "    \n",
    "    # Train XGBoost model\n",
    "    model = train_xgboost_model(X, y)\n",
    "    \n",
    "    # Save model\n",
    "    model.save_model('models_stacking/stacking_xgboost_model.json')\n",
    "    print(\"\\nModelo guardado como 'models/xgboost_model.json'\")\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"Tiempo total de ejecución: {end_time - start_time:.2f} segundos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LIGHTGBM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directory for results\n",
    "os.makedirs('models', exist_ok=True)\n",
    "os.makedirs('results_lightgbm', exist_ok=True)\n",
    "\n",
    "# Set visualization parameters\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "sns.set_palette(\"viridis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load and prepare data\n",
    "def load_data(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(f\"Valores faltantes en el dataset: {df.isnull().sum().sum()}\")\n",
    "    \n",
    "    # Identify categorical features\n",
    "    categorical_features = []\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'object' or col in ['tipo_edificacion', 'calidade_materiais', \n",
    "                                              'cor_favorita_propietario', 'acceso_transporte_publico',\n",
    "                                              'orientacion', 'eficiencia_enerxetica'] or 'tipo_' in col or 'color_' in col:\n",
    "            categorical_features.append(col)\n",
    "    \n",
    "    print(f\"Características categóricas detectadas: {categorical_features}\")\n",
    "    \n",
    "    # Separate features and target\n",
    "    X = df.drop(['prezo_euros', 'id'], axis=1, errors='ignore')\n",
    "    y = df['prezo_euros']\n",
    "    \n",
    "    print(f\"Forma del dataset: {df.shape}\")\n",
    "    print(f\"Features incluidas: {X.columns.tolist()}\")\n",
    "    \n",
    "    return X, y, categorical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train LightGBM model\n",
    "def train_lightgbm_model(X, y, categorical_features=None, random_state=42):\n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.1, random_state=random_state\n",
    "    )\n",
    "    \n",
    "    print(\"\\n=== Entrenando modelo LightGBM ===\")\n",
    "    \n",
    "    # Prepare data\n",
    "    categorical_feature_indices = []\n",
    "    if categorical_features:\n",
    "        categorical_feature_indices = [list(X.columns).index(col) for col in categorical_features if col in X.columns]\n",
    "    \n",
    "    train_data = lgb.Dataset(X_train, label=y_train, categorical_feature=categorical_feature_indices)\n",
    "    valid_data = lgb.Dataset(X_test, label=y_test, categorical_feature=categorical_feature_indices, reference=train_data)\n",
    "    \n",
    "    # Parameters\n",
    "    params = {\n",
    "        'objective': 'regression',\n",
    "        'metric': 'mae',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'num_leaves': 31,\n",
    "        'learning_rate': 0.05,\n",
    "        'feature_fraction': 0.9,\n",
    "        'bagging_fraction': 0.8,\n",
    "        'bagging_freq': 5,\n",
    "        'verbose': -1,\n",
    "        'random_state': random_state\n",
    "    }\n",
    "    \n",
    "    # Grid search for hyperparameters\n",
    "    param_grid = {\n",
    "        'num_leaves': [15, 31, 63],\n",
    "        'learning_rate': [0.01, 0.05, 0.1],\n",
    "        'n_estimators': [100, 200, 500],\n",
    "        'min_child_samples': [5, 10, 20],\n",
    "        'subsample': [0.8, 1.0],\n",
    "        'colsample_bytree': [0.8, 1.0]\n",
    "    }\n",
    "    \n",
    "    # Simplified grid for faster execution\n",
    "    simple_param_grid = {\n",
    "        'num_leaves': [31, 63],\n",
    "        'learning_rate': [0.05, 0.1],\n",
    "        'n_estimators': [200, 500],\n",
    "        'min_child_samples': [10,20],\n",
    "        'subsample': [0.8],\n",
    "        'colsample_bytree': [1.0]\n",
    "    }\n",
    "    \n",
    "    lgb_model = lgb.LGBMRegressor(objective='regression', random_state=random_state, verbose=-1)\n",
    "    \n",
    "    print(\"Realizando búsqueda de hiperparámetros con GridSearchCV...\")\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=lgb_model,\n",
    "        param_grid=simple_param_grid,\n",
    "        cv=5,\n",
    "        scoring='neg_mean_absolute_error',\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Convert categorical features list to indices for sklearn API\n",
    "    if categorical_features:\n",
    "        cat_indices = [i for i, col in enumerate(X.columns) if col in categorical_features]\n",
    "    else:\n",
    "        cat_indices = 'auto'\n",
    "    \n",
    "    grid_search.fit(X_train, y_train, eval_metric='mae', categorical_feature=cat_indices)\n",
    "    \n",
    "    print(f\"Mejores hiperparámetros: {grid_search.best_params_}\")\n",
    "    \n",
    "    # Train final model with best parameters\n",
    "    final_params = {**params, **grid_search.best_params_}\n",
    "    best_model = lgb.train(\n",
    "        final_params,\n",
    "        train_data,\n",
    "        num_boost_round=final_params.get('n_estimators', 500),\n",
    "        valid_sets=[valid_data],\n",
    "        callbacks=[lgb.early_stopping(stopping_rounds=50, verbose=True)]\n",
    "    )\n",
    "    \n",
    "    # Evaluate model\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"Métricas del modelo LightGBM:\")\n",
    "    print(f\"  MAE: {mae:.2f}\")\n",
    "    print(f\"  RMSE: {rmse:.2f}\")\n",
    "    print(f\"  R2: {r2:.4f}\")\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    # Feature importance\n",
    "    feature_importance = best_model.feature_importance()\n",
    "    feature_names = best_model.feature_name()\n",
    "    importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importance})\n",
    "    importance_df = importance_df.sort_values('Importance', ascending=False)\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(x='Importance', y='Feature', data=importance_df.head(20))\n",
    "    plt.title('Top 20 características más importantes (LightGBM)', fontsize=15)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('results_lightgbm/lightgbm_feature_importance.png')\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    print(\"\\nAnálisis de características completado - Ver 'results_lightgbm/lightgbm_feature_importance.png'\")\n",
    "    \n",
    "    return best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Load data\n",
    "    X, y, cat_features = load_data('train_processed.csv')\n",
    "    \n",
    "    # Train LightGBM model\n",
    "    model = train_lightgbm_model(X, y, cat_features)\n",
    "    \n",
    "    # Save model\n",
    "    model.save_model('models_stacking/stacking_lightgbm_model.txt')\n",
    "    print(\"\\nModelo guardado como 'models/lightgbm_model.txt'\")\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"Tiempo total de ejecución: {end_time - start_time:.2f} segundos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP (SCIKIT-LEARN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directory for results\n",
    "os.makedirs('models', exist_ok=True)\n",
    "os.makedirs('results_mlp', exist_ok=True)\n",
    "\n",
    "# Set visualization parameters\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "sns.set_palette(\"viridis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load and prepare data\n",
    "def load_data(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(f\"Valores faltantes en el dataset: {df.isnull().sum().sum()}\")\n",
    "    \n",
    "    # Identify categorical features\n",
    "    categorical_features = []\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'object' or col in ['tipo_edificacion', 'calidade_materiais', \n",
    "                                               'cor_favorita_propietario', 'acceso_transporte_publico',\n",
    "                                               'orientacion', 'eficiencia_enerxetica'] or 'tipo_' in col or 'color_' in col:\n",
    "            categorical_features.append(col)\n",
    "    \n",
    "    print(f\"Características categóricas detectadas: {categorical_features}\")\n",
    "    \n",
    "    # Separate features and target\n",
    "    X = df.drop(['prezo_euros', 'id'], axis=1, errors='ignore')\n",
    "    y = df['prezo_euros']\n",
    "    \n",
    "    print(f\"Forma del dataset: {df.shape}\")\n",
    "    print(f\"Features incluidas: {X.columns.tolist()}\")\n",
    "    \n",
    "    return X, y, categorical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to train MLP model\n",
    "def train_mlp_model(X, y, random_state=42):\n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.1, random_state=random_state\n",
    "    )\n",
    "    \n",
    "    print(\"\\n=== Entrenando modelo MLP (Red Neuronal) ===\")\n",
    "    \n",
    "    # Standardize data for neural network\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Save scaler for later use\n",
    "    joblib.dump(scaler, 'models/mlp_scaler.pkl')\n",
    "    print(\"Scaler guardado como 'models/mlp_scaler.pkl'\")\n",
    "    \n",
    "    # Initial hyperparameters\n",
    "    param_grid = {\n",
    "        'hidden_layer_sizes': [(50,), (100,), (100, 50), (250,100)],\n",
    "        'activation': ['relu'],\n",
    "        'solver': ['adam'],\n",
    "        'alpha': [ 0.001, 0.01],\n",
    "        'learning_rate': ['constant', 'adaptive'],\n",
    "        'max_iter': [500, 1000]\n",
    "    }\n",
    "    \n",
    "    # Simplified grid for faster execution\n",
    "    simple_param_grid = {\n",
    "        'hidden_layer_sizes': [(100,), (100, 50)],\n",
    "        'activation': ['relu', 'tanh'],\n",
    "        'alpha': [0.0001, 0.001],\n",
    "        'max_iter': [500]\n",
    "    }\n",
    "    \n",
    "    # Create model\n",
    "    model = MLPRegressor(random_state=random_state)\n",
    "    \n",
    "    # Grid search with cross-validation\n",
    "    print(\"Realizando búsqueda de hiperparámetros con GridSearchCV...\")\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=model,\n",
    "        param_grid=param_grid,\n",
    "        cv=5,\n",
    "        scoring='neg_mean_absolute_error',\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Fit grid search\n",
    "    grid_search.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Best parameters\n",
    "    print(f\"Mejores hiperparámetros: {grid_search.best_params_}\")\n",
    "    \n",
    "    # Train final model\n",
    "    best_model = MLPRegressor(**grid_search.best_params_, random_state=random_state)\n",
    "    best_model.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Evaluate model\n",
    "    y_pred = best_model.predict(X_test_scaled)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"Métricas del modelo MLP:\")\n",
    "    print(f\"  MAE: {mae:.2f}\")\n",
    "    print(f\"  RMSE: {rmse:.2f}\")\n",
    "    print(f\"  R2: {r2:.4f}\")\n",
    "    \n",
    "    # Create a combined model object with the scaler and MLP model\n",
    "    mlp_model = {\n",
    "        'scaler': scaler,\n",
    "        'model': best_model\n",
    "    }\n",
    "    \n",
    "    # Plot loss curve\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(best_model.loss_curve_)\n",
    "    plt.title('Curva de pérdida durante el entrenamiento', fontsize=15)\n",
    "    plt.xlabel('Iteraciones')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.grid(True)\n",
    "    plt.savefig('results_mlp/mlp_loss_curve.png')\n",
    "    print(\"\\nCurva de pérdida guardada - Ver 'results_mlp/mlp_loss_curve.png'\")\n",
    "    \n",
    "    # Analyze prediction errors\n",
    "    errors = y_test - y_pred\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.scatter(y_test, errors)\n",
    "    plt.axhline(y=0, color='r', linestyle='-')\n",
    "    plt.title('Errores de predicción vs Valores reales', fontsize=15)\n",
    "    plt.xlabel('Precio real')\n",
    "    plt.ylabel('Error')\n",
    "    plt.grid(True)\n",
    "    plt.savefig('results_mlp/mlp_error_analysis.png')\n",
    "    print(\"Análisis de errores guardado - Ver 'results_mlp/mlp_error_analysis.png'\")\n",
    "    \n",
    "    return mlp_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Load data\n",
    "    X, y, cat_features = load_data('train_processed.csv')\n",
    "    \n",
    "    # Train MLP model\n",
    "    model = train_mlp_model(X, y)\n",
    "    \n",
    "    # Save model\n",
    "    joblib.dump(model, 'models_stacking/stacking_mlp_model.pkl')\n",
    "    print(\"\\nModelo guardado como 'models/mlp_model.pkl'\")\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"Tiempo total de ejecución: {end_time - start_time:.2f} segundos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MLP (PyTorch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories\n",
    "os.makedirs('models', exist_ok=True)\n",
    "os.makedirs('results_mlp', exist_ok=True)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Usando dispositivo: {device}\")\n",
    "\n",
    "def load_data(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(f\"Valores faltantes: {df.isnull().sum().sum()}\")\n",
    "    categorical_features = [col for col in df.columns if 'tipo_' in col or 'color_' in col]\n",
    "    X = df.drop(['prezo_euros', 'id'], axis=1, errors='ignore')\n",
    "    y = df['prezo_euros']\n",
    "    return X, y, categorical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPRegressorTorch(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(MLPRegressorTorch, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.SiLU(),\n",
    "            nn.Dropout(0.25),\n",
    "\n",
    "            nn.Linear(256, 128),\n",
    "            nn.BatchNorm1d(128),\n",
    "            nn.SiLU(),\n",
    "\n",
    "            nn.Linear(128, 64),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.SiLU(),\n",
    "\n",
    "            nn.Linear(64, 32),\n",
    "            nn.SiLU(),\n",
    "\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_torch_model(X, y, epochs=300, batch_size=64, lr=0.001):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    joblib.dump(scaler, 'models/mlp_scaler.pkl')\n",
    "\n",
    "    # Convert to tensors\n",
    "    X_train_tensor = torch.tensor(X_train_scaled, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train.values.reshape(-1, 1), dtype=torch.float32)\n",
    "    X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32)\n",
    "    y_test_tensor = torch.tensor(y_test.values.reshape(-1, 1), dtype=torch.float32)\n",
    "\n",
    "    train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    model = MLPRegressorTorch(X.shape[1]).to(device)\n",
    "    criterion = nn.L1Loss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    losses = []\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        avg_loss = running_loss / len(train_loader)\n",
    "        losses.append(avg_loss)\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Época {epoch}: Loss {avg_loss:.4f}\")\n",
    "\n",
    "    # Evaluation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        predictions = model(X_test_tensor.to(device)).cpu().numpy().flatten()\n",
    "    mae = mean_absolute_error(y_test, predictions)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, predictions))\n",
    "    r2 = r2_score(y_test, predictions)\n",
    "\n",
    "    print(f\"\\n--- Métricas ---\\nMAE: {mae:.2f}\\nRMSE: {rmse:.2f}\\nR2: {r2:.4f}\")\n",
    "\n",
    "    # Save model\n",
    "    torch.save(model.state_dict(), 'models_stacking/mlp_torch_model.pt')\n",
    "    print(\"Modelo guardado como 'models/mlp_torch_model.pt'\")\n",
    "\n",
    "    # Loss curve\n",
    "    plt.figure()\n",
    "    plt.plot(losses)\n",
    "    plt.title(\"Curva de pérdida\")\n",
    "    plt.xlabel(\"Épocas\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.grid(True)\n",
    "    plt.savefig(\"results_mlp/mlp_loss_curve.png\")\n",
    "\n",
    "    # Error analysis\n",
    "    errors = y_test.values - predictions\n",
    "    plt.figure()\n",
    "    plt.scatter(y_test.values, errors)\n",
    "    plt.axhline(0, color='r')\n",
    "    plt.title(\"Errores de predicción vs Precio real\")\n",
    "    plt.xlabel(\"Precio real\")\n",
    "    plt.ylabel(\"Error\")\n",
    "    plt.grid(True)\n",
    "    plt.savefig(\"results_mlp/mlp_error_analysis.png\")\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    start = time.time()\n",
    "    X, y, _ = load_data('train_processed.csv')\n",
    "    model = train_torch_model(X, y)\n",
    "    print(f\"\\nTiempo total: {time.time() - start:.2f} s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STACKING V1 (SIMPLE) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results directory\n",
    "os.makedirs('results_stacking', exist_ok=True)\n",
    "\n",
    "# Function to load data\n",
    "def load_data(train_path, test_path):\n",
    "    \"\"\"\n",
    "    Load training and test data\n",
    "    \"\"\"\n",
    "    train_df = pd.read_csv(train_path)\n",
    "    test_df = pd.read_csv(test_path)\n",
    "    \n",
    "    # For training data\n",
    "    X_train = train_df.drop(['prezo_euros', 'id'], axis=1, errors='ignore')\n",
    "    y_train = train_df['prezo_euros']\n",
    "    \n",
    "    # For test data\n",
    "    if 'Unnamed: 0' in test_df.columns:\n",
    "        X_test = test_df.drop(['id', 'Unnamed: 0'], axis=1, errors='ignore')\n",
    "    else:\n",
    "        X_test = test_df.drop(['id'], axis=1, errors='ignore')\n",
    "    test_ids = test_df['id']\n",
    "    \n",
    "    print(f\"Training data shape: {X_train.shape}\")\n",
    "    print(f\"Test data shape: {X_test.shape}\")\n",
    "    return X_train, y_train, X_test, test_ids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to identify categorical features\n",
    "def get_categorical_features(df):\n",
    "    \"\"\"\n",
    "    Identify categorical features in the dataset\n",
    "    \"\"\"\n",
    "    categorical_features = []\n",
    "    for col in df.columns:\n",
    "        if (df[col].dtype == 'object' or\n",
    "            col in ['tipo_edificacion', 'calidade_materiais',\n",
    "                   'cor_favorita_propietario', 'acceso_transporte_publico',\n",
    "                   'orientacion', 'eficiencia_enerxetica'] or\n",
    "            'tipo_' in col or 'color_' in col):\n",
    "            categorical_features.append(col)\n",
    "    print(f\"Categorical features: {categorical_features}\")\n",
    "    return categorical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load pretrained models\n",
    "def load_models(model_paths):\n",
    "    \"\"\"\n",
    "    Load pretrained models from specified paths\n",
    "    \"\"\"\n",
    "    models = {}\n",
    "    \n",
    "    # Load CatBoost model\n",
    "    if 'catboost' in model_paths and os.path.exists(model_paths['catboost']):\n",
    "        print(\"Loading CatBoost model...\")\n",
    "        models['catboost'] = CatBoostRegressor()\n",
    "        models['catboost'].load_model(model_paths['catboost'])\n",
    "    \n",
    "    # Load XGBoost model\n",
    "    if 'xgboost' in model_paths and os.path.exists(model_paths['xgboost']):\n",
    "        print(\"Loading XGBoost model...\")\n",
    "        models['xgboost'] = XGBRegressor()\n",
    "        models['xgboost'].load_model(model_paths['xgboost'])\n",
    "    \n",
    "    # Load LightGBM model\n",
    "    if 'lightgbm' in model_paths and os.path.exists(model_paths['lightgbm']):\n",
    "        print(\"Loading LightGBM model...\")\n",
    "        models['lightgbm'] = lgb.Booster(model_file=model_paths['lightgbm'])\n",
    "    \n",
    "    # Load MLP model (which includes the scaler)\n",
    "    if 'mlp' in model_paths and os.path.exists(model_paths['mlp']):\n",
    "        print(\"Loading MLP model...\")\n",
    "        models['mlp'] = joblib.load(model_paths['mlp'])\n",
    "    \n",
    "    print(f\"Successfully loaded {len(models)} models\")\n",
    "    return models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to make predictions with loaded models\n",
    "def predict_with_models(models, X):\n",
    "    \"\"\"\n",
    "    Make predictions using all loaded models\n",
    "    \"\"\"\n",
    "    predictions = {}\n",
    "    \n",
    "    if 'catboost' in models:\n",
    "        print(\"Predicting with CatBoost...\")\n",
    "        predictions['catboost'] = models['catboost'].predict(X)\n",
    "    \n",
    "    if 'xgboost' in models:\n",
    "        print(\"Predicting with XGBoost...\")\n",
    "        predictions['xgboost'] = models['xgboost'].predict(X)\n",
    "    \n",
    "    if 'lightgbm' in models:\n",
    "        print(\"Predicting with LightGBM...\")\n",
    "        predictions['lightgbm'] = models['lightgbm'].predict(X)\n",
    "    \n",
    "    if 'mlp' in models:\n",
    "        print(\"Predicting with MLP...\")\n",
    "        # Extract model and scaler from the MLP model object\n",
    "        scaler = models['mlp']['scaler']\n",
    "        mlp_model = models['mlp']['model']\n",
    "        # Apply scaling before prediction\n",
    "        X_scaled = scaler.transform(X)\n",
    "        predictions['mlp'] = mlp_model.predict(X_scaled)\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate level 1 features\n",
    "def generate_level1_features(models, X_train, X_test, y_train, categorical_features=None):\n",
    "    \"\"\"\n",
    "    Generate level 1 training features for meta-model using cross-validation\n",
    "    \"\"\"\n",
    "    print(\"Generating level 1 features for stacking...\")\n",
    "    \n",
    "    # For training set, use k-fold cross-validation to avoid data leakage\n",
    "    k = 5\n",
    "    train_preds_df = pd.DataFrame(index=range(X_train.shape[0]))\n",
    "    test_preds_all = {}\n",
    "    \n",
    "    # Create folds\n",
    "    n_samples = X_train.shape[0]\n",
    "    fold_size = n_samples // k\n",
    "    indices = np.arange(n_samples)\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    for fold in range(k):\n",
    "        print(f\"Processing fold {fold+1}/{k}\")\n",
    "        \n",
    "        # Get indices for this fold\n",
    "        start_idx = fold * fold_size\n",
    "        end_idx = (fold + 1) * fold_size if fold < k - 1 else n_samples\n",
    "        val_indices = indices[start_idx:end_idx]\n",
    "        train_indices = np.setdiff1d(indices, val_indices)\n",
    "        \n",
    "        # Split data for this fold\n",
    "        X_train_fold = X_train.iloc[train_indices]\n",
    "        y_train_fold = y_train.iloc[train_indices]\n",
    "        X_val_fold = X_train.iloc[val_indices]\n",
    "        \n",
    "        # Generate predictions for this fold\n",
    "        fold_preds = pd.DataFrame(index=val_indices)\n",
    "        test_fold_preds = {}\n",
    "        \n",
    "        # Train and predict with CatBoost\n",
    "        if 'catboost' in models:\n",
    "            print(\"Training fold with CatBoost...\")\n",
    "            if categorical_features:\n",
    "                train_pool = Pool(X_train_fold, y_train_fold, cat_features=categorical_features)\n",
    "                val_pool = Pool(X_val_fold, cat_features=categorical_features)\n",
    "                test_pool = Pool(X_test, cat_features=categorical_features)\n",
    "                \n",
    "                temp_model = CatBoostRegressor()\n",
    "                temp_model.set_params(**models['catboost'].get_params())\n",
    "                temp_model.fit(train_pool, verbose=False)\n",
    "                fold_preds['catboost'] = temp_model.predict(val_pool)\n",
    "                test_fold_preds['catboost'] = temp_model.predict(test_pool)\n",
    "            else:\n",
    "                temp_model = CatBoostRegressor()\n",
    "                temp_model.set_params(**models['catboost'].get_params())\n",
    "                temp_model.fit(X_train_fold, y_train_fold, verbose=False)\n",
    "                fold_preds['catboost'] = temp_model.predict(X_val_fold)\n",
    "                test_fold_preds['catboost'] = temp_model.predict(X_test)\n",
    "        \n",
    "        # Train and predict with XGBoost\n",
    "        if 'xgboost' in models:\n",
    "            print(\"Training fold with XGBoost...\")\n",
    "            temp_model = XGBRegressor()\n",
    "            temp_model.set_params(**models['xgboost'].get_params())\n",
    "            temp_model.fit(X_train_fold, y_train_fold, verbose=False)\n",
    "            fold_preds['xgboost'] = temp_model.predict(X_val_fold)\n",
    "            test_fold_preds['xgboost'] = temp_model.predict(X_test)\n",
    "        \n",
    "        # Train and predict with LightGBM\n",
    "        if 'lightgbm' in models:\n",
    "            print(\"Training fold with LightGBM...\")\n",
    "            train_lgb = lgb.Dataset(X_train_fold, label=y_train_fold)\n",
    "            params = models['lightgbm'].params if hasattr(models['lightgbm'], 'params') else {}\n",
    "            temp_model = lgb.train(params, train_lgb, num_boost_round=100)\n",
    "            fold_preds['lightgbm'] = temp_model.predict(X_val_fold)\n",
    "            test_fold_preds['lightgbm'] = temp_model.predict(X_test)\n",
    "        \n",
    "        # Train and predict with MLP\n",
    "        if 'mlp' in models:\n",
    "            print(\"Training fold with MLP...\")\n",
    "            # Extract parameters from loaded model\n",
    "            mlp_params = models['mlp']['model'].get_params()\n",
    "            \n",
    "            # Create and fit scaler\n",
    "            scaler = StandardScaler()\n",
    "            X_train_fold_scaled = scaler.fit_transform(X_train_fold)\n",
    "            X_val_fold_scaled = scaler.transform(X_val_fold)\n",
    "            X_test_scaled = scaler.transform(X_test)\n",
    "            \n",
    "            # Create and train MLP model\n",
    "            temp_model = MLPRegressor(**mlp_params)\n",
    "            temp_model.fit(X_train_fold_scaled, y_train_fold)\n",
    "            \n",
    "            fold_preds['mlp'] = temp_model.predict(X_val_fold_scaled)\n",
    "            test_fold_preds['mlp'] = temp_model.predict(X_test_scaled)\n",
    "\n",
    "        if 'torch_mlp' in models:\n",
    "            print(\"Training fold with PyTorch MLP...\")\n",
    "            # Get parameters from the loaded model\n",
    "            torch_mlp_info = models['torch_mlp']\n",
    "            device = torch_mlp_info['device']\n",
    "            \n",
    "            # Create and fit scaler\n",
    "            scaler = StandardScaler()\n",
    "            X_train_fold_scaled = scaler.fit_transform(X_train_fold)\n",
    "            X_val_fold_scaled = scaler.transform(X_val_fold)\n",
    "            X_test_scaled = scaler.transform(X_test)\n",
    "            \n",
    "            # Create a new model with the same architecture\n",
    "            input_dim = X_train_fold_scaled.shape[1]\n",
    "            temp_model = MLPRegressorTorch(input_dim).to(device)\n",
    "            \n",
    "            # Convert to PyTorch tensors\n",
    "            X_train_tensor = torch.tensor(X_train_fold_scaled, dtype=torch.float32).to(device)\n",
    "            y_train_tensor = torch.tensor(y_train_fold.values.reshape(-1, 1), dtype=torch.float32).to(device)\n",
    "            X_val_tensor = torch.tensor(X_val_fold_scaled, dtype=torch.float32).to(device)\n",
    "            X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32).to(device)\n",
    "            \n",
    "            # Train the PyTorch model\n",
    "            criterion = nn.L1Loss()\n",
    "            optimizer = torch.optim.Adam(temp_model.parameters(), lr=0.001)\n",
    "            batch_size = 64\n",
    "            epochs = 100\n",
    "            \n",
    "            for epoch in range(epochs):\n",
    "                # Create mini-batches\n",
    "                perm = torch.randperm(X_train_tensor.size(0))\n",
    "                for start in range(0, X_train_tensor.size(0), batch_size):\n",
    "                    batch_indices = perm[start:start + batch_size]\n",
    "                    batch_X = X_train_tensor[batch_indices]\n",
    "                    batch_y = y_train_tensor[batch_indices]\n",
    "                    \n",
    "                    # Forward and backward pass\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = temp_model(batch_X)\n",
    "                    loss = criterion(outputs, batch_y)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "            \n",
    "            # Get predictions\n",
    "            temp_model.eval()\n",
    "            with torch.no_grad():\n",
    "                fold_preds['torch_mlp'] = temp_model(X_val_tensor).cpu().numpy().flatten()\n",
    "                test_fold_preds['torch_mlp'] = temp_model(X_test_tensor).cpu().numpy().flatten()\n",
    "        \n",
    "        # Add fold predictions to overall predictions DataFrame\n",
    "        for model_name in fold_preds.columns:\n",
    "            train_preds_df.loc[val_indices, model_name] = fold_preds[model_name].values\n",
    "            \n",
    "            if model_name not in test_preds_all:\n",
    "                test_preds_all[model_name] = []\n",
    "            test_preds_all[model_name].append(test_fold_preds[model_name])\n",
    "    \n",
    "    # Create test predictions by averaging fold predictions\n",
    "    test_preds_df = pd.DataFrame()\n",
    "    for model_name, preds_list in test_preds_all.items():\n",
    "        test_preds_df[model_name] = np.mean(preds_list, axis=0)\n",
    "    \n",
    "    print(f\"Generated level 1 features with shape: {train_preds_df.shape} (train), {test_preds_df.shape} (test)\")\n",
    "    return train_preds_df, test_preds_df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_meta_model(level1_train, y_train, level1_test):\n",
    "    \"\"\"\n",
    "    Entrena y optimiza un meta-modelo ElasticNet sobre features de nivel-1,\n",
    "    buscando minimizar el MAE mediante validación cruzada.\n",
    "    \"\"\"\n",
    "    print(\"Buscando hiperparámetros óptimos para ElasticNet (minimizando MAE)...\")\n",
    "    \n",
    "    # Definición de la malla de búsqueda\n",
    "    param_grid = {\n",
    "        'alpha':    [0.001, 0.01, 0.1, 1.0, 10.0],\n",
    "        'l1_ratio': [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "    }\n",
    "    \n",
    "    base_model = ElasticNet(max_iter=10000, random_state=42)\n",
    "    grid = GridSearchCV(\n",
    "        estimator=base_model,\n",
    "        param_grid=param_grid,\n",
    "        cv=5,\n",
    "        scoring='neg_mean_absolute_error',\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Ajuste de la búsqueda de hiperparámetros\n",
    "    grid.fit(level1_train, y_train)\n",
    "    \n",
    "    best_model = grid.best_estimator_\n",
    "    best_mae   = -grid.best_score_\n",
    "    print(f\"Mejores parámetros: {grid.best_params_}\")\n",
    "    print(f\"MAE CV medio óptimo: {best_mae:.4f}\")\n",
    "    \n",
    "    # Dividir para validación adicional\n",
    "    X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "        level1_train, y_train, test_size=0.2, random_state=42\n",
    "    )\n",
    "    best_model.fit(X_tr, y_tr)\n",
    "    val_preds = best_model.predict(X_val)\n",
    "    \n",
    "    val_mae  = mean_absolute_error(y_val, val_preds)\n",
    "    val_rmse = np.sqrt(mean_squared_error(y_val, val_preds))\n",
    "    val_r2   = r2_score(y_val, val_preds)\n",
    "    \n",
    "    print(\"Métricas en conjunto de validación:\")\n",
    "    print(f\"  MAE : {val_mae:.4f}\")\n",
    "    print(f\"  RMSE: {val_rmse:.4f}\")\n",
    "    print(f\"  R²  : {val_r2:.4f}\")\n",
    "    \n",
    "    # Reentrenar con los mejores parámetros sobre todo el dataset de entrenamiento\n",
    "    best_model.fit(level1_train, y_train)\n",
    "    \n",
    "    # Predicciones sobre el conjunto de test\n",
    "    test_preds = best_model.predict(level1_test)\n",
    "    \n",
    "    return best_model, test_preds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create submission file\n",
    "def create_submission(test_ids, predictions, output_file):\n",
    "    \"\"\"\n",
    "    Create submission file with predictions\n",
    "    \"\"\"\n",
    "    submission_df = pd.DataFrame({\n",
    "        'id': test_ids,\n",
    "        'prezo_euros': predictions\n",
    "    })\n",
    "    \n",
    "    submission_df.to_csv(output_file, index=False)\n",
    "    print(f\"Submission saved to {output_file}\")\n",
    "    return submission_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Load data\n",
    "    train_path = 'train_processed.csv'\n",
    "    test_path = 'test_processed.csv'\n",
    "    X_train, y_train, X_test, test_ids = load_data(train_path, test_path)\n",
    "    \n",
    "    # Get categorical features\n",
    "    categorical_features = get_categorical_features(X_train)\n",
    "    \n",
    "    # Define paths to pretrained models\n",
    "    model_paths = {\n",
    "        'catboost': 'models_stacking/stacking_catboost_model.cbm',\n",
    "        'xgboost': 'models_stacking/stacking_xgboost_model.json',\n",
    "        #'lightgbm': 'models_stacking/stacking_lightgbm_model.txt',\n",
    "        #'mlp': 'models_stacking/stacking_mlp_model.pkl',\n",
    "        #'torch_mlp': 'models_stacking/mlp_torch_model.pt'\n",
    "\n",
    "        \n",
    "    }\n",
    "    \n",
    "    # Load models\n",
    "    models = load_models(model_paths)\n",
    "    \n",
    "    # Generate level 1 features\n",
    "    level1_train, level1_test = generate_level1_features(\n",
    "        models, X_train, X_test, y_train, categorical_features\n",
    "    )\n",
    "    \n",
    "    # Train meta-model\n",
    "    meta_model, test_preds = train_meta_model(level1_train, y_train, level1_test)\n",
    "    \n",
    "    # Create submission file\n",
    "    create_submission(test_ids, test_preds, 'submissions_final_stacking.csv')\n",
    "    \n",
    "    # Save meta-model\n",
    "    joblib.dump(meta_model, 'results_stacking/meta_model.pkl')\n",
    "    print(\"Meta-model saved to results_stacking/meta_model.pkl\")\n",
    "    \n",
    "    # End timing\n",
    "    end_time = time.time()\n",
    "    print(f\"Total execution time: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STACKING MULTICAPA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Create results directory\n",
    "os.makedirs('results_stacking', exist_ok=True)\n",
    "\n",
    "# Function to load data\n",
    "def load_data(train_path, test_path):\n",
    "    \"\"\"\n",
    "    Load training and test data\n",
    "    \"\"\"\n",
    "    train_df = pd.read_csv(train_path)\n",
    "    test_df = pd.read_csv(test_path)\n",
    "    \n",
    "    # For training data\n",
    "    X_train = train_df.drop(['prezo_euros', 'id'], axis=1, errors='ignore')\n",
    "    y_train = train_df['prezo_euros']\n",
    "    \n",
    "    # For test data\n",
    "    if 'Unnamed: 0' in test_df.columns:\n",
    "        X_test = test_df.drop(['id', 'Unnamed: 0'], axis=1, errors='ignore')\n",
    "    else:\n",
    "        X_test = test_df.drop(['id'], axis=1, errors='ignore')\n",
    "    test_ids = test_df['id']\n",
    "    \n",
    "    print(f\"Training data shape: {X_train.shape}\")\n",
    "    print(f\"Test data shape: {X_test.shape}\")\n",
    "    return X_train, y_train, X_test, test_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to identify categorical features\n",
    "def get_categorical_features(df):\n",
    "    \"\"\"\n",
    "    Identify categorical features in the dataset\n",
    "    \"\"\"\n",
    "    categorical_features = []\n",
    "    for col in df.columns:\n",
    "        if (df[col].dtype == 'object' or\n",
    "            col in ['tipo_edificacion', 'calidade_materiais',\n",
    "                   'cor_favorita_propietario', 'acceso_transporte_publico',\n",
    "                   'orientacion', 'eficiencia_enerxetica'] or\n",
    "            'tipo_' in col or 'color_' in col):\n",
    "            categorical_features.append(col)\n",
    "    print(f\"Categorical features: {categorical_features}\")\n",
    "    return categorical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load pretrained models\n",
    "def load_models(model_paths):\n",
    "    \"\"\"\n",
    "    Load pretrained models from specified paths\n",
    "    \"\"\"\n",
    "    models = {}\n",
    "    \n",
    "    # Load CatBoost model\n",
    "    if 'catboost' in model_paths and os.path.exists(model_paths['catboost']):\n",
    "        print(\"Loading CatBoost model...\")\n",
    "        models['catboost'] = CatBoostRegressor()\n",
    "        models['catboost'].load_model(model_paths['catboost'])\n",
    "    \n",
    "    # Load XGBoost model\n",
    "    if 'xgboost' in model_paths and os.path.exists(model_paths['xgboost']):\n",
    "        print(\"Loading XGBoost model...\")\n",
    "        models['xgboost'] = XGBRegressor()\n",
    "        models['xgboost'].load_model(model_paths['xgboost'])\n",
    "    \n",
    "    # Load LightGBM model\n",
    "    if 'lightgbm' in model_paths and os.path.exists(model_paths['lightgbm']):\n",
    "        print(\"Loading LightGBM model...\")\n",
    "        models['lightgbm'] = lgb.Booster(model_file=model_paths['lightgbm'])\n",
    "    \n",
    "    # Load MLP model (which includes the scaler)\n",
    "    if 'mlp' in model_paths and os.path.exists(model_paths['mlp']):\n",
    "        print(\"Loading MLP model...\")\n",
    "        models['mlp'] = joblib.load(model_paths['mlp'])\n",
    "    \n",
    "    print(f\"Successfully loaded {len(models)} models\")\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate level 1 features with enhanced cross-validation\n",
    "def generate_level1_features(models, X_train, X_test, y_train, categorical_features=None, n_folds=10):\n",
    "    \"\"\"\n",
    "    Generate level 1 training features for meta-model using cross-validation\n",
    "    \"\"\"\n",
    "    print(\"Generating level 1 features for stacking...\")\n",
    "    \n",
    "    # For training set, use k-fold cross-validation to avoid data leakage\n",
    "    train_preds_df = pd.DataFrame(index=range(X_train.shape[0]))\n",
    "    test_preds_all = {}\n",
    "    \n",
    "    # Create KFold object for stratified cross-validation\n",
    "    kf = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(X_train)):\n",
    "        print(f\"Processing fold {fold+1}/{n_folds}\")\n",
    "        \n",
    "        # Split data for this fold\n",
    "        X_train_fold = X_train.iloc[train_idx]\n",
    "        y_train_fold = y_train.iloc[train_idx]\n",
    "        X_val_fold = X_train.iloc[val_idx]\n",
    "        \n",
    "        # Generate predictions for this fold\n",
    "        fold_preds = pd.DataFrame(index=val_idx)\n",
    "        test_fold_preds = {}\n",
    "        \n",
    "        # Train and predict with CatBoost\n",
    "        if 'catboost' in models:\n",
    "            print(\"Training fold with CatBoost...\")\n",
    "            if categorical_features:\n",
    "                train_pool = Pool(X_train_fold, y_train_fold, cat_features=categorical_features)\n",
    "                val_pool = Pool(X_val_fold, cat_features=categorical_features)\n",
    "                test_pool = Pool(X_test, cat_features=categorical_features)\n",
    "                \n",
    "                temp_model = CatBoostRegressor()\n",
    "                temp_model.set_params(**models['catboost'].get_params())\n",
    "                temp_model.fit(train_pool, verbose=False)\n",
    "                fold_preds['catboost'] = temp_model.predict(val_pool)\n",
    "                test_fold_preds['catboost'] = temp_model.predict(test_pool)\n",
    "            else:\n",
    "                temp_model = CatBoostRegressor()\n",
    "                temp_model.set_params(**models['catboost'].get_params())\n",
    "                temp_model.fit(X_train_fold, y_train_fold, verbose=False)\n",
    "                fold_preds['catboost'] = temp_model.predict(X_val_fold)\n",
    "                test_fold_preds['catboost'] = temp_model.predict(X_test)\n",
    "        \n",
    "        # Train and predict with XGBoost\n",
    "        if 'xgboost' in models:\n",
    "            print(\"Training fold with XGBoost...\")\n",
    "            temp_model = XGBRegressor()\n",
    "            temp_model.set_params(**models['xgboost'].get_params())\n",
    "            temp_model.fit(X_train_fold, y_train_fold, verbose=False)\n",
    "            fold_preds['xgboost'] = temp_model.predict(X_val_fold)\n",
    "            test_fold_preds['xgboost'] = temp_model.predict(X_test)\n",
    "        \n",
    "        # Train and predict with LightGBM\n",
    "        if 'lightgbm' in models:\n",
    "            print(\"Training fold with LightGBM...\")\n",
    "            train_lgb = lgb.Dataset(X_train_fold, label=y_train_fold)\n",
    "            params = models['lightgbm'].params if hasattr(models['lightgbm'], 'params') else {}\n",
    "            temp_model = lgb.train(params, train_lgb, num_boost_round=100)\n",
    "            fold_preds['lightgbm'] = temp_model.predict(X_val_fold)\n",
    "            test_fold_preds['lightgbm'] = temp_model.predict(X_test)\n",
    "        \n",
    "        # Train and predict with MLP\n",
    "        if 'mlp' in models:\n",
    "            print(\"Training fold with MLP...\")\n",
    "            # Extract parameters from loaded model\n",
    "            mlp_params = models['mlp']['model'].get_params()\n",
    "            \n",
    "            # Create and fit scaler\n",
    "            scaler = StandardScaler()\n",
    "            X_train_fold_scaled = scaler.fit_transform(X_train_fold)\n",
    "            X_val_fold_scaled = scaler.transform(X_val_fold)\n",
    "            X_test_scaled = scaler.transform(X_test)\n",
    "            \n",
    "            # Create and train MLP model\n",
    "            temp_model = MLPRegressor(**mlp_params)\n",
    "            temp_model.fit(X_train_fold_scaled, y_train_fold)\n",
    "            \n",
    "            fold_preds['mlp'] = temp_model.predict(X_val_fold_scaled)\n",
    "            test_fold_preds['mlp'] = temp_model.predict(X_test_scaled)\n",
    "        \n",
    "        # Add fold predictions to overall predictions DataFrame\n",
    "        for model_name in fold_preds.columns:\n",
    "            train_preds_df.loc[val_idx, model_name] = fold_preds[model_name].values\n",
    "            \n",
    "            if model_name not in test_preds_all:\n",
    "                test_preds_all[model_name] = []\n",
    "            test_preds_all[model_name].append(test_fold_preds[model_name])\n",
    "    \n",
    "    # Create test predictions by averaging fold predictions\n",
    "    test_preds_df = pd.DataFrame()\n",
    "    for model_name, preds_list in test_preds_all.items():\n",
    "        test_preds_df[model_name] = np.mean(preds_list, axis=0)\n",
    "    \n",
    "    # Add prediction statistics as features\n",
    "    # These additional features can help the meta-model capture more pattern\n",
    "    for model_name in train_preds_df.columns:\n",
    "        # Add squared predictions\n",
    "        train_preds_df[f\"{model_name}_squared\"] = train_preds_df[model_name] ** 2\n",
    "        test_preds_df[f\"{model_name}_squared\"] = test_preds_df[model_name] ** 2\n",
    "        \n",
    "        # Add log predictions (with safeguard for negative values)\n",
    "        train_preds_df[f\"{model_name}_log\"] = np.log1p(np.maximum(0, train_preds_df[model_name]))\n",
    "        test_preds_df[f\"{model_name}_log\"] = np.log1p(np.maximum(0, test_preds_df[model_name]))\n",
    "    \n",
    "    # Add interactions between models\n",
    "    model_names = [col for col in train_preds_df.columns if '_squared' not in col and '_log' not in col]\n",
    "    for i, model_i in enumerate(model_names):\n",
    "        for j, model_j in enumerate(model_names):\n",
    "            if i < j:  # To avoid duplicates\n",
    "                train_preds_df[f\"{model_i}_x_{model_j}\"] = train_preds_df[model_i] * train_preds_df[model_j]\n",
    "                test_preds_df[f\"{model_i}_x_{model_j}\"] = test_preds_df[model_i] * test_preds_df[model_j]\n",
    "                \n",
    "                # Add differences between models\n",
    "                train_preds_df[f\"{model_i}_minus_{model_j}\"] = train_preds_df[model_i] - train_preds_df[model_j]\n",
    "                test_preds_df[f\"{model_i}_minus_{model_j}\"] = test_preds_df[model_i] - test_preds_df[model_j]\n",
    "                \n",
    "                # Add ratios between models (with safeguard for division by zero)\n",
    "                train_preds_df[f\"{model_i}_div_{model_j}\"] = train_preds_df[model_i] / (train_preds_df[model_j] + 1e-5)\n",
    "                test_preds_df[f\"{model_i}_div_{model_j}\"] = test_preds_df[model_i] / (test_preds_df[model_j] + 1e-5)\n",
    "    \n",
    "    # Calculate statistics across models\n",
    "    train_preds_df['mean_pred'] = train_preds_df[model_names].mean(axis=1)\n",
    "    train_preds_df['median_pred'] = train_preds_df[model_names].median(axis=1)\n",
    "    train_preds_df['max_pred'] = train_preds_df[model_names].max(axis=1)\n",
    "    train_preds_df['min_pred'] = train_preds_df[model_names].min(axis=1)\n",
    "    train_preds_df['std_pred'] = train_preds_df[model_names].std(axis=1)\n",
    "    \n",
    "    test_preds_df['mean_pred'] = test_preds_df[model_names].mean(axis=1)\n",
    "    test_preds_df['median_pred'] = test_preds_df[model_names].median(axis=1)\n",
    "    test_preds_df['max_pred'] = test_preds_df[model_names].max(axis=1)\n",
    "    test_preds_df['min_pred'] = test_preds_df[model_names].min(axis=1)\n",
    "    test_preds_df['std_pred'] = test_preds_df[model_names].std(axis=1)\n",
    "    \n",
    "    print(f\"Generated level 1 features with shape: {train_preds_df.shape} (train), {test_preds_df.shape} (test)\")\n",
    "    return train_preds_df, test_preds_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_advanced_meta_model(level1_train, y_train, level1_test):\n",
    "    \"\"\"\n",
    "    Entrena un conjunto de meta-modelos usando un enfoque de stacking multi-nivel,\n",
    "    optimizando para minimizar el MAE.\n",
    "    \"\"\"\n",
    "    print(\"Entrenando meta-modelos avanzados para stacking...\")\n",
    "    \n",
    "    # Etapa 1: Crear varios meta-modelos de nivel 1\n",
    "    \n",
    "    # ElasticNet\n",
    "    print(\"Optimizando ElasticNet...\")\n",
    "    param_grid_elastic = {\n",
    "        'alpha': [0.0001, 0.001, 0.01, 0.1, 0.5, 1.0],\n",
    "        'l1_ratio': [0.1, 0.3, 0.5, 0.7, 0.9, 0.95, 0.99]\n",
    "    }\n",
    "    elastic_grid = GridSearchCV(\n",
    "        ElasticNet(max_iter=10000, random_state=42),\n",
    "        param_grid_elastic,\n",
    "        scoring='neg_mean_absolute_error',\n",
    "        cv=5,\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    elastic_grid.fit(level1_train, y_train)\n",
    "    elastic_model = elastic_grid.best_estimator_\n",
    "    print(f\"Mejores parámetros ElasticNet: {elastic_grid.best_params_}\")\n",
    "    \n",
    "    # Ridge Regression\n",
    "    print(\"Optimizando Ridge...\")\n",
    "    param_grid_ridge = {\n",
    "        'alpha': [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]\n",
    "    }\n",
    "    ridge_grid = GridSearchCV(\n",
    "        Ridge(max_iter=10000, random_state=42),\n",
    "        param_grid_ridge,\n",
    "        scoring='neg_mean_absolute_error',\n",
    "        cv=5,\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    ridge_grid.fit(level1_train, y_train)\n",
    "    ridge_model = ridge_grid.best_estimator_\n",
    "    print(f\"Mejores parámetros Ridge: {ridge_grid.best_params_}\")\n",
    "    \n",
    "    # Lasso Regression\n",
    "    print(\"Optimizando Lasso...\")\n",
    "    param_grid_lasso = {\n",
    "        'alpha': [0.0001, 0.001, 0.01, 0.1, 0.5, 1.0]\n",
    "    }\n",
    "    lasso_grid = GridSearchCV(\n",
    "        Lasso(max_iter=10000, random_state=42),\n",
    "        param_grid_lasso,\n",
    "        scoring='neg_mean_absolute_error',\n",
    "        cv=5,\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    lasso_grid.fit(level1_train, y_train)\n",
    "    lasso_model = lasso_grid.best_estimator_\n",
    "    print(f\"Mejores parámetros Lasso: {lasso_grid.best_params_}\")\n",
    "    \n",
    "    # Support Vector Regression\n",
    "    print(\"Optimizando SVR...\")\n",
    "    param_grid_svr = {\n",
    "        'C': [0.1, 1.0, 10.0],\n",
    "        'gamma': ['scale', 'auto'],\n",
    "        'epsilon': [0.01, 0.1, 0.2]\n",
    "    }\n",
    "    pipeline_svr = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('svr', SVR())\n",
    "    ])\n",
    "    svr_grid = GridSearchCV(\n",
    "        pipeline_svr,\n",
    "        {'svr__' + key: value for key, value in param_grid_svr.items()},\n",
    "        scoring='neg_mean_absolute_error',\n",
    "        cv=5,\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    svr_grid.fit(level1_train, y_train)\n",
    "    svr_model = svr_grid.best_estimator_\n",
    "    print(f\"Mejores parámetros SVR: {svr_grid.best_params_}\")\n",
    "    \n",
    "    \"\"\"\n",
    "    # Kernel Ridge Regression\n",
    "    print(\"Optimizando Kernel Ridge...\")\n",
    "    param_grid_kr = {\n",
    "        'alpha': [0.001, 0.01, 0.1, 1.0],\n",
    "        'kernel': ['linear', 'rbf'],\n",
    "        'gamma': [0.001, 0.01, 0.1, 1.0]\n",
    "    }\n",
    "    pipeline_kr = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('kr', KernelRidge())\n",
    "    ])\n",
    "    kr_grid = GridSearchCV(\n",
    "        pipeline_kr,\n",
    "        {'kr__' + key: value for key, value in param_grid_kr.items()},\n",
    "        scoring='neg_mean_absolute_error',\n",
    "        cv=5,\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    kr_grid.fit(level1_train, y_train)\n",
    "    kr_model = kr_grid.best_estimator_\n",
    "    print(f\"Mejores parámetros KernelRidge: {kr_grid.best_params_}\")\n",
    "    \"\"\"\n",
    "    # Random Forest\n",
    "    print(\"Optimizando Random Forest...\")\n",
    "    param_grid_rf = {\n",
    "        'n_estimators': [100, 200],\n",
    "        'max_depth': [None, 10, 20],\n",
    "        'min_samples_split': [2, 5, 10]\n",
    "    }\n",
    "    rf_grid = GridSearchCV(\n",
    "        RandomForestRegressor(random_state=42, n_jobs=-1),\n",
    "        param_grid_rf,\n",
    "        scoring='neg_mean_absolute_error',\n",
    "        cv=5,\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    rf_grid.fit(level1_train, y_train)\n",
    "    rf_model = rf_grid.best_estimator_\n",
    "    print(f\"Mejores parámetros RF: {rf_grid.best_params_}\")\n",
    "    \n",
    "    # Gradient Boosting\n",
    "    print(\"Optimizando Gradient Boosting...\")\n",
    "    param_grid_gb = {\n",
    "        'n_estimators': [100, 200],\n",
    "        'learning_rate': [0.01, 0.05, 0.1],\n",
    "        'max_depth': [3, 5, 7]\n",
    "    }\n",
    "    gb_grid = GridSearchCV(\n",
    "        GradientBoostingRegressor(random_state=42),\n",
    "        param_grid_gb,\n",
    "        scoring='neg_mean_absolute_error',\n",
    "        cv=5,\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    gb_grid.fit(level1_train, y_train)\n",
    "    gb_model = gb_grid.best_estimator_\n",
    "    print(f\"Mejores parámetros GB: {gb_grid.best_params_}\")\n",
    "    \n",
    "    # Etapa 2: Evaluación de los modelos de nivel 1\n",
    "    print(\"\\nEvaluando modelos de nivel 1...\")\n",
    "    X_tr, X_val, y_tr, y_val = train_test_split(level1_train, y_train, test_size=0.2, random_state=42)\n",
    "    \n",
    "    models_level1 = {\n",
    "        'ElasticNet': elastic_model,\n",
    "        'Ridge': ridge_model,\n",
    "        'Lasso': lasso_model,\n",
    "        'SVR': svr_model,\n",
    "        #'KernelRidge': kr_model,\n",
    "        'RandomForest': rf_model,\n",
    "        'GradientBoosting': gb_model\n",
    "    }\n",
    "    \n",
    "    val_scores = {}\n",
    "    for name, model in models_level1.items():\n",
    "        model.fit(X_tr, y_tr)\n",
    "        preds = model.predict(X_val)\n",
    "        mae = mean_absolute_error(y_val, preds)\n",
    "        val_scores[name] = -mae  # Negative MAE for consistency with GridSearchCV scoring\n",
    "        print(f\"{name} - Validation MAE: {mae:.4f}\")\n",
    "    \n",
    "    # Etapa 3: Creación del meta-modelo final usando ensemble (stacking + blending)\n",
    "    print(\"\\nCreando meta-modelo final...\")\n",
    "    \n",
    "    # Opción 1: StackingRegressor\n",
    "    # Selecciona los mejores modelos base según su rendimiento en validación\n",
    "    best_models = sorted(val_scores.items(), key=lambda x: x[1], reverse=True)[:5]  # Top 5 models\n",
    "    print(f\"Mejores modelos seleccionados para stacking: {[model[0] for model in best_models]}\")\n",
    "    \n",
    "    # Crea el StackingRegressor\n",
    "    estimators = [(name, models_level1[name]) for name, _ in best_models]\n",
    "    stacking_regressor = StackingRegressor(\n",
    "        estimators=estimators,\n",
    "        final_estimator=GradientBoostingRegressor(n_estimators=100, learning_rate=0.05, max_depth=3, random_state=42),\n",
    "        cv=5,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    \n",
    "    # Opción 2: Voting Regressor con pesos optimizados\n",
    "    # Pesos proporcionales al rendimiento en validación\n",
    "    weights = [score for _, score in best_models]\n",
    "    weights = np.array(weights) / sum(weights)\n",
    "    voting_regressor = VotingRegressor(\n",
    "        estimators=estimators,\n",
    "        weights=weights\n",
    "    )\n",
    "    \n",
    "    # Entrenar modelos de ensamble\n",
    "    print(\"Entrenando StackingRegressor...\")\n",
    "    stacking_regressor.fit(level1_train, y_train)\n",
    "    \n",
    "    print(\"Entrenando VotingRegressor...\")\n",
    "    voting_regressor.fit(level1_train, y_train)\n",
    "    \n",
    "    # Crear un meta-meta-modelo que combine los dos enfoques anteriores\n",
    "    print(\"Creando meta-meta-modelo...\")\n",
    "    # Genera predicciones de los modelos de ensemble\n",
    "    stacking_preds = stacking_regressor.predict(level1_train)\n",
    "    voting_preds = voting_regressor.predict(level1_train)\n",
    "    \n",
    "    # Crea un DataFrame con las predicciones como características\n",
    "    meta_meta_train = pd.DataFrame({\n",
    "        'stacking_pred': stacking_preds,\n",
    "        'voting_pred': voting_preds\n",
    "    })\n",
    "    \n",
    "    # Entrena un modelo final simple\n",
    "    final_model = ElasticNet(alpha=0.01, l1_ratio=0.5, max_iter=10000, random_state=42)\n",
    "    final_model.fit(meta_meta_train, y_train)\n",
    "    \n",
    "    # Genera predicciones para los datos de test\n",
    "    stacking_test_preds = stacking_regressor.predict(level1_test)\n",
    "    voting_test_preds = voting_regressor.predict(level1_test)\n",
    "    meta_meta_test = pd.DataFrame({\n",
    "        'stacking_pred': stacking_test_preds,\n",
    "        'voting_pred': voting_test_preds\n",
    "    })\n",
    "    final_test_preds = final_model.predict(meta_meta_test)\n",
    "    \n",
    "    # Evaluar el modelo final en una partición de validación\n",
    "    X_tr, X_val, y_tr, y_val = train_test_split(level1_train, y_train, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Entrena el modelo final en el conjunto de entrenamiento\n",
    "    stacking_tr_preds = stacking_regressor.predict(X_tr)\n",
    "    voting_tr_preds = voting_regressor.predict(X_tr)\n",
    "    meta_meta_tr = pd.DataFrame({\n",
    "        'stacking_pred': stacking_tr_preds,\n",
    "        'voting_pred': voting_tr_preds\n",
    "    })\n",
    "    final_model.fit(meta_meta_tr, y_tr)\n",
    "    \n",
    "    # Evalúa en el conjunto de validación\n",
    "    stacking_val_preds = stacking_regressor.predict(X_val)\n",
    "    voting_val_preds = voting_regressor.predict(X_val)\n",
    "    meta_meta_val = pd.DataFrame({\n",
    "        'stacking_pred': stacking_val_preds,\n",
    "        'voting_pred': voting_val_preds\n",
    "    })\n",
    "    final_val_preds = final_model.predict(meta_meta_val)\n",
    "    \n",
    "    val_mae = mean_absolute_error(y_val, final_val_preds)\n",
    "    val_rmse = np.sqrt(mean_squared_error(y_val, final_val_preds))\n",
    "    val_r2 = r2_score(y_val, final_val_preds)\n",
    "    \n",
    "    print(\"\\nRendimiento del meta-modelo final:\")\n",
    "    print(f\"  MAE : {val_mae:.4f}\")\n",
    "    print(f\"  RMSE: {val_rmse:.4f}\")\n",
    "    print(f\"  R²  : {val_r2:.4f}\")\n",
    "    \n",
    "    # Guarda todos los modelos\n",
    "    print(\"\\nGuardando modelos...\")\n",
    "    model_dict = {\n",
    "        'level1_models': models_level1,\n",
    "        'stacking_regressor': stacking_regressor,\n",
    "        'voting_regressor': voting_regressor,\n",
    "        'final_model': final_model\n",
    "    }\n",
    "    joblib.dump(model_dict, 'results_stacking/advanced_meta_models.pkl')\n",
    "    \n",
    "    return final_model, final_test_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create submission file\n",
    "def create_submission(test_ids, predictions, output_file):\n",
    "    \"\"\"\n",
    "    Create submission file with predictions\n",
    "    \"\"\"\n",
    "    submission_df = pd.DataFrame({\n",
    "        'id': test_ids,\n",
    "        'prezo_euros': predictions\n",
    "    })\n",
    "    \n",
    "    submission_df.to_csv(output_file, index=False)\n",
    "    print(f\"Submission saved to {output_file}\")\n",
    "    return submission_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Load data\n",
    "    train_path = 'train_processed.csv'\n",
    "    test_path = 'test_processed.csv'\n",
    "    X_train, y_train, X_test, test_ids = load_data(train_path, test_path)\n",
    "    \n",
    "    # Get categorical features\n",
    "    categorical_features = gal_features(X_train)\n",
    "    \n",
    "    # Define paths to pretrained models\n",
    "    model_paths = {\n",
    "        'catboost': 'models_stacking/stacking_catboost_model.cbm',\n",
    "        'xgboost': 'models_stacking/stacking_xgboost_model.json',\n",
    "        # Uncomment if you have these models\n",
    "        #'lightgbm': 'models_stacking/stacking_lightgbm_model.txt',\n",
    "        #'mlp': 'models_stacking/stacking_mlp_model.pkl'\n",
    "    }\n",
    "    \n",
    "    # Load models\n",
    "    models = load_models(model_paths)\n",
    "    \n",
    "    # Generate level 1 features\n",
    "    level1_train, level1_test = generate_level1_features(\n",
    "        models, X_train, X_test, y_train, categorical_features, n_folds=10\n",
    "    )\n",
    "    \n",
    "    # Train advanced meta-model\n",
    "    meta_model, test_preds = train_advanced_meta_model(level1_train, y_train, level1_test)\n",
    "    \n",
    "    # Create submission file\n",
    "    submission_df = create_submission(test_ids, test_preds, 'submissions_advanced_stacking.csv')\n",
    "    \n",
    "    # End timing\n",
    "    end_time = time.time()\n",
    "    print(f\"Total execution time: {end_time - start_time:.2f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
