{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/netapp2/Store_uni/home/usc/ci/avs/personal/aprendizaje/p4/venvP4/lib/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----------------------------\n",
    "# 1. Configuración y semillas\n",
    "# ----------------------------\n",
    "DATA_PATH    = 'train.csv'\n",
    "TEST_PATH    = 'test.csv'\n",
    "MODEL_DIR    = 'models_nn'\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "random.seed(RANDOM_STATE)\n",
    "torch.manual_seed(RANDOM_STATE)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(RANDOM_STATE)\n",
    "\n",
    "DEVICE       = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "BATCH_SIZE   = 256\n",
    "MAX_EPOCHS   = 200\n",
    "PATIENCE     = 50\n",
    "LEARNING_RATE= 1e-3\n",
    "TEST_SIZE    = 0.1\n",
    "SUBMIT_FILE  = os.path.join(MODEL_DIR, 'submission_nn_preprocessed_data.csv')\n",
    "\n",
    "# ----------------------------\n",
    "# 2. Carga y preprocessado train\n",
    "# ----------------------------\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "y  = df['prezo_euros'].values\n",
    "X  = df.drop(columns=['prezo_euros']).copy()\n",
    "\n",
    "# 2.1 Columnas numéricas y categóricas\n",
    "num_cols = X.select_dtypes(include=['float64','int64']).columns.tolist()\n",
    "cat_cols = X.select_dtypes(include=['object']).columns.tolist()\n",
    "\n",
    "# 2.2 Imputación\n",
    "for col in num_cols:\n",
    "    med = X[col].median()\n",
    "    X[col].fillna(med, inplace=True)\n",
    "for col in cat_cols:\n",
    "    X[col] = X[col].fillna('Missing').astype(str)\n",
    "\n",
    "# 2.3 One-hot encoding\n",
    "X_enc      = pd.get_dummies(X, columns=cat_cols, drop_first=True)\n",
    "FEATURES   = X_enc.columns.tolist()\n",
    "\n",
    "# 2.4 Escalado numérico\n",
    "scaler     = StandardScaler()\n",
    "X_enc[num_cols] = scaler.fit_transform(X_enc[num_cols])\n",
    "\n",
    "# 2.5 Train/val split\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_enc.values, y,\n",
    "    test_size=TEST_SIZE,\n",
    "    random_state=RANDOM_STATE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----------------------------\n",
    "# 3. Dataset & DataLoader\n",
    "# ----------------------------\n",
    "class HousePriceDataset(Dataset):\n",
    "    def __init__(self, features, targets=None):\n",
    "        self.X = torch.from_numpy(features).float()\n",
    "        self.y = torch.from_numpy(targets).float() if targets is not None else None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.y is None:\n",
    "            return self.X[idx]\n",
    "        return self.X[idx], self.y[idx]\n",
    "\n",
    "train_ds    = HousePriceDataset(X_train, y_train)\n",
    "val_ds      = HousePriceDataset(X_val,   y_val)\n",
    "train_loader= DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True,  num_workers=2, pin_memory=True)\n",
    "val_loader  = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 4. Definición del modelo (arquitectura aumentada)\n",
    "# ----------------------------\n",
    "class RegressionNNBig(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Linear(input_dim, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.4),\n",
    "\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "\n",
    "            nn.Linear(32, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x).squeeze(1)\n",
    "\n",
    "# Instanciación\n",
    "model = RegressionNNBig(X_train.shape[1]).to(DEVICE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------\n",
    "# 5. Optimizer y loss\n",
    "# ----------------------------\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.MSELoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 \u0014 Val RMSE: 103615.67\n",
      "Epoch 002 \u0014 Val RMSE: 70316.78\n",
      "Epoch 003 \u0014 Val RMSE: 66032.06\n",
      "Epoch 004 \u0014 Val RMSE: 62871.04\n",
      "Epoch 005 \u0014 Val RMSE: 60479.87\n",
      "Epoch 006 \u0014 Val RMSE: 60137.95\n",
      "Epoch 007 \u0014 Val RMSE: 58251.78\n",
      "Epoch 008 \u0014 Val RMSE: 59974.90\n",
      "Epoch 009 \u0014 Val RMSE: 58992.83\n",
      "Epoch 010 \u0014 Val RMSE: 56004.72\n",
      "Epoch 011 \u0014 Val RMSE: 55302.87\n",
      "Epoch 012 \u0014 Val RMSE: 55130.22\n",
      "Epoch 013 \u0014 Val RMSE: 54375.40\n",
      "Epoch 014 \u0014 Val RMSE: 54873.35\n",
      "Epoch 015 \u0014 Val RMSE: 54176.80\n",
      "Epoch 016 \u0014 Val RMSE: 55071.05\n",
      "Epoch 017 \u0014 Val RMSE: 52998.12\n",
      "Epoch 018 \u0014 Val RMSE: 52931.83\n",
      "Epoch 019 \u0014 Val RMSE: 52630.81\n",
      "Epoch 020 \u0014 Val RMSE: 53332.57\n",
      "Epoch 021 \u0014 Val RMSE: 51374.77\n",
      "Epoch 022 \u0014 Val RMSE: 51167.92\n",
      "Epoch 023 \u0014 Val RMSE: 51078.95\n",
      "Epoch 024 \u0014 Val RMSE: 50863.62\n",
      "Epoch 025 \u0014 Val RMSE: 52956.92\n",
      "Epoch 026 \u0014 Val RMSE: 50031.53\n",
      "Epoch 027 \u0014 Val RMSE: 49899.88\n",
      "Epoch 028 \u0014 Val RMSE: 50090.94\n",
      "Epoch 029 \u0014 Val RMSE: 50137.43\n",
      "Epoch 030 \u0014 Val RMSE: 50029.06\n",
      "Epoch 031 \u0014 Val RMSE: 48601.99\n",
      "Epoch 032 \u0014 Val RMSE: 48453.41\n",
      "Epoch 033 \u0014 Val RMSE: 54945.56\n",
      "Epoch 034 \u0014 Val RMSE: 48392.55\n",
      "Epoch 035 \u0014 Val RMSE: 49380.00\n",
      "Epoch 036 \u0014 Val RMSE: 48066.32\n",
      "Epoch 037 \u0014 Val RMSE: 46823.15\n",
      "Epoch 038 \u0014 Val RMSE: 48145.30\n",
      "Epoch 039 \u0014 Val RMSE: 48855.85\n",
      "Epoch 040 \u0014 Val RMSE: 50296.18\n",
      "Epoch 041 \u0014 Val RMSE: 46490.47\n",
      "Epoch 042 \u0014 Val RMSE: 46618.25\n",
      "Epoch 043 \u0014 Val RMSE: 45545.95\n",
      "Epoch 044 \u0014 Val RMSE: 45844.38\n",
      "Epoch 045 \u0014 Val RMSE: 49681.28\n",
      "Epoch 046 \u0014 Val RMSE: 47217.36\n",
      "Epoch 047 \u0014 Val RMSE: 52835.14\n",
      "Epoch 048 \u0014 Val RMSE: 46265.11\n",
      "Epoch 049 \u0014 Val RMSE: 45503.95\n",
      "Epoch 050 \u0014 Val RMSE: 45649.65\n",
      "Epoch 051 \u0014 Val RMSE: 45061.36\n",
      "Epoch 052 \u0014 Val RMSE: 45781.13\n",
      "Epoch 053 \u0014 Val RMSE: 45201.69\n",
      "Epoch 054 \u0014 Val RMSE: 45011.66\n",
      "Epoch 055 \u0014 Val RMSE: 44797.93\n",
      "Epoch 056 \u0014 Val RMSE: 44835.53\n",
      "Epoch 057 \u0014 Val RMSE: 45335.29\n",
      "Epoch 058 \u0014 Val RMSE: 46869.10\n",
      "Epoch 059 \u0014 Val RMSE: 44664.50\n",
      "Epoch 060 \u0014 Val RMSE: 45277.52\n",
      "Epoch 061 \u0014 Val RMSE: 44758.53\n",
      "Epoch 062 \u0014 Val RMSE: 45058.88\n",
      "Epoch 063 \u0014 Val RMSE: 44485.16\n",
      "Epoch 064 \u0014 Val RMSE: 46042.02\n",
      "Epoch 065 \u0014 Val RMSE: 48597.80\n",
      "Epoch 066 \u0014 Val RMSE: 46283.98\n",
      "Epoch 067 \u0014 Val RMSE: 46178.28\n",
      "Epoch 068 \u0014 Val RMSE: 50114.07\n",
      "Epoch 069 \u0014 Val RMSE: 45208.41\n",
      "Epoch 070 \u0014 Val RMSE: 44788.30\n",
      "Epoch 071 \u0014 Val RMSE: 44607.19\n",
      "Epoch 072 \u0014 Val RMSE: 45043.87\n",
      "Epoch 073 \u0014 Val RMSE: 44432.80\n",
      "Epoch 074 \u0014 Val RMSE: 45520.97\n",
      "Epoch 075 \u0014 Val RMSE: 44416.86\n",
      "Epoch 076 \u0014 Val RMSE: 45841.91\n",
      "Epoch 077 \u0014 Val RMSE: 44343.14\n",
      "Epoch 078 \u0014 Val RMSE: 51786.20\n",
      "Epoch 079 \u0014 Val RMSE: 44152.51\n",
      "Epoch 080 \u0014 Val RMSE: 46185.77\n",
      "Epoch 081 \u0014 Val RMSE: 45484.89\n",
      "Epoch 082 \u0014 Val RMSE: 44498.83\n",
      "Epoch 083 \u0014 Val RMSE: 44494.85\n",
      "Epoch 084 \u0014 Val RMSE: 50402.95\n",
      "Epoch 085 \u0014 Val RMSE: 46282.57\n",
      "Epoch 086 \u0014 Val RMSE: 44465.34\n",
      "Epoch 087 \u0014 Val RMSE: 48228.14\n",
      "Epoch 088 \u0014 Val RMSE: 44149.29\n",
      "Epoch 089 \u0014 Val RMSE: 49935.59\n",
      "Epoch 090 \u0014 Val RMSE: 44132.76\n",
      "Epoch 091 \u0014 Val RMSE: 44567.00\n",
      "Epoch 092 \u0014 Val RMSE: 45568.28\n",
      "Epoch 093 \u0014 Val RMSE: 45554.16\n",
      "Epoch 094 \u0014 Val RMSE: 46085.62\n",
      "Epoch 095 \u0014 Val RMSE: 46324.64\n",
      "Epoch 096 \u0014 Val RMSE: 47038.83\n",
      "Epoch 097 \u0014 Val RMSE: 44334.19\n",
      "Epoch 098 \u0014 Val RMSE: 46669.19\n",
      "Epoch 099 \u0014 Val RMSE: 43906.64\n",
      "Epoch 100 \u0014 Val RMSE: 44276.81\n",
      "Epoch 101 \u0014 Val RMSE: 44166.83\n",
      "Epoch 102 \u0014 Val RMSE: 43912.30\n",
      "Epoch 103 \u0014 Val RMSE: 47423.89\n",
      "Epoch 104 \u0014 Val RMSE: 45206.28\n",
      "Epoch 105 \u0014 Val RMSE: 44515.72\n",
      "Epoch 106 \u0014 Val RMSE: 43804.23\n",
      "Epoch 107 \u0014 Val RMSE: 44458.45\n",
      "Epoch 108 \u0014 Val RMSE: 43930.51\n",
      "Epoch 109 \u0014 Val RMSE: 47381.56\n",
      "Epoch 110 \u0014 Val RMSE: 45411.11\n",
      "Epoch 111 \u0014 Val RMSE: 47285.21\n",
      "Epoch 112 \u0014 Val RMSE: 43815.13\n",
      "Epoch 113 \u0014 Val RMSE: 43476.35\n",
      "Epoch 114 \u0014 Val RMSE: 43723.16\n",
      "Epoch 115 \u0014 Val RMSE: 47757.26\n",
      "Epoch 116 \u0014 Val RMSE: 43571.97\n",
      "Epoch 117 \u0014 Val RMSE: 43634.98\n",
      "Epoch 118 \u0014 Val RMSE: 43384.92\n",
      "Epoch 119 \u0014 Val RMSE: 47727.89\n",
      "Epoch 120 \u0014 Val RMSE: 44339.44\n",
      "Epoch 121 \u0014 Val RMSE: 44561.33\n",
      "Epoch 122 \u0014 Val RMSE: 43350.38\n",
      "Epoch 123 \u0014 Val RMSE: 44418.51\n",
      "Epoch 124 \u0014 Val RMSE: 43496.36\n",
      "Epoch 125 \u0014 Val RMSE: 45782.95\n",
      "Epoch 126 \u0014 Val RMSE: 44875.00\n",
      "Epoch 127 \u0014 Val RMSE: 44202.01\n",
      "Epoch 128 \u0014 Val RMSE: 46836.10\n",
      "Epoch 129 \u0014 Val RMSE: 43319.12\n",
      "Epoch 130 \u0014 Val RMSE: 46188.66\n",
      "Epoch 131 \u0014 Val RMSE: 47949.34\n",
      "Epoch 132 \u0014 Val RMSE: 44209.36\n",
      "Epoch 133 \u0014 Val RMSE: 43705.20\n",
      "Epoch 134 \u0014 Val RMSE: 44533.94\n",
      "Epoch 135 \u0014 Val RMSE: 43594.54\n",
      "Epoch 136 \u0014 Val RMSE: 46893.73\n",
      "Epoch 137 \u0014 Val RMSE: 43147.31\n",
      "Epoch 138 \u0014 Val RMSE: 43966.91\n",
      "Epoch 139 \u0014 Val RMSE: 43176.78\n",
      "Epoch 140 \u0014 Val RMSE: 43708.74\n",
      "Epoch 141 \u0014 Val RMSE: 44085.57\n",
      "Epoch 142 \u0014 Val RMSE: 43357.05\n",
      "Epoch 143 \u0014 Val RMSE: 43178.97\n",
      "Epoch 144 \u0014 Val RMSE: 43551.55\n",
      "Epoch 145 \u0014 Val RMSE: 43102.01\n",
      "Epoch 146 \u0014 Val RMSE: 43987.36\n",
      "Epoch 147 \u0014 Val RMSE: 42826.05\n",
      "Epoch 148 \u0014 Val RMSE: 42989.24\n",
      "Epoch 149 \u0014 Val RMSE: 46051.19\n",
      "Epoch 150 \u0014 Val RMSE: 44676.60\n",
      "Epoch 151 \u0014 Val RMSE: 46918.98\n",
      "Epoch 152 \u0014 Val RMSE: 42659.45\n",
      "Epoch 153 \u0014 Val RMSE: 42809.55\n",
      "Epoch 154 \u0014 Val RMSE: 43257.51\n",
      "Epoch 155 \u0014 Val RMSE: 43950.60\n",
      "Epoch 156 \u0014 Val RMSE: 45330.88\n",
      "Epoch 157 \u0014 Val RMSE: 44336.84\n",
      "Epoch 158 \u0014 Val RMSE: 43099.93\n",
      "Epoch 159 \u0014 Val RMSE: 48988.48\n",
      "Epoch 160 \u0014 Val RMSE: 43851.61\n",
      "Epoch 161 \u0014 Val RMSE: 44508.48\n",
      "Epoch 162 \u0014 Val RMSE: 42771.56\n",
      "Epoch 163 \u0014 Val RMSE: 44255.13\n",
      "Epoch 164 \u0014 Val RMSE: 47006.10\n",
      "Epoch 165 \u0014 Val RMSE: 43588.04\n",
      "Epoch 166 \u0014 Val RMSE: 44239.41\n",
      "Epoch 167 \u0014 Val RMSE: 43382.05\n",
      "Epoch 168 \u0014 Val RMSE: 45166.92\n",
      "Epoch 169 \u0014 Val RMSE: 43001.17\n",
      "Epoch 170 \u0014 Val RMSE: 42701.87\n",
      "Epoch 171 \u0014 Val RMSE: 44951.45\n",
      "Epoch 172 \u0014 Val RMSE: 42672.33\n",
      "Epoch 173 \u0014 Val RMSE: 44153.49\n",
      "Epoch 174 \u0014 Val RMSE: 43391.93\n",
      "Epoch 175 \u0014 Val RMSE: 42614.18\n",
      "Epoch 176 \u0014 Val RMSE: 44042.42\n",
      "Epoch 177 \u0014 Val RMSE: 42670.68\n",
      "Epoch 178 \u0014 Val RMSE: 45734.39\n",
      "Epoch 179 \u0014 Val RMSE: 42702.93\n",
      "Epoch 180 \u0014 Val RMSE: 46772.83\n",
      "Epoch 181 \u0014 Val RMSE: 42856.65\n",
      "Epoch 182 \u0014 Val RMSE: 43524.93\n",
      "Epoch 183 \u0014 Val RMSE: 45432.70\n",
      "Epoch 184 \u0014 Val RMSE: 42641.19\n",
      "Epoch 185 \u0014 Val RMSE: 44578.88\n",
      "Epoch 186 \u0014 Val RMSE: 42355.91\n",
      "Epoch 187 \u0014 Val RMSE: 43216.21\n",
      "Epoch 188 \u0014 Val RMSE: 42598.48\n",
      "Epoch 189 \u0014 Val RMSE: 42686.37\n",
      "Epoch 190 \u0014 Val RMSE: 49676.68\n",
      "Epoch 191 \u0014 Val RMSE: 42613.40\n",
      "Epoch 192 \u0014 Val RMSE: 47565.48\n",
      "Epoch 193 \u0014 Val RMSE: 42327.43\n",
      "Epoch 194 \u0014 Val RMSE: 42760.99\n",
      "Epoch 195 \u0014 Val RMSE: 42633.73\n",
      "Epoch 196 \u0014 Val RMSE: 44710.98\n",
      "Epoch 197 \u0014 Val RMSE: 42763.87\n",
      "Epoch 198 \u0014 Val RMSE: 43116.70\n",
      "Epoch 199 \u0014 Val RMSE: 42906.89\n",
      "Epoch 200 \u0014 Val RMSE: 45048.70\n",
      "Epoch 201 \u0014 Val RMSE: 42917.68\n",
      "Epoch 202 \u0014 Val RMSE: 44059.27\n",
      "Epoch 203 \u0014 Val RMSE: 43415.98\n",
      "Epoch 204 \u0014 Val RMSE: 42520.45\n",
      "Epoch 205 \u0014 Val RMSE: 44573.15\n",
      "Epoch 206 \u0014 Val RMSE: 43795.14\n",
      "Epoch 207 \u0014 Val RMSE: 45159.41\n",
      "Epoch 208 \u0014 Val RMSE: 42786.18\n",
      "Epoch 209 \u0014 Val RMSE: 44035.95\n",
      "Epoch 210 \u0014 Val RMSE: 45045.68\n",
      "Epoch 211 \u0014 Val RMSE: 42353.08\n",
      "Epoch 212 \u0014 Val RMSE: 49832.68\n",
      "Epoch 213 \u0014 Val RMSE: 42585.20\n",
      "Epoch 214 \u0014 Val RMSE: 43191.05\n",
      "Epoch 215 \u0014 Val RMSE: 42867.86\n",
      "Epoch 216 \u0014 Val RMSE: 43565.39\n",
      "Epoch 217 \u0014 Val RMSE: 43970.74\n",
      "Epoch 218 \u0014 Val RMSE: 45253.51\n",
      "Epoch 219 \u0014 Val RMSE: 45454.48\n",
      "Epoch 220 \u0014 Val RMSE: 42881.77\n",
      "Epoch 221 \u0014 Val RMSE: 43305.19\n",
      "Epoch 222 \u0014 Val RMSE: 43035.52\n",
      "Epoch 223 \u0014 Val RMSE: 44804.54\n",
      "Epoch 224 \u0014 Val RMSE: 42923.20\n",
      "Epoch 225 \u0014 Val RMSE: 42705.59\n",
      "Epoch 226 \u0014 Val RMSE: 42525.86\n",
      "Epoch 227 \u0014 Val RMSE: 42937.80\n",
      "Epoch 228 \u0014 Val RMSE: 44902.73\n",
      "Epoch 229 \u0014 Val RMSE: 43681.84\n",
      "Epoch 230 \u0014 Val RMSE: 44660.61\n",
      "Epoch 231 \u0014 Val RMSE: 43460.71\n",
      "Epoch 232 \u0014 Val RMSE: 44168.05\n",
      "Epoch 233 \u0014 Val RMSE: 42507.92\n",
      "Epoch 234 \u0014 Val RMSE: 42680.26\n",
      "Epoch 235 \u0014 Val RMSE: 43525.25\n",
      "Epoch 236 \u0014 Val RMSE: 45371.75\n",
      "Epoch 237 \u0014 Val RMSE: 42134.43\n",
      "Epoch 238 \u0014 Val RMSE: 44531.67\n",
      "Epoch 239 \u0014 Val RMSE: 42940.59\n",
      "Epoch 240 \u0014 Val RMSE: 42703.09\n",
      "Epoch 241 \u0014 Val RMSE: 42749.77\n",
      "Epoch 242 \u0014 Val RMSE: 42772.22\n",
      "Epoch 243 \u0014 Val RMSE: 42569.10\n",
      "Epoch 244 \u0014 Val RMSE: 50473.69\n",
      "Epoch 245 \u0014 Val RMSE: 43444.24\n",
      "Epoch 246 \u0014 Val RMSE: 43518.08\n",
      "Epoch 247 \u0014 Val RMSE: 44859.26\n",
      "Epoch 248 \u0014 Val RMSE: 42870.17\n",
      "Epoch 249 \u0014 Val RMSE: 42636.81\n",
      "Epoch 250 \u0014 Val RMSE: 43246.44\n",
      "Epoch 251 \u0014 Val RMSE: 47695.02\n",
      "Epoch 252 \u0014 Val RMSE: 42469.03\n",
      "Epoch 253 \u0014 Val RMSE: 43095.48\n",
      "Epoch 254 \u0014 Val RMSE: 42296.28\n",
      "Epoch 255 \u0014 Val RMSE: 44533.04\n",
      "Epoch 256 \u0014 Val RMSE: 43804.94\n",
      "Epoch 257 \u0014 Val RMSE: 45246.36\n",
      "Epoch 258 \u0014 Val RMSE: 43279.80\n",
      "Epoch 259 \u0014 Val RMSE: 45939.12\n",
      "Epoch 260 \u0014 Val RMSE: 42375.04\n",
      "Epoch 261 \u0014 Val RMSE: 42621.36\n",
      "Epoch 262 \u0014 Val RMSE: 42834.98\n",
      "Epoch 263 \u0014 Val RMSE: 42485.98\n",
      "Epoch 264 \u0014 Val RMSE: 42544.93\n",
      "Epoch 265 \u0014 Val RMSE: 42759.67\n",
      "Epoch 266 \u0014 Val RMSE: 44929.92\n",
      "Epoch 267 \u0014 Val RMSE: 42983.75\n",
      "Epoch 268 \u0014 Val RMSE: 43677.32\n",
      "Epoch 269 \u0014 Val RMSE: 44284.06\n",
      "Epoch 270 \u0014 Val RMSE: 47862.92\n",
      "Epoch 271 \u0014 Val RMSE: 46884.83\n",
      "Epoch 272 \u0014 Val RMSE: 42067.11\n",
      "Epoch 273 \u0014 Val RMSE: 44483.93\n",
      "Epoch 274 \u0014 Val RMSE: 46134.75\n",
      "Epoch 275 \u0014 Val RMSE: 43247.89\n",
      "Epoch 276 \u0014 Val RMSE: 45205.03\n",
      "Epoch 277 \u0014 Val RMSE: 42168.93\n",
      "Epoch 278 \u0014 Val RMSE: 42194.04\n",
      "Epoch 279 \u0014 Val RMSE: 45086.40\n",
      "Epoch 280 \u0014 Val RMSE: 42741.56\n",
      "Epoch 281 \u0014 Val RMSE: 42318.36\n",
      "Epoch 282 \u0014 Val RMSE: 42886.41\n",
      "Epoch 283 \u0014 Val RMSE: 42297.16\n",
      "Epoch 284 \u0014 Val RMSE: 44845.85\n",
      "Epoch 285 \u0014 Val RMSE: 43414.36\n",
      "Epoch 286 \u0014 Val RMSE: 42659.19\n",
      "Epoch 287 \u0014 Val RMSE: 44090.18\n",
      "Epoch 288 \u0014 Val RMSE: 43056.10\n",
      "Epoch 289 \u0014 Val RMSE: 43421.42\n",
      "Epoch 290 \u0014 Val RMSE: 43310.82\n",
      "Epoch 291 \u0014 Val RMSE: 43399.51\n",
      "Epoch 292 \u0014 Val RMSE: 44035.72\n",
      "Epoch 293 \u0014 Val RMSE: 42414.91\n",
      "Epoch 294 \u0014 Val RMSE: 42427.76\n",
      "Epoch 295 \u0014 Val RMSE: 42619.65\n",
      "Epoch 296 \u0014 Val RMSE: 42987.89\n",
      "Epoch 297 \u0014 Val RMSE: 42813.88\n",
      "Epoch 298 \u0014 Val RMSE: 42355.66\n",
      "Epoch 299 \u0014 Val RMSE: 42592.77\n",
      "Epoch 300 \u0014 Val RMSE: 42721.86\n",
      "Epoch 301 \u0014 Val RMSE: 42723.38\n",
      "Epoch 302 \u0014 Val RMSE: 44418.57\n",
      "Epoch 303 \u0014 Val RMSE: 44694.21\n",
      "Epoch 304 \u0014 Val RMSE: 43274.10\n",
      "Epoch 305 \u0014 Val RMSE: 42526.87\n",
      "Epoch 306 \u0014 Val RMSE: 45121.38\n",
      "Epoch 307 \u0014 Val RMSE: 45696.42\n",
      "Epoch 308 \u0014 Val RMSE: 44727.23\n",
      "Epoch 309 \u0014 Val RMSE: 42253.60\n",
      "Epoch 310 \u0014 Val RMSE: 42269.71\n",
      "Epoch 311 \u0014 Val RMSE: 42315.51\n",
      "Epoch 312 \u0014 Val RMSE: 42896.82\n",
      "Epoch 313 \u0014 Val RMSE: 47042.76\n",
      "Epoch 314 \u0014 Val RMSE: 45420.59\n",
      "Epoch 315 \u0014 Val RMSE: 43145.27\n",
      "Epoch 316 \u0014 Val RMSE: 42211.81\n",
      "Epoch 317 \u0014 Val RMSE: 43240.15\n",
      "Epoch 318 \u0014 Val RMSE: 42795.93\n",
      "Epoch 319 \u0014 Val RMSE: 42986.22\n",
      "Epoch 320 \u0014 Val RMSE: 43428.12\n",
      "Epoch 321 \u0014 Val RMSE: 42976.69\n",
      "Epoch 322 \u0014 Val RMSE: 42293.71\n",
      "Epoch 323 \u0014 Val RMSE: 42432.98\n",
      "Epoch 324 \u0014 Val RMSE: 42967.25\n",
      "Epoch 325 \u0014 Val RMSE: 44513.33\n",
      "Epoch 326 \u0014 Val RMSE: 44806.29\n",
      "Epoch 327 \u0014 Val RMSE: 42562.18\n",
      "Epoch 328 \u0014 Val RMSE: 42765.76\n",
      "Epoch 329 \u0014 Val RMSE: 42946.70\n",
      "Epoch 330 \u0014 Val RMSE: 42819.18\n",
      "Epoch 331 \u0014 Val RMSE: 42399.33\n",
      "Epoch 332 \u0014 Val RMSE: 45131.73\n",
      "Epoch 333 \u0014 Val RMSE: 42426.87\n",
      "Epoch 334 \u0014 Val RMSE: 42873.65\n",
      "Epoch 335 \u0014 Val RMSE: 42517.36\n",
      "Epoch 336 \u0014 Val RMSE: 42940.83\n",
      "Epoch 337 \u0014 Val RMSE: 42503.89\n",
      "Epoch 338 \u0014 Val RMSE: 42212.41\n",
      "Epoch 339 \u0014 Val RMSE: 43218.18\n",
      "Epoch 340 \u0014 Val RMSE: 42354.48\n",
      "Epoch 341 \u0014 Val RMSE: 42794.48\n",
      "Epoch 342 \u0014 Val RMSE: 43663.25\n",
      "Epoch 343 \u0014 Val RMSE: 43102.78\n",
      "Epoch 344 \u0014 Val RMSE: 42329.59\n",
      "Epoch 345 \u0014 Val RMSE: 42860.74\n",
      "Epoch 346 \u0014 Val RMSE: 45290.22\n",
      "Epoch 347 \u0014 Val RMSE: 44509.71\n",
      "Epoch 348 \u0014 Val RMSE: 43189.76\n",
      "Epoch 349 \u0014 Val RMSE: 43763.91\n",
      "Epoch 350 \u0014 Val RMSE: 43370.60\n",
      "Epoch 351 \u0014 Val RMSE: 42424.05\n",
      "Epoch 352 \u0014 Val RMSE: 45543.44\n",
      "Epoch 353 \u0014 Val RMSE: 44821.61\n",
      "Epoch 354 \u0014 Val RMSE: 43587.68\n",
      "Epoch 355 \u0014 Val RMSE: 43119.34\n",
      "Epoch 356 \u0014 Val RMSE: 42683.54\n",
      "Epoch 357 \u0014 Val RMSE: 42621.62\n",
      "Epoch 358 \u0014 Val RMSE: 43838.15\n",
      "Epoch 359 \u0014 Val RMSE: 47839.91\n",
      "Epoch 360 \u0014 Val RMSE: 42997.62\n",
      "Epoch 361 \u0014 Val RMSE: 43067.29\n",
      "Epoch 362 \u0014 Val RMSE: 42596.62\n",
      "Epoch 363 \u0014 Val RMSE: 44287.28\n",
      "Epoch 364 \u0014 Val RMSE: 42758.59\n",
      "Epoch 365 \u0014 Val RMSE: 43837.90\n",
      "Epoch 366 \u0014 Val RMSE: 44063.77\n",
      "Epoch 367 \u0014 Val RMSE: 42837.54\n",
      "Epoch 368 \u0014 Val RMSE: 43919.77\n",
      "Epoch 369 \u0014 Val RMSE: 45394.94\n",
      "Epoch 370 \u0014 Val RMSE: 43502.92\n",
      "Epoch 371 \u0014 Val RMSE: 42756.18\n",
      "Epoch 372 \u0014 Val RMSE: 44100.52\n",
      "Epoch 373 \u0014 Val RMSE: 43272.49\n",
      "Epoch 374 \u0014 Val RMSE: 43275.29\n",
      "Epoch 375 \u0014 Val RMSE: 42710.41\n",
      "Epoch 376 \u0014 Val RMSE: 44235.69\n",
      "Epoch 377 \u0014 Val RMSE: 42581.15\n",
      "Epoch 378 \u0014 Val RMSE: 42477.86\n",
      "Epoch 379 \u0014 Val RMSE: 44808.13\n",
      "Epoch 380 \u0014 Val RMSE: 43148.77\n",
      "Epoch 381 \u0014 Val RMSE: 45270.87\n",
      "Epoch 382 \u0014 Val RMSE: 42426.19\n",
      "Epoch 383 \u0014 Val RMSE: 43310.15\n",
      "Epoch 384 \u0014 Val RMSE: 42352.69\n",
      "Epoch 385 \u0014 Val RMSE: 44286.47\n",
      "Epoch 386 \u0014 Val RMSE: 42628.42\n",
      "Epoch 387 \u0014 Val RMSE: 44051.39\n",
      "Epoch 388 \u0014 Val RMSE: 43972.83\n",
      "Epoch 389 \u0014 Val RMSE: 43007.13\n",
      "Epoch 390 \u0014 Val RMSE: 42520.35\n",
      "Epoch 391 \u0014 Val RMSE: 43966.97\n",
      "Epoch 392 \u0014 Val RMSE: 42383.99\n",
      "Epoch 393 \u0014 Val RMSE: 43311.93\n",
      "Epoch 394 \u0014 Val RMSE: 44464.57\n",
      "Epoch 395 \u0014 Val RMSE: 44069.64\n",
      "Epoch 396 \u0014 Val RMSE: 42753.44\n",
      "Epoch 397 \u0014 Val RMSE: 42185.10\n",
      "Epoch 398 \u0014 Val RMSE: 45102.75\n",
      "Epoch 399 \u0014 Val RMSE: 42515.74\n",
      "Epoch 400 \u0014 Val RMSE: 42309.37\n",
      "Epoch 401 \u0014 Val RMSE: 42893.39\n",
      "Epoch 402 \u0014 Val RMSE: 42290.16\n",
      "Epoch 403 \u0014 Val RMSE: 45248.98\n",
      "Epoch 404 \u0014 Val RMSE: 44552.28\n",
      "Epoch 405 \u0014 Val RMSE: 43420.44\n",
      "Epoch 406 \u0014 Val RMSE: 42818.30\n",
      "Epoch 407 \u0014 Val RMSE: 42368.71\n",
      "Epoch 408 \u0014 Val RMSE: 43478.09\n",
      "Epoch 409 \u0014 Val RMSE: 48310.17\n",
      "Epoch 410 \u0014 Val RMSE: 45100.55\n",
      "Epoch 411 \u0014 Val RMSE: 42586.46\n",
      "Epoch 412 \u0014 Val RMSE: 45218.90\n",
      "Epoch 413 \u0014 Val RMSE: 44108.52\n",
      "Epoch 414 \u0014 Val RMSE: 42573.30\n",
      "Epoch 415 \u0014 Val RMSE: 42487.07\n",
      "Epoch 416 \u0014 Val RMSE: 45694.48\n",
      "Epoch 417 \u0014 Val RMSE: 43093.00\n",
      "Epoch 418 \u0014 Val RMSE: 42101.60\n",
      "Epoch 419 \u0014 Val RMSE: 48660.86\n",
      "Epoch 420 \u0014 Val RMSE: 42394.71\n",
      "Epoch 421 \u0014 Val RMSE: 43527.24\n",
      "Epoch 422 \u0014 Val RMSE: 42265.58\n",
      "Epoch 423 \u0014 Val RMSE: 44777.71\n",
      "Epoch 424 \u0014 Val RMSE: 45531.46\n",
      "Epoch 425 \u0014 Val RMSE: 43032.92\n",
      "Epoch 426 \u0014 Val RMSE: 43013.80\n",
      "Epoch 427 \u0014 Val RMSE: 42432.22\n",
      "Epoch 428 \u0014 Val RMSE: 42347.88\n",
      "Epoch 429 \u0014 Val RMSE: 43015.06\n",
      "Epoch 430 \u0014 Val RMSE: 43733.26\n",
      "Epoch 431 \u0014 Val RMSE: 43129.29\n",
      "Epoch 432 \u0014 Val RMSE: 45688.75\n",
      "Epoch 433 \u0014 Val RMSE: 44533.01\n",
      "Epoch 434 \u0014 Val RMSE: 42891.37\n",
      "Epoch 435 \u0014 Val RMSE: 42690.71\n",
      "Epoch 436 \u0014 Val RMSE: 42174.19\n",
      "Epoch 437 \u0014 Val RMSE: 43899.12\n",
      "Epoch 438 \u0014 Val RMSE: 43219.47\n",
      "Epoch 439 \u0014 Val RMSE: 43740.05\n",
      "Epoch 440 \u0014 Val RMSE: 43572.14\n",
      "Epoch 441 \u0014 Val RMSE: 42668.68\n",
      "Epoch 442 \u0014 Val RMSE: 44170.61\n",
      "Epoch 443 \u0014 Val RMSE: 43217.46\n",
      "Epoch 444 \u0014 Val RMSE: 43745.30\n",
      "Epoch 445 \u0014 Val RMSE: 43027.12\n",
      "Epoch 446 \u0014 Val RMSE: 43199.98\n",
      "Epoch 447 \u0014 Val RMSE: 43119.96\n",
      "Epoch 448 \u0014 Val RMSE: 43437.90\n",
      "Epoch 449 \u0014 Val RMSE: 43192.18\n",
      "Epoch 450 \u0014 Val RMSE: 43227.77\n",
      "Epoch 451 \u0014 Val RMSE: 42942.57\n",
      "Epoch 452 \u0014 Val RMSE: 43178.83\n",
      "Epoch 453 \u0014 Val RMSE: 43646.79\n",
      "Epoch 454 \u0014 Val RMSE: 42093.27\n",
      "Epoch 455 \u0014 Val RMSE: 42600.78\n",
      "Epoch 456 \u0014 Val RMSE: 43832.84\n",
      "Epoch 457 \u0014 Val RMSE: 43231.50\n",
      "Epoch 458 \u0014 Val RMSE: 43753.36\n",
      "Epoch 459 \u0014 Val RMSE: 44230.40\n",
      "Epoch 460 \u0014 Val RMSE: 42500.17\n",
      "Epoch 461 \u0014 Val RMSE: 42468.16\n",
      "Epoch 462 \u0014 Val RMSE: 43237.06\n",
      "Epoch 463 \u0014 Val RMSE: 46492.30\n",
      "Epoch 464 \u0014 Val RMSE: 45819.57\n",
      "Epoch 465 \u0014 Val RMSE: 42558.55\n",
      "Epoch 466 \u0014 Val RMSE: 42637.90\n",
      "Epoch 467 \u0014 Val RMSE: 42788.21\n",
      "Epoch 468 \u0014 Val RMSE: 42942.18\n",
      "Epoch 469 \u0014 Val RMSE: 42549.36\n",
      "Epoch 470 \u0014 Val RMSE: 42717.93\n",
      "Epoch 471 \u0014 Val RMSE: 43278.61\n",
      "Epoch 472 \u0014 Val RMSE: 42932.98\n",
      "Epoch 473 \u0014 Val RMSE: 42495.06\n",
      "Epoch 474 \u0014 Val RMSE: 42801.42\n",
      "Epoch 475 \u0014 Val RMSE: 42562.54\n",
      "Epoch 476 \u0014 Val RMSE: 42710.57\n",
      "Epoch 477 \u0014 Val RMSE: 44483.40\n",
      "Epoch 478 \u0014 Val RMSE: 45869.66\n",
      "Epoch 479 \u0014 Val RMSE: 44765.33\n",
      "Epoch 480 \u0014 Val RMSE: 47893.85\n",
      "Epoch 481 \u0014 Val RMSE: 42937.62\n",
      "Epoch 482 \u0014 Val RMSE: 44321.30\n",
      "Epoch 483 \u0014 Val RMSE: 42402.33\n",
      "Epoch 484 \u0014 Val RMSE: 42737.94\n",
      "Epoch 485 \u0014 Val RMSE: 43111.11\n",
      "Epoch 486 \u0014 Val RMSE: 43381.95\n",
      "Epoch 487 \u0014 Val RMSE: 43349.84\n",
      "Epoch 488 \u0014 Val RMSE: 43453.62\n",
      "Epoch 489 \u0014 Val RMSE: 44283.67\n",
      "Epoch 490 \u0014 Val RMSE: 42848.23\n",
      "Epoch 491 \u0014 Val RMSE: 42391.67\n",
      "Epoch 492 \u0014 Val RMSE: 48470.06\n",
      "Epoch 493 \u0014 Val RMSE: 44799.09\n",
      "Epoch 494 \u0014 Val RMSE: 45168.31\n",
      "Epoch 495 \u0014 Val RMSE: 42761.87\n",
      "Epoch 496 \u0014 Val RMSE: 44147.32\n",
      "Epoch 497 \u0014 Val RMSE: 42700.35\n",
      "Epoch 498 \u0014 Val RMSE: 43117.82\n",
      "Epoch 499 \u0014 Val RMSE: 43027.85\n",
      "Epoch 500 \u0014 Val RMSE: 42980.90\n",
      "Epoch 501 \u0014 Val RMSE: 42864.80\n",
      "Epoch 502 \u0014 Val RMSE: 44241.04\n",
      "Epoch 503 \u0014 Val RMSE: 44187.74\n",
      "Epoch 504 \u0014 Val RMSE: 42870.71\n",
      "Epoch 505 \u0014 Val RMSE: 43728.84\n",
      "Epoch 506 \u0014 Val RMSE: 43426.29\n",
      "Epoch 507 \u0014 Val RMSE: 43265.76\n",
      "Epoch 508 \u0014 Val RMSE: 42870.63\n",
      "Epoch 509 \u0014 Val RMSE: 42745.79\n",
      "Epoch 510 \u0014 Val RMSE: 43105.11\n",
      "Epoch 511 \u0014 Val RMSE: 43165.68\n",
      "Epoch 512 \u0014 Val RMSE: 42680.22\n",
      "Epoch 513 \u0014 Val RMSE: 43066.35\n",
      "Epoch 514 \u0014 Val RMSE: 42570.14\n",
      "Epoch 515 \u0014 Val RMSE: 42850.20\n",
      "Epoch 516 \u0014 Val RMSE: 42638.81\n",
      "Epoch 517 \u0014 Val RMSE: 47044.30\n",
      "Epoch 518 \u0014 Val RMSE: 45097.26\n",
      "Epoch 519 \u0014 Val RMSE: 42769.82\n",
      "Epoch 520 \u0014 Val RMSE: 43733.06\n",
      "Epoch 521 \u0014 Val RMSE: 47792.27\n",
      "Epoch 522 \u0014 Val RMSE: 43477.28\n",
      "Epoch 523 \u0014 Val RMSE: 45577.11\n",
      "Epoch 524 \u0014 Val RMSE: 42826.33\n",
      "Epoch 525 \u0014 Val RMSE: 42693.11\n",
      "Epoch 526 \u0014 Val RMSE: 45498.77\n",
      "Epoch 527 \u0014 Val RMSE: 44174.52\n",
      "Epoch 528 \u0014 Val RMSE: 44516.33\n",
      "Epoch 529 \u0014 Val RMSE: 43002.69\n",
      "Epoch 530 \u0014 Val RMSE: 45943.00\n",
      "Epoch 531 \u0014 Val RMSE: 43732.52\n",
      "Epoch 532 \u0014 Val RMSE: 42727.89\n",
      "Epoch 533 \u0014 Val RMSE: 43994.22\n",
      "Epoch 534 \u0014 Val RMSE: 44590.56\n",
      "Epoch 535 \u0014 Val RMSE: 43741.95\n",
      "Epoch 536 \u0014 Val RMSE: 42988.25\n",
      "Epoch 537 \u0014 Val RMSE: 42498.44\n",
      "Epoch 538 \u0014 Val RMSE: 42418.31\n",
      "Epoch 539 \u0014 Val RMSE: 42410.16\n",
      "Epoch 540 \u0014 Val RMSE: 42653.81\n",
      "Epoch 541 \u0014 Val RMSE: 44973.12\n",
      "Epoch 542 \u0014 Val RMSE: 42868.93\n",
      "Epoch 543 \u0014 Val RMSE: 42681.70\n",
      "Epoch 544 \u0014 Val RMSE: 43171.85\n",
      "Epoch 545 \u0014 Val RMSE: 43666.62\n",
      "Epoch 546 \u0014 Val RMSE: 45070.07\n",
      "Epoch 547 \u0014 Val RMSE: 45801.99\n",
      "Epoch 548 \u0014 Val RMSE: 47315.55\n",
      "Epoch 549 \u0014 Val RMSE: 42428.67\n",
      "Epoch 550 \u0014 Val RMSE: 45675.07\n",
      "Epoch 551 \u0014 Val RMSE: 42619.87\n",
      "Epoch 552 \u0014 Val RMSE: 45115.63\n",
      "Epoch 553 \u0014 Val RMSE: 44991.49\n",
      "Epoch 554 \u0014 Val RMSE: 43048.28\n",
      "Epoch 555 \u0014 Val RMSE: 42305.87\n",
      "Epoch 556 \u0014 Val RMSE: 43368.69\n",
      "Epoch 557 \u0014 Val RMSE: 45263.74\n",
      "Epoch 558 \u0014 Val RMSE: 43983.52\n",
      "Epoch 559 \u0014 Val RMSE: 42664.76\n",
      "Epoch 560 \u0014 Val RMSE: 47270.90\n",
      "Epoch 561 \u0014 Val RMSE: 42483.82\n",
      "Epoch 562 \u0014 Val RMSE: 46357.63\n",
      "Epoch 563 \u0014 Val RMSE: 42739.17\n",
      "Epoch 564 \u0014 Val RMSE: 44165.02\n",
      "Epoch 565 \u0014 Val RMSE: 42713.63\n",
      "Epoch 566 \u0014 Val RMSE: 42676.60\n",
      "Epoch 567 \u0014 Val RMSE: 44084.27\n",
      "Epoch 568 \u0014 Val RMSE: 42545.69\n",
      "Epoch 569 \u0014 Val RMSE: 42674.05\n",
      "Epoch 570 \u0014 Val RMSE: 43886.70\n",
      "Epoch 571 \u0014 Val RMSE: 44216.61\n",
      "Epoch 572 \u0014 Val RMSE: 42811.55\n",
      "Deteniendo tras 572 epochs sin mejora.\n",
      "Mejor RMSE en validación: 42067.11 euros\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ----------------------------\n",
    "# 6. Entrenamiento con EarlyStopping\n",
    "# ----------------------------\n",
    "best_val_rmse     = np.inf\n",
    "epochs_no_improve = 0\n",
    "\n",
    "for epoch in range(1, MAX_EPOCHS + 1):\n",
    "    # Training\n",
    "    model.train()\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "        preds  = model(xb)\n",
    "        loss   = criterion(preds, yb)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_losses = []\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            xb, yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            preds  = model(xb)\n",
    "            val_losses.append(((preds - yb)**2).mean().item())\n",
    "    val_rmse = np.sqrt(np.mean(val_losses))\n",
    "    print(f\"Epoch {epoch:03d} \u0014 Val RMSE: {val_rmse:.2f}\")\n",
    "\n",
    "    # EarlyStopping\n",
    "    if val_rmse + 1e-4 < best_val_rmse:\n",
    "        best_val_rmse     = val_rmse\n",
    "        epochs_no_improve = 0\n",
    "        torch.save(model.state_dict(), os.path.join(MODEL_DIR, 'best_model.pt'))\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve >= PATIENCE:\n",
    "            print(f\"Deteniendo tras {epoch} epochs sin mejora.\")\n",
    "            break\n",
    "\n",
    "# Carga mejor modelo\n",
    "model.load_state_dict(torch.load(os.path.join(MODEL_DIR, 'best_model.pt')))\n",
    "print(f\"Mejor RMSE en validación: {best_val_rmse:.2f} euros\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission guardada en models_nn/submission_pytorch_nn.csv\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ----------------------------\n",
    "# 7. Generar submission final\n",
    "# ----------------------------\n",
    "if os.path.exists(TEST_PATH):\n",
    "    # Leer test.csv evitando columna Unnamed\n",
    "    df_test = pd.read_csv(TEST_PATH, index_col=0)\n",
    "    X_test = df_test.reindex(columns=FEATURES, fill_value=0).copy()\n",
    "    #X_test  = df_test.reindex(columns=FEATURES).copy()\n",
    "\n",
    "    # Imputación y encoding idénticos al train\n",
    "    for col in num_cols:\n",
    "        X_test[col].fillna(df[col].median(), inplace=True)\n",
    "    for col in cat_cols:\n",
    "        # no es necesario porque las dummies ya están alineadas\n",
    "        pass\n",
    "\n",
    "    # Escalado\n",
    "    X_test[num_cols] = scaler.transform(X_test[num_cols])\n",
    "\n",
    "    # DataLoader test\n",
    "    test_ds    = HousePriceDataset(X_test.values)\n",
    "    test_loader= DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False,\n",
    "                            num_workers=2, pin_memory=True)\n",
    "\n",
    "    # Predicción\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for xb in test_loader:\n",
    "            xb = xb.to(DEVICE)\n",
    "            p  = model(xb).cpu().numpy()\n",
    "            preds.append(p)\n",
    "    preds = np.concatenate(preds, axis=0)\n",
    "\n",
    "    submission = pd.DataFrame({\n",
    "        'id':          df_test['id'],\n",
    "        'prezo_euros': preds\n",
    "    })\n",
    "    submission.to_csv(SUBMIT_FILE, index=False)\n",
    "    print(f\"Submission guardada en {SUBMIT_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0523ecfdfd03da9535a2cd394fa2b2a2368df119d71b1e2a5e4a2b8711053260"
  },
  "kernelspec": {
   "display_name": "Python 3.7.8 64-bit ('venvP4': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
