{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: catboost in /home/av/.local/lib/python3.12/site-packages (1.2.8)\n",
      "Requirement already satisfied: graphviz in /usr/lib/python3/dist-packages (from catboost) (0.20.2)\n",
      "Requirement already satisfied: matplotlib in /usr/lib/python3/dist-packages (from catboost) (3.6.3)\n",
      "Requirement already satisfied: numpy<3.0,>=1.16.0 in /usr/lib/python3/dist-packages (from catboost) (1.26.4)\n",
      "Requirement already satisfied: pandas>=0.24 in /usr/lib/python3/dist-packages (from catboost) (2.1.4+dfsg)\n",
      "Requirement already satisfied: scipy in /usr/lib/python3/dist-packages (from catboost) (1.11.4)\n",
      "Requirement already satisfied: plotly in /home/av/.local/lib/python3.12/site-packages (from catboost) (6.0.1)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from catboost) (1.16.0)\n",
      "Requirement already satisfied: narwhals>=1.15.1 in /home/av/.local/lib/python3.12/site-packages (from plotly->catboost) (1.38.0)\n",
      "Requirement already satisfied: packaging in /usr/lib/python3/dist-packages (from plotly->catboost) (24.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting torch\n",
      "  Using cached torch-2.7.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (29 kB)\n",
      "Collecting filelock (from torch)\n",
      "  Using cached filelock-3.18.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/lib/python3/dist-packages (from torch) (4.10.0)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (from torch) (68.1.2)\n",
      "Collecting sympy>=1.13.3 (from torch)\n",
      "  Using cached sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting networkx (from torch)\n",
      "  Using cached networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: jinja2 in /usr/lib/python3/dist-packages (from torch) (3.1.2)\n",
      "Collecting fsspec (from torch)\n",
      "  Using cached fsspec-2025.3.2-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.6.77 (from torch)\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.6.77 (from torch)\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.6.80 (from torch)\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.5.1.17 (from torch)\n",
      "  Using cached nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.6.4.1 (from torch)\n",
      "  Using cached nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.3.0.4 (from torch)\n",
      "  Using cached nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.7.77 (from torch)\n",
      "  Using cached nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.7.1.2 (from torch)\n",
      "  Using cached nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.5.4.2 (from torch)\n",
      "  Using cached nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparselt-cu12==0.6.3 (from torch)\n",
      "  Using cached nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
      "Collecting nvidia-nccl-cu12==2.26.2 (from torch)\n",
      "  Using cached nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.6.77 (from torch)\n",
      "  Using cached nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.6.85 (from torch)\n",
      "  Using cached nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufile-cu12==1.11.1.6 (from torch)\n",
      "  Using cached nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting triton==3.3.0 (from torch)\n",
      "  Using cached triton-3.3.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy>=1.13.3->torch)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Using cached torch-2.7.0-cp312-cp312-manylinux_2_28_x86_64.whl (865.0 MB)\n",
      "Using cached nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (393.1 MB)\n",
      "Using cached nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.9 MB)\n",
      "Using cached nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (23.7 MB)\n",
      "Using cached nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (897 kB)\n",
      "Using cached nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl (571.0 MB)\n",
      "Using cached nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (200.2 MB)\n",
      "Using cached nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.1 MB)\n",
      "Using cached nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (56.3 MB)\n",
      "\u001b[31mERROR: Could not install packages due to an OSError: [Errno 28] No space left on device\n",
      "\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 23\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[1;32m     22\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39msystem(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpip install torch --break-system-packages\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 23\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m nn\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodel_selection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m train_test_split, GridSearchCV, KFold, cross_val_predict\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n",
    "from sklearn.linear_model import ElasticNet\n",
    "!pip install catboost --break-system-packages\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "from xgboost import XGBRegressor\n",
    "import lightgbm as lgb\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import time\n",
    "!pip install torch --break-system-packages\n",
    "import torch\n",
    "from torch import nn\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold, cross_val_predict\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n",
    "from sklearn.linear_model import ElasticNet, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, StackingRegressor, VotingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.kernel_ridge import KernelRidge\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "from xgboost import XGBRegressor\n",
    "import lightgbm as lgb\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import warnings\n",
    "\n",
    "\n",
    "# Create results directory\n",
    "os.makedirs('results_stacking', exist_ok=True)\n",
    "\n",
    "# Function to load data\n",
    "def load_data(train_path, test_path):\n",
    "    \"\"\"\n",
    "    Load training and test data\n",
    "    \"\"\"\n",
    "    train_df = pd.read_csv(train_path)\n",
    "    test_df = pd.read_csv(test_path)\n",
    "    \n",
    "    # For training data\n",
    "    X_train = train_df.drop(['prezo_euros', 'id'], axis=1, errors='ignore')\n",
    "    y_train = train_df['prezo_euros']\n",
    "    \n",
    "    # For test data\n",
    "    if 'Unnamed: 0' in test_df.columns:\n",
    "        X_test = test_df.drop(['id', 'Unnamed: 0'], axis=1, errors='ignore')\n",
    "    else:\n",
    "        X_test = test_df.drop(['id'], axis=1, errors='ignore')\n",
    "    test_ids = test_df['id']\n",
    "    \n",
    "    print(f\"Training data shape: {X_train.shape}\")\n",
    "    print(f\"Test data shape: {X_test.shape}\")\n",
    "    return X_train, y_train, X_test, test_ids\n",
    "\n",
    "# Function to identify categorical features\n",
    "def get_categorical_features(df):\n",
    "    \"\"\"\n",
    "    Identify categorical features in the dataset\n",
    "    \"\"\"\n",
    "    categorical_features = []\n",
    "    for col in df.columns:\n",
    "        if (df[col].dtype == 'object' or\n",
    "            col in ['tipo_edificacion', 'calidade_materiais',\n",
    "                   'cor_favorita_propietario', 'acceso_transporte_publico',\n",
    "                   'orientacion', 'eficiencia_enerxetica'] or\n",
    "            'tipo_' in col or 'color_' in col):\n",
    "            categorical_features.append(col)\n",
    "    print(f\"Categorical features: {categorical_features}\")\n",
    "    return categorical_features\n",
    "\n",
    "# Function to load pretrained models\n",
    "def load_models(model_paths):\n",
    "    \"\"\"\n",
    "    Load pretrained models from specified paths\n",
    "    \"\"\"\n",
    "    models = {}\n",
    "    \n",
    "    # Load CatBoost model\n",
    "    if 'catboost' in model_paths and os.path.exists(model_paths['catboost']):\n",
    "        print(\"Loading CatBoost model...\")\n",
    "        models['catboost'] = CatBoostRegressor()\n",
    "        models['catboost'].load_model(model_paths['catboost'])\n",
    "    \n",
    "    # Load XGBoost model\n",
    "    if 'xgboost' in model_paths and os.path.exists(model_paths['xgboost']):\n",
    "        print(\"Loading XGBoost model...\")\n",
    "        models['xgboost'] = XGBRegressor()\n",
    "        models['xgboost'].load_model(model_paths['xgboost'])\n",
    "    \n",
    "    # Load LightGBM model\n",
    "    if 'lightgbm' in model_paths and os.path.exists(model_paths['lightgbm']):\n",
    "        print(\"Loading LightGBM model...\")\n",
    "        models['lightgbm'] = lgb.Booster(model_file=model_paths['lightgbm'])\n",
    "    \n",
    "    # Load MLP model (which includes the scaler)\n",
    "    if 'mlp' in model_paths and os.path.exists(model_paths['mlp']):\n",
    "        print(\"Loading MLP model...\")\n",
    "        models['mlp'] = joblib.load(model_paths['mlp'])\n",
    "    \n",
    "    print(f\"Successfully loaded {len(models)} models\")\n",
    "    return models\n",
    "\n",
    "# Function to make predictions with loaded models\n",
    "def predict_with_models(models, X):\n",
    "    \"\"\"\n",
    "    Make predictions using all loaded models\n",
    "    \"\"\"\n",
    "    predictions = {}\n",
    "    \n",
    "    if 'catboost' in models:\n",
    "        print(\"Predicting with CatBoost...\")\n",
    "        predictions['catboost'] = models['catboost'].predict(X)\n",
    "    \n",
    "    if 'xgboost' in models:\n",
    "        print(\"Predicting with XGBoost...\")\n",
    "        predictions['xgboost'] = models['xgboost'].predict(X)\n",
    "    \n",
    "    if 'lightgbm' in models:\n",
    "        print(\"Predicting with LightGBM...\")\n",
    "        predictions['lightgbm'] = models['lightgbm'].predict(X)\n",
    "    \n",
    "    if 'mlp' in models:\n",
    "        print(\"Predicting with MLP...\")\n",
    "        # Extract model and scaler from the MLP model object\n",
    "        scaler = models['mlp']['scaler']\n",
    "        mlp_model = models['mlp']['model']\n",
    "        # Apply scaling before prediction\n",
    "        X_scaled = scaler.transform(X)\n",
    "        predictions['mlp'] = mlp_model.predict(X_scaled)\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# Function to generate level 1 features\n",
    "def generate_level1_features(models, X_train, X_test, y_train, categorical_features=None):\n",
    "    \"\"\"\n",
    "    Generate level 1 training features for meta-model using cross-validation\n",
    "    \"\"\"\n",
    "    print(\"Generating level 1 features for stacking...\")\n",
    "    \n",
    "    # For training set, use k-fold cross-validation to avoid data leakage\n",
    "    k = 5\n",
    "    train_preds_df = pd.DataFrame(index=range(X_train.shape[0]))\n",
    "    test_preds_all = {}\n",
    "    \n",
    "    # Create folds\n",
    "    n_samples = X_train.shape[0]\n",
    "    fold_size = n_samples // k\n",
    "    indices = np.arange(n_samples)\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    for fold in range(k):\n",
    "        print(f\"Processing fold {fold+1}/{k}\")\n",
    "        \n",
    "        # Get indices for this fold\n",
    "        start_idx = fold * fold_size\n",
    "        end_idx = (fold + 1) * fold_size if fold < k - 1 else n_samples\n",
    "        val_indices = indices[start_idx:end_idx]\n",
    "        train_indices = np.setdiff1d(indices, val_indices)\n",
    "        \n",
    "        # Split data for this fold\n",
    "        X_train_fold = X_train.iloc[train_indices]\n",
    "        y_train_fold = y_train.iloc[train_indices]\n",
    "        X_val_fold = X_train.iloc[val_indices]\n",
    "        \n",
    "        # Generate predictions for this fold\n",
    "        fold_preds = pd.DataFrame(index=val_indices)\n",
    "        test_fold_preds = {}\n",
    "        \n",
    "        # Train and predict with CatBoost\n",
    "        if 'catboost' in models:\n",
    "            print(\"Training fold with CatBoost...\")\n",
    "            if categorical_features:\n",
    "                train_pool = Pool(X_train_fold, y_train_fold, cat_features=categorical_features)\n",
    "                val_pool = Pool(X_val_fold, cat_features=categorical_features)\n",
    "                test_pool = Pool(X_test, cat_features=categorical_features)\n",
    "                \n",
    "                temp_model = CatBoostRegressor()\n",
    "                temp_model.set_params(**models['catboost'].get_params())\n",
    "                temp_model.fit(train_pool, verbose=False)\n",
    "                fold_preds['catboost'] = temp_model.predict(val_pool)\n",
    "                test_fold_preds['catboost'] = temp_model.predict(test_pool)\n",
    "            else:\n",
    "                temp_model = CatBoostRegressor()\n",
    "                temp_model.set_params(**models['catboost'].get_params())\n",
    "                temp_model.fit(X_train_fold, y_train_fold, verbose=False)\n",
    "                fold_preds['catboost'] = temp_model.predict(X_val_fold)\n",
    "                test_fold_preds['catboost'] = temp_model.predict(X_test)\n",
    "        \n",
    "        # Train and predict with XGBoost\n",
    "        if 'xgboost' in models:\n",
    "            print(\"Training fold with XGBoost...\")\n",
    "            temp_model = XGBRegressor()\n",
    "            temp_model.set_params(**models['xgboost'].get_params())\n",
    "            temp_model.fit(X_train_fold, y_train_fold, verbose=False)\n",
    "            fold_preds['xgboost'] = temp_model.predict(X_val_fold)\n",
    "            test_fold_preds['xgboost'] = temp_model.predict(X_test)\n",
    "        \n",
    "        # Train and predict with LightGBM\n",
    "        if 'lightgbm' in models:\n",
    "            print(\"Training fold with LightGBM...\")\n",
    "            train_lgb = lgb.Dataset(X_train_fold, label=y_train_fold)\n",
    "            params = models['lightgbm'].params if hasattr(models['lightgbm'], 'params') else {}\n",
    "            temp_model = lgb.train(params, train_lgb, num_boost_round=100)\n",
    "            fold_preds['lightgbm'] = temp_model.predict(X_val_fold)\n",
    "            test_fold_preds['lightgbm'] = temp_model.predict(X_test)\n",
    "        \n",
    "        # Train and predict with MLP\n",
    "        if 'mlp' in models:\n",
    "            print(\"Training fold with MLP...\")\n",
    "            # Extract parameters from loaded model\n",
    "            mlp_params = models['mlp']['model'].get_params()\n",
    "            \n",
    "            # Create and fit scaler\n",
    "            scaler = StandardScaler()\n",
    "            X_train_fold_scaled = scaler.fit_transform(X_train_fold)\n",
    "            X_val_fold_scaled = scaler.transform(X_val_fold)\n",
    "            X_test_scaled = scaler.transform(X_test)\n",
    "            \n",
    "            # Create and train MLP model\n",
    "            temp_model = MLPRegressor(**mlp_params)\n",
    "            temp_model.fit(X_train_fold_scaled, y_train_fold)\n",
    "            \n",
    "            fold_preds['mlp'] = temp_model.predict(X_val_fold_scaled)\n",
    "            test_fold_preds['mlp'] = temp_model.predict(X_test_scaled)\n",
    "\n",
    "        if 'torch_mlp' in models:\n",
    "            print(\"Training fold with PyTorch MLP...\")\n",
    "            # Get parameters from the loaded model\n",
    "            torch_mlp_info = models['torch_mlp']\n",
    "            device = torch_mlp_info['device']\n",
    "            \n",
    "            # Create and fit scaler\n",
    "            scaler = StandardScaler()\n",
    "            X_train_fold_scaled = scaler.fit_transform(X_train_fold)\n",
    "            X_val_fold_scaled = scaler.transform(X_val_fold)\n",
    "            X_test_scaled = scaler.transform(X_test)\n",
    "            \n",
    "            # Create a new model with the same architecture\n",
    "            input_dim = X_train_fold_scaled.shape[1]\n",
    "            temp_model = MLPRegressorTorch(input_dim).to(device)\n",
    "            \n",
    "            # Convert to PyTorch tensors\n",
    "            X_train_tensor = torch.tensor(X_train_fold_scaled, dtype=torch.float32).to(device)\n",
    "            y_train_tensor = torch.tensor(y_train_fold.values.reshape(-1, 1), dtype=torch.float32).to(device)\n",
    "            X_val_tensor = torch.tensor(X_val_fold_scaled, dtype=torch.float32).to(device)\n",
    "            X_test_tensor = torch.tensor(X_test_scaled, dtype=torch.float32).to(device)\n",
    "            \n",
    "            # Train the PyTorch model\n",
    "            criterion = nn.L1Loss()\n",
    "            optimizer = torch.optim.Adam(temp_model.parameters(), lr=0.001)\n",
    "            batch_size = 64\n",
    "            epochs = 100\n",
    "            \n",
    "            for epoch in range(epochs):\n",
    "                # Create mini-batches\n",
    "                perm = torch.randperm(X_train_tensor.size(0))\n",
    "                for start in range(0, X_train_tensor.size(0), batch_size):\n",
    "                    batch_indices = perm[start:start + batch_size]\n",
    "                    batch_X = X_train_tensor[batch_indices]\n",
    "                    batch_y = y_train_tensor[batch_indices]\n",
    "                    \n",
    "                    # Forward and backward pass\n",
    "                    optimizer.zero_grad()\n",
    "                    outputs = temp_model(batch_X)\n",
    "                    loss = criterion(outputs, batch_y)\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "            \n",
    "            # Get predictions\n",
    "            temp_model.eval()\n",
    "            with torch.no_grad():\n",
    "                fold_preds['torch_mlp'] = temp_model(X_val_tensor).cpu().numpy().flatten()\n",
    "                test_fold_preds['torch_mlp'] = temp_model(X_test_tensor).cpu().numpy().flatten()\n",
    "        \n",
    "        # Add fold predictions to overall predictions DataFrame\n",
    "        for model_name in fold_preds.columns:\n",
    "            train_preds_df.loc[val_indices, model_name] = fold_preds[model_name].values\n",
    "            \n",
    "            if model_name not in test_preds_all:\n",
    "                test_preds_all[model_name] = []\n",
    "            test_preds_all[model_name].append(test_fold_preds[model_name])\n",
    "    \n",
    "    # Create test predictions by averaging fold predictions\n",
    "    test_preds_df = pd.DataFrame()\n",
    "    for model_name, preds_list in test_preds_all.items():\n",
    "        test_preds_df[model_name] = np.mean(preds_list, axis=0)\n",
    "    \n",
    "    print(f\"Generated level 1 features with shape: {train_preds_df.shape} (train), {test_preds_df.shape} (test)\")\n",
    "    return train_preds_df, test_preds_df\n",
    "\n",
    "\n",
    "\n",
    "def train_meta_model(level1_train, y_train, level1_test):\n",
    "    \"\"\"\n",
    "    Entrena y optimiza un meta-modelo ElasticNet sobre features de nivel-1,\n",
    "    buscando minimizar el MAE mediante validación cruzada.\n",
    "    \"\"\"\n",
    "    print(\"Buscando hiperparámetros óptimos para ElasticNet (minimizando MAE)...\")\n",
    "    \n",
    "    # Definición de la malla de búsqueda\n",
    "    param_grid = {\n",
    "        'alpha':    [0.001, 0.01, 0.1, 1.0, 10.0],\n",
    "        'l1_ratio': [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "    }\n",
    "    \n",
    "    base_model = ElasticNet(max_iter=10000, random_state=42)\n",
    "    grid = GridSearchCV(\n",
    "        estimator=base_model,\n",
    "        param_grid=param_grid,\n",
    "        cv=5,\n",
    "        scoring='neg_mean_absolute_error',\n",
    "        n_jobs=-1,\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Ajuste de la búsqueda de hiperparámetros\n",
    "    grid.fit(level1_train, y_train)\n",
    "    \n",
    "    best_model = grid.best_estimator_\n",
    "    best_mae   = -grid.best_score_\n",
    "    print(f\"Mejores parámetros: {grid.best_params_}\")\n",
    "    print(f\"MAE CV medio óptimo: {best_mae:.4f}\")\n",
    "    \n",
    "    # Dividir para validación adicional\n",
    "    X_tr, X_val, y_tr, y_val = train_test_split(\n",
    "        level1_train, y_train, test_size=0.2, random_state=42\n",
    "    )\n",
    "    best_model.fit(X_tr, y_tr)\n",
    "    val_preds = best_model.predict(X_val)\n",
    "    \n",
    "    val_mae  = mean_absolute_error(y_val, val_preds)\n",
    "    val_rmse = np.sqrt(mean_squared_error(y_val, val_preds))\n",
    "    val_r2   = r2_score(y_val, val_preds)\n",
    "    \n",
    "    print(\"Métricas en conjunto de validación:\")\n",
    "    print(f\"  MAE : {val_mae:.4f}\")\n",
    "    print(f\"  RMSE: {val_rmse:.4f}\")\n",
    "    print(f\"  R²  : {val_r2:.4f}\")\n",
    "    \n",
    "    # Reentrenar con los mejores parámetros sobre todo el dataset de entrenamiento\n",
    "    best_model.fit(level1_train, y_train)\n",
    "    \n",
    "    # Predicciones sobre el conjunto de test\n",
    "    test_preds = best_model.predict(level1_test)\n",
    "    \n",
    "    return best_model, test_preds\n",
    "\n",
    "\n",
    "# Function to create submission file\n",
    "def create_submission(test_ids, predictions, output_file):\n",
    "    \"\"\"\n",
    "    Create submission file with predictions\n",
    "    \"\"\"\n",
    "    submission_df = pd.DataFrame({\n",
    "        'id': test_ids,\n",
    "        'prezo_euros': predictions\n",
    "    })\n",
    "    \n",
    "    submission_df.to_csv(output_file, index=False)\n",
    "    print(f\"Submission saved to {output_file}\")\n",
    "    return submission_df\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Load data\n",
    "    train_path = 'train_processed.csv'\n",
    "    test_path = 'test_processed.csv'\n",
    "    X_train, y_train, X_test, test_ids = load_data(train_path, test_path)\n",
    "    \n",
    "    # Get categorical features\n",
    "    categorical_features = get_categorical_features(X_train)\n",
    "    \n",
    "    # Define paths to pretrained models\n",
    "    model_paths = {\n",
    "        'catboost': 'models_stacking/stacking_catboost_model.cbm',\n",
    "        'xgboost': 'models_stacking/stacking_xgboost_model.json',\n",
    "        #'lightgbm': 'models_stacking/stacking_lightgbm_model.txt',\n",
    "        #'mlp': 'models_stacking/stacking_mlp_model.pkl',\n",
    "        'torch_mlp': 'models_stacking/mlp_torch_model.pt'\n",
    "\n",
    "        \n",
    "    }\n",
    "    \n",
    "    # Load models\n",
    "    models = load_models(model_paths)\n",
    "    \n",
    "    # Generate level 1 features\n",
    "    level1_train, level1_test = generate_level1_features(\n",
    "        models, X_train, X_test, y_train, categorical_features\n",
    "    )\n",
    "    \n",
    "    # Train meta-model\n",
    "    meta_model, test_preds = train_meta_model(level1_train, y_train, level1_test)\n",
    "    \n",
    "    # Create submission file\n",
    "    create_submission(test_ids, test_preds, 'submissions_final_stacking.csv')\n",
    "    \n",
    "    # Save meta-model\n",
    "    joblib.dump(meta_model, 'results_stacking/meta_model.pkl')\n",
    "    print(\"Meta-model saved to results_stacking/meta_model.pkl\")\n",
    "    \n",
    "    # End timing\n",
    "    end_time = time.time()\n",
    "    print(f\"Total execution time: {end_time - start_time:.2f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
