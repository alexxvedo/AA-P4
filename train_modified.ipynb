{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cargando datos procesados...\n",
      "Dimensiones del conjunto de entrenamiento: (18299, 73)\n",
      "Dimensiones del conjunto de prueba: (10000, 71)\n",
      "\n",
      "Preparando datos para el modelado...\n",
      "Usando transformación logarítmica de precios para el modelado...\n",
      "Tamaño del train: (14639, 63), validación: (3660, 63)\n"
     ]
    }
   ],
   "source": [
    "# Modelado Avanzado para Competición Kaggle - Precios de Viviendas en Galicia\n",
    "# =============================================================================\n",
    "# Importación de bibliotecas necesarias\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold, RandomizedSearchCV, cross_val_score\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, HuberRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor, VotingRegressor, StackingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.feature_selection import SelectFromModel, RFECV\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import catboost as cb\n",
    "import os\n",
    "import joblib\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuración para reproducibilidad\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# Configuración de visualización\n",
    "sns.set_palette(\"viridis\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# %%\n",
    "# =============================================================================\n",
    "# 1. CARGA DE DATOS PROCESADOS\n",
    "# =============================================================================\n",
    "print(\"Cargando datos procesados...\")\n",
    "if not os.path.exists('train_processed.csv') or not os.path.exists('test_processed.csv'):\n",
    "    raise FileNotFoundError(\"No se encontraron los archivos procesados. Ejecute primero el script de preprocesamiento.\")\n",
    "\n",
    "train_data = pd.read_csv('train_processed.csv')\n",
    "test_data = pd.read_csv('test_processed.csv')\n",
    "\n",
    "print(f\"Dimensiones del conjunto de entrenamiento: {train_data.shape}\")\n",
    "print(f\"Dimensiones del conjunto de prueba: {test_data.shape}\")\n",
    "\n",
    "# %%\n",
    "# =============================================================================\n",
    "# 2. PREPARACIÓN PARA EL MODELADO\n",
    "# =============================================================================\n",
    "print(\"\\nPreparando datos para el modelado...\")\n",
    "\n",
    "# Verificar si tenemos transformación logarítmica de los precios\n",
    "if 'log_prezo' in train_data.columns:\n",
    "    print(\"Usando transformación logarítmica de precios para el modelado...\")\n",
    "    y = train_data['log_prezo']\n",
    "    use_log = True\n",
    "else:\n",
    "    print(\"Usando precios originales para el modelado...\")\n",
    "    y = train_data['prezo_euros']\n",
    "    use_log = False\n",
    "\n",
    "# Excluir ID, target, indicador outlier y variables categóricas originales\n",
    "exclude = ['id', 'prezo_euros', 'log_prezo', 'is_outlier',\n",
    "           'tipo_edificacion', 'calidade_materiais', 'cor_favorita_propietario',\n",
    "           'acceso_transporte_publico', 'orientacion', 'eficiencia_enerxetica']\n",
    "\n",
    "feature_cols = [c for c in train_data.columns if c not in exclude]\n",
    "X = train_data[feature_cols]\n",
    "\n",
    "# Imputar faltantes restantes\n",
    "if X.isnull().sum().sum() > 0:\n",
    "    print(\"ADVERTENCIA: Hay valores faltantes en las características. Imputando con la mediana...\")\n",
    "    X = X.fillna(X.median())\n",
    "\n",
    "# Dividir en train/validation con un split específico para evaluación final\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=RANDOM_SEED)\n",
    "print(f\"Tamaño del train: {X_train.shape}, validación: {X_val.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Analizando importancia de características...\n",
      "\n",
      "Top 15 características más importantes:\n",
      "                               feature  importance\n",
      "14                    superficie_total    0.751029\n",
      "15             ratio_interior_exterior    0.073838\n",
      "25                      calidade_valor    0.018210\n",
      "11               numero_arboles_xardin    0.012165\n",
      "18                         dist_coruna    0.010506\n",
      "5                            lonxitude    0.009314\n",
      "10                indice_criminalidade    0.009043\n",
      "13           superficie_por_habitacion    0.007520\n",
      "6                             latitude    0.007069\n",
      "16                      densidad_banos    0.007037\n",
      "7   temperatura_media_mes_construccion    0.006974\n",
      "8                  distancia_centro_km    0.006619\n",
      "1               superficie_exterior_m2    0.006360\n",
      "20                       dist_santiago    0.006247\n",
      "9                  distancia_escola_km    0.005888\n",
      "\n",
      "Creando características polinómicas para las top 5 características...\n",
      "Tamaño del conjunto de datos con características polinómicas: (17576, 78)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# =============================================================================\n",
    "# 3. ANÁLISIS DE CARACTERÍSTICAS\n",
    "# =============================================================================\n",
    "print(\"\\nAnalizando importancia de características...\")\n",
    "\n",
    "# Utilizamos un RandomForest para estimar importancia inicial de características\n",
    "feature_analyzer = RandomForestRegressor(n_estimators=100, random_state=RANDOM_SEED, n_jobs=-1)\n",
    "feature_analyzer.fit(X_train, y_train)\n",
    "\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'importance': feature_analyzer.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 15 características más importantes:\")\n",
    "print(feature_importance.head(15))\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='importance', y='feature', data=feature_importance.head(20))\n",
    "plt.title('Importancia de Características (Random Forest)')\n",
    "plt.tight_layout()\n",
    "plt.savefig('feature_importance.png')\n",
    "plt.close()\n",
    "\n",
    "# Obtener las 20 características más importantes para posible filtrado\n",
    "top_features = feature_importance.head(20)['feature'].tolist()\n",
    "\n",
    "# Añadir ingeniería de características polinómicas para las top 5 features\n",
    "print(\"\\nCreando características polinómicas para las top 5 características...\")\n",
    "poly_features = PolynomialFeatures(degree=2, include_bias=False, interaction_only=False)\n",
    "top5_features = feature_importance.head(5)['feature'].tolist()\n",
    "\n",
    "X_train_poly = X_train.copy()\n",
    "X_val_poly = X_val.copy()\n",
    "\n",
    "poly_features_df = pd.DataFrame(\n",
    "    poly_features.fit_transform(X_train[top5_features]),\n",
    "    columns=poly_features.get_feature_names_out(top5_features)\n",
    ")\n",
    "\n",
    "poly_features_df_val = pd.DataFrame(\n",
    "    poly_features.transform(X_val[top5_features]),\n",
    "    columns=poly_features.get_feature_names_out(top5_features)\n",
    ")\n",
    "\n",
    "# Eliminar las columnas originales para evitar duplicados\n",
    "poly_features_df = poly_features_df.drop(columns=top5_features)\n",
    "poly_features_df_val = poly_features_df_val.drop(columns=top5_features)\n",
    "\n",
    "# Unir con el conjunto original\n",
    "X_train_poly = pd.concat([X_train, poly_features_df], axis=1)\n",
    "X_val_poly = pd.concat([X_val, poly_features_df_val], axis=1)\n",
    "\n",
    "print(f\"Tamaño del conjunto de datos con características polinómicas: {X_train_poly.shape}\")\n",
    "\n",
    "# %%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# 4. FUNCIONES AUXILIARES PARA EVALUACIÓN\n",
    "# =============================================================================\n",
    "def evaluate_model(model, X_val, y_val, use_log=False):\n",
    "    \"\"\"Evalúa un modelo en conjunto de validación y devuelve métricas\"\"\"\n",
    "    y_pred = model.predict(X_val)\n",
    "    \n",
    "    if use_log:\n",
    "        y_pred = np.expm1(y_pred)\n",
    "        y_true = np.expm1(y_val)\n",
    "    else:\n",
    "        y_true = y_val\n",
    "        \n",
    "    return {\n",
    "        'MAE': mean_absolute_error(y_true, y_pred),\n",
    "        'RMSE': np.sqrt(mean_squared_error(y_true, y_pred)),\n",
    "        'R2': r2_score(y_true, y_pred)\n",
    "    }\n",
    "\n",
    "def cross_validate_model(model, X, y, cv=5, use_log=False):\n",
    "    \"\"\"Realiza validación cruzada y devuelve métricas promedio\"\"\"\n",
    "    kf = KFold(n_splits=cv, shuffle=True, random_state=RANDOM_SEED)\n",
    "    maes, rmses, r2s = [], [], []\n",
    "    is_xgb = isinstance(model, xgb.XGBRegressor)\n",
    "\n",
    "    for train_idx, val_idx in kf.split(X):\n",
    "        X_tr = X.iloc[train_idx]\n",
    "        X_va = X.iloc[val_idx]\n",
    "        y_tr = y.iloc[train_idx]\n",
    "        y_va = y.iloc[val_idx]\n",
    "\n",
    "        if is_xgb:\n",
    "            X_tr = X_tr.values\n",
    "            X_va = X_va.values\n",
    "            y_tr = y_tr.values\n",
    "\n",
    "        model.fit(X_tr, y_tr)\n",
    "        y_pred = model.predict(X_va)\n",
    "\n",
    "        if use_log:\n",
    "            y_pred = np.expm1(y_pred)\n",
    "            y_va_true = np.expm1(y_va)\n",
    "        else:\n",
    "            y_va_true = y_va\n",
    "\n",
    "        maes.append(mean_absolute_error(y_va_true, y_pred))\n",
    "        rmses.append(np.sqrt(mean_squared_error(y_va_true, y_pred)))\n",
    "        r2s.append(r2_score(y_va_true, y_pred))\n",
    "\n",
    "    return {\n",
    "        'MAE_CV': np.mean(maes),\n",
    "        'MAE_STD': np.std(maes),\n",
    "        'RMSE_CV': np.mean(rmses),\n",
    "        'RMSE_STD': np.std(rmses),\n",
    "        'R2_CV': np.mean(r2s),\n",
    "        'R2_STD': np.std(r2s)\n",
    "    }\n",
    "\n",
    "def make_submission(model, test_data, feature_cols, filename='submission.csv', use_log=False, \n",
    "                    poly_features=None, top5_features=None):\n",
    "    \"\"\"Genera archivo de submisión para Kaggle\"\"\"\n",
    "    X_test = test_data[feature_cols]\n",
    "    \n",
    "    # Imputar valores faltantes si existen\n",
    "    if X_test.isnull().sum().sum() > 0:\n",
    "        X_test = X_test.fillna(X_test.median())\n",
    "    \n",
    "    # Aplicar transformación polinómica si corresponde\n",
    "    if poly_features is not None and top5_features is not None:\n",
    "        poly_features_test = pd.DataFrame(\n",
    "            poly_features.transform(X_test[top5_features]),\n",
    "            columns=poly_features.get_feature_names_out(top5_features)\n",
    "        )\n",
    "        # Eliminar las columnas originales\n",
    "        poly_features_test = poly_features_test.drop(columns=top5_features)\n",
    "        # Unir con el conjunto original\n",
    "        X_test = pd.concat([X_test, poly_features_test], axis=1)\n",
    "    \n",
    "    # Predicción\n",
    "    preds = model.predict(X_test)\n",
    "    \n",
    "    # Transformar de vuelta si se usó log\n",
    "    if use_log:\n",
    "        preds = np.expm1(preds)\n",
    "        \n",
    "    # Crear y guardar submisión\n",
    "    submission = pd.DataFrame({'id': test_data['id'], 'prezo_euros': preds})\n",
    "    submission.to_csv(filename, index=False)\n",
    "    print(f\"Submission guardado como {filename}\")\n",
    "    \n",
    "    # Visualizar distribución de predicciones\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(preds, bins=50, kde=True)\n",
    "    plt.title('Distribución de Predicciones')\n",
    "    plt.xlabel('Precio (euros)')\n",
    "    plt.ylabel('Frecuencia')\n",
    "    plt.savefig('predicciones_distribucion.png')\n",
    "    plt.close()\n",
    "    \n",
    "    return submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Entrenando modelos básicos...\n",
      "\n",
      "Entrenando con características originales:\n",
      "XGBoost: MAE=31315.25, RMSE=44825.64, R2=0.9246, time=12.6s\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.050808 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4477\n",
      "[LightGBM] [Info] Number of data points in the train set: 14639, number of used features: 63\n",
      "[LightGBM] [Info] Start training from score 12.146740\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.046284 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4477\n",
      "[LightGBM] [Info] Number of data points in the train set: 14639, number of used features: 63\n",
      "[LightGBM] [Info] Start training from score 12.146740\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001190 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4473\n",
      "[LightGBM] [Info] Number of data points in the train set: 14639, number of used features: 63\n",
      "[LightGBM] [Info] Start training from score 12.151242\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.006076 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4472\n",
      "[LightGBM] [Info] Number of data points in the train set: 14639, number of used features: 63\n",
      "[LightGBM] [Info] Start training from score 12.152273\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001268 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4474\n",
      "[LightGBM] [Info] Number of data points in the train set: 14639, number of used features: 63\n",
      "[LightGBM] [Info] Start training from score 12.153129\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.001216 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4473\n",
      "[LightGBM] [Info] Number of data points in the train set: 14640, number of used features: 63\n",
      "[LightGBM] [Info] Start training from score 12.150144\n",
      "LightGBM: MAE=31063.14, RMSE=44594.83, R2=0.9254, time=654.1s\n",
      "CatBoost: MAE=31024.13, RMSE=44614.43, R2=0.9253, time=18.4s\n",
      "LinearRegression: MAE=36439.65, RMSE=55665.83, R2=0.8837, time=0.0s\n",
      "Ridge: MAE=36276.81, RMSE=55237.85, R2=0.8855, time=0.0s\n",
      "Lasso: MAE=36272.52, RMSE=55495.75, R2=0.8844, time=0.4s\n",
      "ElasticNet: MAE=36243.12, RMSE=55386.42, R2=0.8849, time=1.9s\n",
      "HuberRegressor: MAE=35916.04, RMSE=65209.61, R2=0.8404, time=3.2s\n",
      "RandomForest: MAE=33548.97, RMSE=47597.98, R2=0.9150, time=1.5s\n",
      "ExtraTrees: MAE=33340.31, RMSE=47441.48, R2=0.9155, time=0.9s\n",
      "GradientBoosting: MAE=31607.48, RMSE=45191.65, R2=0.9233, time=17.0s\n",
      "\n",
      "Entrenando modelos con características polinómicas:\n"
     ]
    },
    {
     "ename": "XGBoostError",
     "evalue": "[17:32:11] ../src/data/data.cc:452: Check failed: this->labels.Size() % this->num_row_ == 0 (14639 vs. 0) : Incorrect size for labels.\nStack trace:\n  [bt] (0) /mnt/netapp2/Store_uni/home/usc/ci/avs/personal/aprendizaje/p4/venvP4/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x16b9c9) [0x7fb768f1b9c9]\n  [bt] (1) /mnt/netapp2/Store_uni/home/usc/ci/avs/personal/aprendizaje/p4/venvP4/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x177f8a) [0x7fb768f27f8a]\n  [bt] (2) /mnt/netapp2/Store_uni/home/usc/ci/avs/personal/aprendizaje/p4/venvP4/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x179d10) [0x7fb768f29d10]\n  [bt] (3) /mnt/netapp2/Store_uni/home/usc/ci/avs/personal/aprendizaje/p4/venvP4/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGDMatrixSetInfoFromInterface+0xa4) [0x7fb768e6c754]\n  [bt] (4) /mnt/netapp1/Optcesga_FT2_RHEL7/2020/gentoo/22072020/usr/lib64/libffi.so.7(+0x6bdd) [0x7fb7b9f71bdd]\n  [bt] (5) /mnt/netapp1/Optcesga_FT2_RHEL7/2020/gentoo/22072020/usr/lib64/libffi.so.7(+0x6149) [0x7fb7b9f71149]\n  [bt] (6) /mnt/netapp1/Optcesga_FT2_RHEL7/2020/gentoo/22072020/usr/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2f6) [0x7fb7b8852176]\n  [bt] (7) /mnt/netapp1/Optcesga_FT2_RHEL7/2020/gentoo/22072020/usr/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0xa341) [0x7fb7b884b341]\n  [bt] (8) /mnt/netapp1/Optcesga_FT2_RHEL7/2020/gentoo/22072020/usr/lib64/libpython3.7m.so.1.0(_PyObject_FastCallKeywords+0xd7) [0x7fb7bc31c307]\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mXGBoostError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/mnt/lustre/scratch/nlsas/home/usc/ci/avs/desktop.5FZAnOkU/ipykernel_1785909/49832090.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mboosting_algorithms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m     \u001b[0mtrain_and_record\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train_poly\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_val_poly\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nResumen inicial:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/lustre/scratch/nlsas/home/usc/ci/avs/desktop.5FZAnOkU/ipykernel_1785909/49832090.py\u001b[0m in \u001b[0;36mtrain_and_record\u001b[0;34m(name, model, X_tr, X_va)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mt0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_tr_fit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_tr_fit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mt0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/netapp2/Store_uni/home/usc/ci/avs/personal/aprendizaje/p4/venvP4/lib/python3.7/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    573\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/netapp2/Store_uni/home/usc/ci/avs/personal/aprendizaje/p4/venvP4/lib/python3.7/site-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, base_margin_eval_set, feature_weights, callbacks)\u001b[0m\n\u001b[1;32m    944\u001b[0m             \u001b[0meval_qid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m             \u001b[0mcreate_dmatrix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDMatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnthread\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 946\u001b[0;31m             \u001b[0menable_categorical\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_categorical\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    947\u001b[0m         )\n\u001b[1;32m    948\u001b[0m         \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_xgb_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/netapp2/Store_uni/home/usc/ci/avs/personal/aprendizaje/p4/venvP4/lib/python3.7/site-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36m_wrap_evaluation_matrices\u001b[0;34m(missing, X, y, group, qid, sample_weight, base_margin, feature_weights, eval_set, sample_weight_eval_set, base_margin_eval_set, eval_group, eval_qid, create_dmatrix, enable_categorical)\u001b[0m\n\u001b[1;32m    408\u001b[0m         \u001b[0mfeature_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0mmissing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmissing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 410\u001b[0;31m         \u001b[0menable_categorical\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menable_categorical\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    411\u001b[0m     )\n\u001b[1;32m    412\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/netapp2/Store_uni/home/usc/ci/avs/personal/aprendizaje/p4/venvP4/lib/python3.7/site-packages/xgboost/sklearn.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(**kwargs)\u001b[0m\n\u001b[1;32m    943\u001b[0m             \u001b[0meval_group\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m             \u001b[0meval_qid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 945\u001b[0;31m             \u001b[0mcreate_dmatrix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDMatrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnthread\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    946\u001b[0m             \u001b[0menable_categorical\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_categorical\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m         )\n",
      "\u001b[0;32m/mnt/netapp2/Store_uni/home/usc/ci/avs/personal/aprendizaje/p4/venvP4/lib/python3.7/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    573\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/netapp2/Store_uni/home/usc/ci/avs/personal/aprendizaje/p4/venvP4/lib/python3.7/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, data, label, weight, base_margin, missing, silent, feature_names, feature_types, nthread, group, qid, label_lower_bound, label_upper_bound, feature_weights, enable_categorical)\u001b[0m\n\u001b[1;32m    703\u001b[0m             \u001b[0mlabel_lower_bound\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabel_lower_bound\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0mlabel_upper_bound\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabel_upper_bound\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m             \u001b[0mfeature_weights\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeature_weights\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m         )\n\u001b[1;32m    707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/netapp2/Store_uni/home/usc/ci/avs/personal/aprendizaje/p4/venvP4/lib/python3.7/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    573\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 575\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    576\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/netapp2/Store_uni/home/usc/ci/avs/personal/aprendizaje/p4/venvP4/lib/python3.7/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36mset_info\u001b[0;34m(self, label, weight, base_margin, group, qid, label_lower_bound, label_upper_bound, feature_names, feature_types, feature_weights)\u001b[0m\n\u001b[1;32m    760\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    761\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlabel\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 762\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    763\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mweight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/netapp2/Store_uni/home/usc/ci/avs/personal/aprendizaje/p4/venvP4/lib/python3.7/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36mset_label\u001b[0;34m(self, label)\u001b[0m\n\u001b[1;32m    891\u001b[0m         \"\"\"\n\u001b[1;32m    892\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdispatch_meta_backend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 893\u001b[0;31m         \u001b[0mdispatch_meta_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'label'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'float'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    895\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_weight\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mArrayLike\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/netapp2/Store_uni/home/usc/ci/avs/personal/aprendizaje/p4/venvP4/lib/python3.7/site-packages/xgboost/data.py\u001b[0m in \u001b[0;36mdispatch_meta_backend\u001b[0;34m(matrix, data, name, dtype)\u001b[0m\n\u001b[1;32m   1026\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1027\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_numpy_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1028\u001b[0;31m         \u001b[0m_meta_from_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1029\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1030\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0m_is_pandas_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/netapp2/Store_uni/home/usc/ci/avs/personal/aprendizaje/p4/venvP4/lib/python3.7/site-packages/xgboost/data.py\u001b[0m in \u001b[0;36m_meta_from_numpy\u001b[0;34m(data, field, dtype, handle)\u001b[0m\n\u001b[1;32m    967\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Masked array is not supported.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    968\u001b[0m     \u001b[0minterface_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_array_interface\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 969\u001b[0;31m     \u001b[0m_check_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXGDMatrixSetInfoFromInterface\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfield\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minterface_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    970\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    971\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/netapp2/Store_uni/home/usc/ci/avs/personal/aprendizaje/p4/venvP4/lib/python3.7/site-packages/xgboost/core.py\u001b[0m in \u001b[0;36m_check_call\u001b[0;34m(ret)\u001b[0m\n\u001b[1;32m    244\u001b[0m     \"\"\"\n\u001b[1;32m    245\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 246\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mXGBoostError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpy_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mXGBGetLastError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mXGBoostError\u001b[0m: [17:32:11] ../src/data/data.cc:452: Check failed: this->labels.Size() % this->num_row_ == 0 (14639 vs. 0) : Incorrect size for labels.\nStack trace:\n  [bt] (0) /mnt/netapp2/Store_uni/home/usc/ci/avs/personal/aprendizaje/p4/venvP4/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x16b9c9) [0x7fb768f1b9c9]\n  [bt] (1) /mnt/netapp2/Store_uni/home/usc/ci/avs/personal/aprendizaje/p4/venvP4/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x177f8a) [0x7fb768f27f8a]\n  [bt] (2) /mnt/netapp2/Store_uni/home/usc/ci/avs/personal/aprendizaje/p4/venvP4/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(+0x179d10) [0x7fb768f29d10]\n  [bt] (3) /mnt/netapp2/Store_uni/home/usc/ci/avs/personal/aprendizaje/p4/venvP4/lib/python3.7/site-packages/xgboost/lib/libxgboost.so(XGDMatrixSetInfoFromInterface+0xa4) [0x7fb768e6c754]\n  [bt] (4) /mnt/netapp1/Optcesga_FT2_RHEL7/2020/gentoo/22072020/usr/lib64/libffi.so.7(+0x6bdd) [0x7fb7b9f71bdd]\n  [bt] (5) /mnt/netapp1/Optcesga_FT2_RHEL7/2020/gentoo/22072020/usr/lib64/libffi.so.7(+0x6149) [0x7fb7b9f71149]\n  [bt] (6) /mnt/netapp1/Optcesga_FT2_RHEL7/2020/gentoo/22072020/usr/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(_ctypes_callproc+0x2f6) [0x7fb7b8852176]\n  [bt] (7) /mnt/netapp1/Optcesga_FT2_RHEL7/2020/gentoo/22072020/usr/lib/python3.7/lib-dynload/_ctypes.cpython-37m-x86_64-linux-gnu.so(+0xa341) [0x7fb7b884b341]\n  [bt] (8) /mnt/netapp1/Optcesga_FT2_RHEL7/2020/gentoo/22072020/usr/lib64/libpython3.7m.so.1.0(_PyObject_FastCallKeywords+0xd7) [0x7fb7bc31c307]\n\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# =============================================================================\n",
    "# 5. ENTRENAMIENTO DE MODELOS BÁSICOS MEJORADOS\n",
    "# =============================================================================\n",
    "print(\"\\nEntrenando modelos básicos...\")\n",
    "\n",
    "models = {}\n",
    "\n",
    "def train_and_record(name, model, X_tr=X_train, X_va=X_val):\n",
    "    \"\"\"Entrena, evalúa y registra resultados de un modelo\"\"\"\n",
    "    # si es XGB, pasamos arrays\n",
    "    if isinstance(model, xgb.XGBRegressor):\n",
    "        X_tr_fit = X_tr.values\n",
    "        y_tr_fit = y_train.values\n",
    "    else:\n",
    "        X_tr_fit = X_tr\n",
    "        y_tr_fit = y_train\n",
    "\n",
    "    t0 = time.time()\n",
    "    model.fit(X_tr_fit, y_tr_fit)\n",
    "    t = time.time() - t0\n",
    "\n",
    "    ev = evaluate_model(model,\n",
    "                        X_va.values if isinstance(model, xgb.XGBRegressor) else X_va,\n",
    "                        y_val, use_log)\n",
    "    cv = cross_validate_model(model, X, y, cv=5, use_log=use_log)\n",
    "\n",
    "    models[name] = {**ev, **cv, 'train_time': t, 'model': model}\n",
    "    print(f\"{name}: MAE={ev['MAE']:.2f}, RMSE={ev['RMSE']:.2f}, \"\n",
    "          f\"R2={ev['R2']:.4f}, time={t:.1f}s\")\n",
    "\n",
    "# Lista de algoritmos básicos a probar\n",
    "algorithms = [\n",
    "    ('XGBoost', xgb.XGBRegressor(n_estimators=200, learning_rate=0.05, n_jobs=-1, random_state=RANDOM_SEED)),\n",
    "    ('LightGBM', lgb.LGBMRegressor(n_estimators=200, learning_rate=0.05, n_jobs=-1, random_state=RANDOM_SEED)),\n",
    "    ('CatBoost', cb.CatBoostRegressor(iterations=200, learning_rate=0.05, verbose=0, random_state=RANDOM_SEED)),\n",
    "    ('LinearRegression', LinearRegression()),\n",
    "    ('Ridge', Ridge(alpha=1.0, random_state=RANDOM_SEED)),\n",
    "    ('Lasso', Lasso(alpha=0.001, max_iter=10000, random_state=RANDOM_SEED)),\n",
    "    ('ElasticNet', ElasticNet(alpha=0.001, l1_ratio=0.5, max_iter=10000, random_state=RANDOM_SEED)),\n",
    "    ('HuberRegressor', HuberRegressor(epsilon=1.35, max_iter=1000)),\n",
    "    ('RandomForest', RandomForestRegressor(n_estimators=200, n_jobs=-1, random_state=RANDOM_SEED)),\n",
    "    ('ExtraTrees', ExtraTreesRegressor(n_estimators=200, n_jobs=-1, random_state=RANDOM_SEED)),\n",
    "    ('GradientBoosting', GradientBoostingRegressor(n_estimators=200, random_state=RANDOM_SEED))\n",
    "    \n",
    "]\n",
    "\n",
    "# Entrenar modelos con datos originales\n",
    "print(\"\\nEntrenando con características originales:\")\n",
    "for n, m in algorithms: \n",
    "    train_and_record(n, m)\n",
    "\n",
    "# Entrenar modelos con datos polinómicos para los mejores algoritmos\n",
    "print(\"\\nEntrenando modelos con características polinómicas:\")\n",
    "boosting_algorithms = [\n",
    "    ('XGB_Poly', xgb.XGBRegressor(n_estimators=200, learning_rate=0.05, n_jobs=-1, random_state=RANDOM_SEED)),\n",
    "    ('LGBM_Poly', lgb.LGBMRegressor(n_estimators=200, learning_rate=0.05, n_jobs=-1, random_state=RANDOM_SEED)),\n",
    "    ('CatBoost_Poly', cb.CatBoostRegressor(iterations=200, learning_rate=0.05, verbose=0, random_state=RANDOM_SEED))\n",
    "]\n",
    "\n",
    "#for n, m in boosting_algorithms:\n",
    "    #train_and_record(n, m, X_train_poly, X_val_poly)\n",
    "\n",
    "print(\"\\nResumen inicial:\")\n",
    "init_results = pd.DataFrame({m: {\n",
    "    'MAE': models[m]['MAE'],\n",
    "    'RMSE': models[m]['RMSE'],\n",
    "    'R2': models[m]['R2'],\n",
    "    'MAE_CV': models[m]['MAE_CV'],\n",
    "    'R2_CV': models[m]['R2_CV']\n",
    "} for m in models}).T.sort_values('MAE')\n",
    "\n",
    "print(init_results)\n",
    "\n",
    "# Visualizar comparación de modelos\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=init_results.index, y='MAE', data=init_results.reset_index().rename(columns={'index': 'Modelo'}))\n",
    "plt.title('Comparación de MAE por Modelo')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.savefig('model_comparison.png')\n",
    "plt.close()\n",
    "\n",
    "# %%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Resumen inicial:\n",
      "                           MAE          RMSE        R2        MAE_CV     R2_CV\n",
      "CatBoost          31024.128159  44614.429490  0.925288  31198.527833  0.926501\n",
      "LightGBM          31063.139412  44594.832447  0.925353  31177.872447  0.926937\n",
      "XGBoost           31315.254844  44825.635264  0.924578  31322.882352  0.926336\n",
      "GradientBoosting  31607.477038  45191.647352  0.923342  31459.520185  0.925520\n",
      "ExtraTrees        33340.314376  47441.480077  0.915519  33199.166818  0.917942\n",
      "RandomForest      33548.969020  47597.978540  0.914961  33638.606645  0.915576\n",
      "HuberRegressor    35916.036264  65209.613107  0.840388  36575.046703  0.779370\n",
      "ElasticNet        36243.124905  55386.424869  0.884854  36740.238211  0.861187\n",
      "Lasso             36272.524831  55495.746006  0.884399  36782.050382  0.860836\n",
      "Ridge             36276.807190  55237.845212  0.885471  36752.244745  0.862013\n",
      "LinearRegression  36439.646462  55665.831086  0.883689  36857.916454  0.860847\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nResumen inicial:\")\n",
    "init_results = pd.DataFrame({m: {\n",
    "    'MAE': models[m]['MAE'],\n",
    "    'RMSE': models[m]['RMSE'],\n",
    "    'R2': models[m]['R2'],\n",
    "    'MAE_CV': models[m]['MAE_CV'],\n",
    "    'R2_CV': models[m]['R2_CV']\n",
    "} for m in models}).T.sort_values('MAE')\n",
    "\n",
    "print(init_results)\n",
    "\n",
    "# Visualizar comparación de modelos\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=init_results.index, y='MAE', data=init_results.reset_index().rename(columns={'index': 'Modelo'}))\n",
    "plt.title('Comparación de MAE por Modelo')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.savefig('model_comparison.png')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Seleccionados para optimización: ['CatBoost', 'LightGBM', 'XGBoost', 'GradientBoosting', 'ExtraTrees']\n",
      "\n",
      "Optimizando CatBoost...\n",
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n",
      "CatBoost mejores parámetros: {'learning_rate': 0.05, 'l2_leaf_reg': 1, 'iterations': 300, 'depth': 6, 'border_count': 32}\n",
      "CatBoost MAE: 30719.4807, RMSE: 44111.9044, R2: 0.9270\n",
      "\n",
      "Optimizando LightGBM...\n",
      "Fitting 5 folds for each of 25 candidates, totalling 125 fits\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.080929 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4469\n",
      "[LightGBM] [Info] Number of data points in the train set: 11711, number of used features: 63\n",
      "[LightGBM] [Info] Start training from score 12.144955\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.950732 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4469\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.741481 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4471\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 2.798528 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4471\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.179195 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4471\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 2.203832 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4469\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.727693 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4475\n",
      "[LightGBM] [Info] Number of data points in the train set: 11711, number of used features: 63\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 2.277535 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4473\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 2.190816 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4471\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.601520 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4475\n",
      "[LightGBM] [Info] Number of data points in the train set: 11711, number of used features: 63\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 2.326928 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4473\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.728913 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4473\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.962193 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4471\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.743926 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4475\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 2.099152 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4469\n",
      "[LightGBM] [Info] Number of data points in the train set: 11711, number of used features: 63\n",
      "[LightGBM] [Info] Number of data points in the train set: 11711, number of used features: 63\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of data points in the train set: 11711, number of used features: 63\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of data points in the train set: 11711, number of used features: 63\n",
      "[LightGBM] [Info] Start training from score 12.146506\n",
      "[LightGBM] [Info] Number of data points in the train set: 11711, number of used features: 63\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of data points in the train set: 11712, number of used features: 63\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 2.870921 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4476\n",
      "[LightGBM] [Info] Number of data points in the train set: 11711, number of used features: 63\n",
      "[LightGBM] [Info] Number of data points in the train set: 11711, number of used features: 63\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.746266 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4475\n",
      "[LightGBM] [Info] Start training from score 12.150571\n",
      "[LightGBM] [Info] Number of data points in the train set: 11712, number of used features: 63\n",
      "[LightGBM] [Info] Number of data points in the train set: 11712, number of used features: 63\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 2.786812 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4476\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 2.624734 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4476\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of data points in the train set: 11711, number of used features: 63\n",
      "[LightGBM] [Info] Number of data points in the train set: 11711, number of used features: 63\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 2.507508 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4476\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 3.305425 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4469\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 3.504963 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4475\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.799027 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4469\n",
      "[LightGBM] [Info] Start training from score 12.146506\n",
      "[LightGBM] [Info] Number of data points in the train set: 11711, number of used features: 63\n",
      "[LightGBM] [Info] Start training from score 12.150571\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Start training from score 12.144955\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 2.030079 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4473\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of data points in the train set: 11711, number of used features: 63\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 3.257136 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4475\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 3.418831 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4469\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 3.022920 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4475\n",
      "[LightGBM] [Info] Start training from score 12.146506\n",
      "[LightGBM] [Info] Start training from score 12.146506\n",
      "[LightGBM] [Info] Number of data points in the train set: 11711, number of used features: 63\n",
      "[LightGBM] [Info] Number of data points in the train set: 11711, number of used features: 63\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 3.453771 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4473\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 2.540767 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4471\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 3.584123 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4471\n",
      "[LightGBM] [Info] Start training from score 12.146506\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Number of data points in the train set: 11711, number of used features: 63\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 2.306950 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4471\n",
      "[LightGBM] [Info] Start training from score 12.144955\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 2.818113 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4473\n",
      "[LightGBM] [Info] Number of data points in the train set: 11711, number of used features: 63\n",
      "[LightGBM] [Info] Number of data points in the train set: 11712, number of used features: 63\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.829305 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4471\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 2.853772 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4471\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 3.316664 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4473\n",
      "[LightGBM] [Info] Number of data points in the train set: 11711, number of used features: 63\n",
      "[LightGBM] [Info] Start training from score 12.142794\n",
      "[LightGBM] [Info] Number of data points in the train set: 11711, number of used features: 63\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 2.515560 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4476\n",
      "[LightGBM] [Info] Number of data points in the train set: 11711, number of used features: 63\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 3.371035 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4469\n",
      "[LightGBM] [Info] Start training from score 12.142794\n",
      "[LightGBM] [Info] Start training from score 12.150571\n",
      "[LightGBM] [Info] Number of data points in the train set: 11711, number of used features: 63\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 4.029174 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4473\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.778100 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4473\n",
      "[LightGBM] [Info] Number of data points in the train set: 11711, number of used features: 63\n",
      "[LightGBM] [Info] Number of data points in the train set: 11711, number of used features: 63\n",
      "[LightGBM] [Info] Number of data points in the train set: 11712, number of used features: 63\n",
      "[LightGBM] [Info] Start training from score 12.148875\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 3.673800 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4476\n",
      "[LightGBM] [Info] Number of data points in the train set: 11711, number of used features: 63\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.704220 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4473\n",
      "[LightGBM] [Info] Number of data points in the train set: 11711, number of used features: 63\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 4.553267 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4476\n",
      "[LightGBM] [Info] Start training from score 12.144955\n",
      "[LightGBM] [Info] Number of data points in the train set: 11711, number of used features: 63\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 3.588772 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4469\n",
      "[LightGBM] [Info] Number of data points in the train set: 11711, number of used features: 63\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 3.266885 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4469\n",
      "[LightGBM] [Info] Start training from score 12.142794\n",
      "[LightGBM] [Info] Number of data points in the train set: 11711, number of used features: 63\n",
      "[LightGBM] [Info] Number of data points in the train set: 11712, number of used features: 63\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 2.194251 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4473\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 2.342282 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4476\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 2.388052 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4476\n",
      "[LightGBM] [Info] Number of data points in the train set: 11712, number of used features: 63\n",
      "[LightGBM] [Info] Start training from score 12.150571\n",
      "[LightGBM] [Info] Number of data points in the train set: 11712, number of used features: 63\n",
      "[LightGBM] [Warning] Found whitespace in feature_names, replace with underlines\n",
      "[LightGBM] [Info] Start training from score 12.150571\n",
      "[LightGBM] [Info] Start training from score 12.142794\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 4.216782 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4471\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 3.291878 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4471\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 2.762793 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4475\n",
      "[LightGBM] [Info] Start training from score 12.144955\n",
      "[LightGBM] [Info] Start training from score 12.142794\n",
      "[LightGBM] [Info] Number of data points in the train set: 11712, number of used features: 63\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 3.349775 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4476\n",
      "[LightGBM] [Info] Start training from score 12.148875\n",
      "[LightGBM] [Info] Number of data points in the train set: 11711, number of used features: 63\n",
      "[LightGBM] [Info] Number of data points in the train set: 11712, number of used features: 63\n",
      "[LightGBM] [Info] Number of data points in the train set: 11711, number of used features: 63\n",
      "[LightGBM] [Info] Start training from score 12.146506\n",
      "[LightGBM] [Info] Number of data points in the train set: 11711, number of used features: 63\n",
      "[LightGBM] [Info] Start training from score 12.148875\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 1.313874 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4476\n",
      "[LightGBM] [Info] Number of data points in the train set: 11712, number of used features: 63\n",
      "[LightGBM] [Info] Number of data points in the train set: 11711, number of used features: 63\n",
      "[LightGBM] [Info] Number of data points in the train set: 11711, number of used features: 63\n",
      "[LightGBM] [Info] Number of data points in the train set: 11711, number of used features: 63\n",
      "[LightGBM] [Info] Start training from score 12.146506\n",
      "[LightGBM] [Info] Number of data points in the train set: 11711, number of used features: 63\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 2.032893 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4469\n",
      "[LightGBM] [Info] Number of data points in the train set: 11711, number of used features: 63\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 2.606837 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 4475\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 2.896214 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4475\n",
      "[LightGBM] [Info] Start training from score 12.142794\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 3.590066 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4475\n",
      "[LightGBM] [Info] Start training from score 12.144955\n",
      "[LightGBM] [Info] Start training from score 12.142794\n",
      "[LightGBM] [Info] Start training from score 12.144955\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 1.874901 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4475\n",
      "[LightGBM] [Info] Number of data points in the train set: 11711, number of used features: 63\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 2.647073 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4476\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 3.653544 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4469\n",
      "[LightGBM] [Info] Number of data points in the train set: 11711, number of used features: 63\n",
      "[LightGBM] [Info] Start training from score 12.142794\n",
      "[LightGBM] [Info] Start training from score 12.144955\n",
      "[LightGBM] [Info] Number of data points in the train set: 11711, number of used features: 63\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 3.779079 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4469\n",
      "[LightGBM] [Info] Number of data points in the train set: 11711, number of used features: 63\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 3.683789 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4471\n",
      "[LightGBM] [Info] Start training from score 12.150571\n",
      "[LightGBM] [Info] Start training from score 12.148875\n",
      "[LightGBM] [Info] Number of data points in the train set: 11711, number of used features: 63\n",
      "[LightGBM] [Info] Start training from score 12.144955\n",
      "[LightGBM] [Info] Number of data points in the train set: 11711, number of used features: 63\n",
      "[LightGBM] [Info] Number of data points in the train set: 11711, number of used features: 63\n",
      "[LightGBM] [Info] Number of data points in the train set: 11711, number of used features: 63\n",
      "[LightGBM] [Info] Number of data points in the train set: 11711, number of used features: 63\n",
      "[LightGBM] [Info] Start training from score 12.150571\n",
      "[LightGBM] [Info] Start training from score 12.142794\n",
      "[LightGBM] [Info] Start training from score 12.142794\n",
      "[LightGBM] [Info] Number of data points in the train set: 11711, number of used features: 63\n",
      "[LightGBM] [Info] Number of data points in the train set: 11711, number of used features: 63\n",
      "[LightGBM] [Info] Start training from score 12.148875\n",
      "[LightGBM] [Info] Start training from score 12.148875\n",
      "[LightGBM] [Info] Start training from score 12.142794\n",
      "[LightGBM] [Info] Number of data points in the train set: 11711, number of used features: 63\n",
      "[LightGBM] [Info] Start training from score 12.146506\n",
      "[LightGBM] [Info] Start training from score 12.144955\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 2.582966 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4473\n",
      "[LightGBM] [Info] Start training from score 12.148875\n",
      "[LightGBM] [Info] Start training from score 12.146506\n",
      "[LightGBM] [Info] Start training from score 12.146506\n",
      "[LightGBM] [Info] Start training from score 12.148875\n",
      "[LightGBM] [Info] Start training from score 12.148875\n",
      "[LightGBM] [Info] Start training from score 12.150571\n",
      "[LightGBM] [Info] Number of data points in the train set: 11711, number of used features: 63\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 3.981051 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4476\n",
      "[LightGBM] [Info] Start training from score 12.146506\n",
      "[LightGBM] [Info] Start training from score 12.144955\n",
      "[LightGBM] [Info] Number of data points in the train set: 11712, number of used features: 63\n",
      "[LightGBM] [Info] Start training from score 12.146506\n",
      "[LightGBM] [Info] Start training from score 12.148875\n",
      "[LightGBM] [Info] Start training from score 12.144955\n",
      "[LightGBM] [Info] Start training from score 12.148875\n",
      "[LightGBM] [Info] Start training from score 12.150571\n",
      "[LightGBM] [Info] Start training from score 12.150571\n",
      "[LightGBM] [Info] Number of data points in the train set: 11711, number of used features: 63\n",
      "[LightGBM] [Info] Start training from score 12.144955\n",
      "[LightGBM] [Info] Start training from score 12.146506\n",
      "[LightGBM] [Info] Start training from score 12.148875\n",
      "[LightGBM] [Info] Start training from score 12.150571\n",
      "[LightGBM] [Info] Start training from score 12.142794\n",
      "[LightGBM] [Info] Start training from score 12.150571\n",
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 4.090624 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 4475\n",
      "[LightGBM] [Info] Start training from score 12.148875\n",
      "[LightGBM] [Info] Number of data points in the train set: 11711, number of used features: 63\n",
      "[LightGBM] [Info] Start training from score 12.150571\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# 6. BÚSQUEDA EXHAUSTIVA DE HIPERPARÁMETROS PARA TOP MODELOS\n",
    "# =============================================================================\n",
    "# Seleccionar los 5 mejores modelos\n",
    "top5 = init_results.index[:5].tolist()\n",
    "print(f\"\\nSeleccionados para optimización: {top5}\")\n",
    "\n",
    "# Definir grids de búsqueda mucho más extensos\n",
    "param_grids = {\n",
    "    'Ridge': {\n",
    "        'alpha': [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]\n",
    "    },\n",
    "    'Lasso': {\n",
    "        'alpha': [0.0001, 0.0005, 0.001, 0.005, 0.01]\n",
    "    },\n",
    "    'ElasticNet': {\n",
    "        'alpha': [0.0001, 0.0005, 0.001, 0.005, 0.01],\n",
    "        'l1_ratio': [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "    },\n",
    "    'RandomForest': {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [None, 10, 20, 30],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        'max_features': ['sqrt', 'log2', None]\n",
    "    },\n",
    "    'ExtraTrees': {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [None, 10, 20, 30],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    },\n",
    "    'GradientBoosting': {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'learning_rate': [0.01, 0.05, 0.1],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'min_samples_split': [2, 5],\n",
    "        'subsample': [0.8, 0.9, 1.0]\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'learning_rate': [0.01, 0.05, 0.1],\n",
    "        'max_depth': [3, 5, 7, 9],\n",
    "        'subsample': [0.8, 0.9, 1.0],\n",
    "        'colsample_bytree': [0.8, 0.9, 1.0],\n",
    "        'gamma': [0, 0.1, 0.2]\n",
    "    },\n",
    "    'XGB_Poly': {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'learning_rate': [0.01, 0.05, 0.1],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'subsample': [0.8, 0.9, 1.0],\n",
    "        'colsample_bytree': [0.8, 0.9, 1.0]\n",
    "    },\n",
    "    'LightGBM': {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'learning_rate': [0.01, 0.05, 0.1],\n",
    "        'num_leaves': [31, 50, 70],\n",
    "        'subsample': [0.8, 0.9, 1.0],\n",
    "        'colsample_bytree': [0.8, 0.9, 1.0],\n",
    "        'reg_alpha': [0, 0.1, 0.5],\n",
    "        'reg_lambda': [0, 0.1, 0.5]\n",
    "    },\n",
    "    'LGBM_Poly': {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'learning_rate': [0.01, 0.05, 0.1],\n",
    "        'num_leaves': [31, 50, 70],\n",
    "        'subsample': [0.8, 0.9, 1.0],\n",
    "        'colsample_bytree': [0.8, 0.9, 1.0]\n",
    "    },\n",
    "    'CatBoost': {\n",
    "        'iterations': [100, 200, 300],\n",
    "        'learning_rate': [0.01, 0.05, 0.1],\n",
    "        'depth': [6, 8, 10],\n",
    "        'l2_leaf_reg': [1, 3, 5, 7],\n",
    "        'border_count': [32, 64, 128]\n",
    "    },\n",
    "    'CatBoost_Poly': {\n",
    "        'iterations': [100, 200, 300],\n",
    "        'learning_rate': [0.01, 0.05, 0.1],\n",
    "        'depth': [6, 8, 10],\n",
    "        'l2_leaf_reg': [1, 3, 5, 7]\n",
    "    }\n",
    "}\n",
    "\n",
    "tuned_models = {}\n",
    "\n",
    "for name in top5:\n",
    "    print(f\"\\nOptimizando {name}...\")\n",
    "    base = models[name]['model']\n",
    "    \n",
    "    # Determinar si usamos datos polinómicos\n",
    "    use_poly = name.endswith('_Poly')\n",
    "    X_tune = X_train_poly if use_poly else X_train\n",
    "    X_tune_val = X_val_poly if use_poly else X_val\n",
    "    \n",
    "    # Usar RandomizedSearchCV en lugar de GridSearchCV para búsqueda más eficiente\n",
    "    grid = RandomizedSearchCV(\n",
    "        base.__class__(**{k: v for k, v in base.get_params().items() if k in ['random_state', 'n_jobs', 'verbose']}),\n",
    "        param_grids.get(name, {}),\n",
    "        n_iter=25,  # Número de combinaciones a probar\n",
    "        cv=5,\n",
    "        scoring='neg_mean_absolute_error',\n",
    "        verbose=1,\n",
    "        n_jobs=-1,\n",
    "        random_state=RANDOM_SEED\n",
    "    )\n",
    "    \n",
    "    grid.fit(X_tune, y_train)\n",
    "    best = grid.best_estimator_\n",
    "    \n",
    "    ev = evaluate_model(best, X_tune_val, y_val, use_log)\n",
    "    cv = cross_validate_model(best, X, y, cv=5, use_log=use_log)\n",
    "    \n",
    "    tuned_models[name + '_tuned'] = {**ev, **cv, 'model': best, 'best_params': grid.best_params_}\n",
    "    \n",
    "    print(f\"{name} mejores parámetros: {grid.best_params_}\")\n",
    "    print(f\"{name} MAE: {ev['MAE']:.4f}, RMSE: {ev['RMSE']:.4f}, R2: {ev['R2']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# =============================================================================\n",
    "# 7. ENSAMBLADO DE MODELOS\n",
    "# =============================================================================\n",
    "print(\"\\nCreando modelos de ensamblado...\")\n",
    "\n",
    "# Obtener los 3 mejores modelos tunados para el ensamblado\n",
    "tuned_results = pd.DataFrame({m: {\n",
    "    'MAE': tuned_models[m]['MAE'],\n",
    "    'RMSE': tuned_models[m]['RMSE'],\n",
    "    'R2': tuned_models[m]['R2'],\n",
    "    'MAE_CV': tuned_models[m]['MAE_CV'],\n",
    "    'R2_CV': tuned_models[m]['R2_CV']\n",
    "} for m in tuned_models}).T.sort_values('MAE')\n",
    "\n",
    "print(\"\\nResultados de modelos tunados:\")\n",
    "print(tuned_results)\n",
    "\n",
    "# Seleccionar los 3 mejores modelos\n",
    "top3_models = [tuned_models[name]['model'] for name in tuned_results.index[:3]]\n",
    "top3_names = tuned_results.index[:3].tolist()\n",
    "\n",
    "# Crear un modelo de votación (promedio de predicciones)\n",
    "voting_regressor = VotingRegressor(\n",
    "    estimators=[(name.replace('_tuned', ''), model) for name, model in zip(top3_names, top3_models)],\n",
    "    weights=[0.4, 0.3, 0.3]  # Dar más peso al mejor modelo\n",
    ")\n",
    "\n",
    "# Determinar si alguno de los modelos top usa características polinómicas\n",
    "use_poly_ensemble = any(name.startswith(('XGB_Poly', 'LGBM_Poly', 'CatBoost_Poly')) for name in top3_names)\n",
    "X_ensemble = X_train_poly if use_poly_ensemble else X_train\n",
    "X_val_ensemble = X_val_poly if use_poly_ensemble else X_val\n",
    "\n",
    "print(\"\\nEntrenando modelo de ensamblado (Voting)...\")\n",
    "voting_regressor.fit(X_ensemble, y_train)\n",
    "ev_voting = evaluate_model(voting_regressor, X_val_ensemble, y_val, use_log)\n",
    "cv_voting = cross_validate_model(voting_regressor, X, y, cv=5, use_log=use_log)\n",
    "\n",
    "models['VotingRegressor'] = {**ev_voting, **cv_voting, 'model': voting_regressor}\n",
    "print(f\"VotingRegressor: MAE={ev_voting['MAE']:.4f}, RMSE={ev_voting['RMSE']:.4f}, R2={ev_voting['R2']:.4f}\")\n",
    "\n",
    "# Crear un modelo de Stacking\n",
    "base_models = [(name.replace('_tuned', ''), model) for name, model in zip(top3_names, top3_models)]\n",
    "stacking_regressor = StackingRegressor(\n",
    "    estimators=base_models,\n",
    "    final_estimator=Ridge(alpha=1.0, random_state=RANDOM_SEED)\n",
    ")\n",
    "\n",
    "print(\"\\nEntrenando modelo de ensamblado (Stacking)...\")\n",
    "stacking_regressor.fit(X_ensemble, y_train)\n",
    "ev_stacking = evaluate_model(stacking_regressor, X_val_ensemble, y_val, use_log)\n",
    "cv_stacking = cross_validate_model(stacking_regressor, X, y, cv=5, use_log=use_log)\n",
    "\n",
    "models['StackingRegressor'] = {**ev_stacking, **cv_stacking, 'model': stacking_regressor}\n",
    "print(f\"StackingRegressor: MAE={ev_stacking['MAE']:.4f}, RMSE={ev_stacking['RMSE']:.4f}, R2={ev_stacking['R2']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%\n",
    "# =============================================================================\n",
    "# 8. RESUMEN FINAL Y SELECCIÓN DE MEJOR MODELO\n",
    "# =============================================================================\n",
    "# Combinar todos los resultados\n",
    "all_results = {}\n",
    "for name, model_info in models.items():\n",
    "    if name in ['VotingRegressor', 'StackingRegressor'] or name in top5:\n",
    "        all_results[name] = {\n",
    "            'MAE': model_info['MAE'],\n",
    "            'RMSE': model_info['RMSE'],\n",
    "            'R2': model_info['R2'],\n",
    "            'MAE_CV': model_info['MAE_CV'],\n",
    "            'R2_CV': model_info['R2_CV']\n",
    "        }\n",
    "\n",
    "for name, model_info in tuned_models.items():\n",
    "    all_results[name] = {\n",
    "        'MAE': model_info['MAE'],\n",
    "        'RMSE': model_info['RMSE'],\n",
    "        'R2': model_info['R2'],\n",
    "        'MAE_CV': model_info['MAE_CV'],\n",
    "        'R2_CV': model_info['R2_CV']\n",
    "    }\n",
    "\n",
    "final_results = pd.DataFrame(all_results).T.sort_values('MAE')\n",
    "\n",
    "print(\"\\nResumen final de todos los modelos:\")\n",
    "print(final_results)\n",
    "\n",
    "# Guardar resultados\n",
    "final_results.to_csv('model_results.csv')\n",
    "\n",
    "best_name = final_results.index[0]\n",
    "if best_name in tuned_models:\n",
    "    best_model = tuned_models[best_name]['model']\n",
    "else:\n",
    "    best_model = models[best_name]['model']\n",
    "\n",
    "print(f\"\\nMejor modelo final: {best_name} con MAE {final_results.loc[best_name, 'MAE']:.4f}\")\n",
    "\n",
    "# Guardar el mejor modelo\n",
    "joblib.dump(best_model, f\"best_model_{best_name}.pkl\")\n",
    "print(f\"Mejor modelo guardado como best_model_{best_name}.pkl\")\n",
    "\n",
    "# %%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To add a new cell, type '# %%'\n",
    "# To add a new markdown cell, type '# %% [markdown]'\n",
    "# %%\n",
    "# Modelado Avanzado para Competición Kaggle - Precios de Viviendas en Galicia\n",
    "# =============================================================================\n",
    "# Importación de bibliotecas necesarias\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, KFold, RandomizedSearchCV, cross_val_score\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet, HuberRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor, VotingRegressor, StackingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.feature_selection import SelectFromModel, RFECV\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "import catboost as cb\n",
    "import os\n",
    "import joblib\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configuración para reproducibilidad\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# Configuración de visualización\n",
    "sns.set_palette(\"viridis\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# %%\n",
    "# =============================================================================\n",
    "# 1. CARGA DE DATOS PROCESADOS\n",
    "# =============================================================================\n",
    "print(\"Cargando datos procesados...\")\n",
    "if not os.path.exists('train_processed.csv') or not os.path.exists('test_processed.csv'):\n",
    "    raise FileNotFoundError(\"No se encontraron los archivos procesados. Ejecute primero el script de preprocesamiento.\")\n",
    "\n",
    "train_data = pd.read_csv('train_processed.csv')\n",
    "test_data = pd.read_csv('test_processed.csv')\n",
    "\n",
    "print(f\"Dimensiones del conjunto de entrenamiento: {train_data.shape}\")\n",
    "print(f\"Dimensiones del conjunto de prueba: {test_data.shape}\")\n",
    "\n",
    "# %%\n",
    "# =============================================================================\n",
    "# 2. PREPARACIÓN PARA EL MODELADO\n",
    "# =============================================================================\n",
    "print(\"\\nPreparando datos para el modelado...\")\n",
    "\n",
    "# Verificar si tenemos transformación logarítmica de los precios\n",
    "if 'log_prezo' in train_data.columns:\n",
    "    print(\"Usando transformación logarítmica de precios para el modelado...\")\n",
    "    y = train_data['log_prezo']\n",
    "    use_log = True\n",
    "else:\n",
    "    print(\"Usando precios originales para el modelado...\")\n",
    "    y = train_data['prezo_euros']\n",
    "    use_log = False\n",
    "\n",
    "# Excluir ID, target, indicador outlier y variables categóricas originales\n",
    "exclude = ['id', 'prezo_euros', 'log_prezo', 'is_outlier',\n",
    "           'tipo_edificacion', 'calidade_materiais', 'cor_favorita_propietario',\n",
    "           'acceso_transporte_publico', 'orientacion', 'eficiencia_enerxetica']\n",
    "\n",
    "feature_cols = [c for c in train_data.columns if c not in exclude]\n",
    "X = train_data[feature_cols]\n",
    "\n",
    "# Imputar faltantes restantes\n",
    "if X.isnull().sum().sum() > 0:\n",
    "    print(\"ADVERTENCIA: Hay valores faltantes en las características. Imputando con la mediana...\")\n",
    "    X = X.fillna(X.median())\n",
    "\n",
    "# Dividir en train/validation con un split específico para evaluación final\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=RANDOM_SEED)\n",
    "print(f\"Tamaño del train: {X_train.shape}, validación: {X_val.shape}\")\n",
    "\n",
    "# %%\n",
    "# =============================================================================\n",
    "# 3. ANÁLISIS DE CARACTERÍSTICAS\n",
    "# =============================================================================\n",
    "print(\"\\nAnalizando importancia de características...\")\n",
    "\n",
    "# Utilizamos un RandomForest para estimar importancia inicial de características\n",
    "feature_analyzer = RandomForestRegressor(n_estimators=100, random_state=RANDOM_SEED, n_jobs=-1)\n",
    "feature_analyzer.fit(X_train, y_train)\n",
    "\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'importance': feature_analyzer.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(\"\\nTop 15 características más importantes:\")\n",
    "print(feature_importance.head(15))\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.barplot(x='importance', y='feature', data=feature_importance.head(20))\n",
    "plt.title('Importancia de Características (Random Forest)')\n",
    "plt.tight_layout()\n",
    "plt.savefig('feature_importance.png')\n",
    "plt.close()\n",
    "\n",
    "# Obtener las 20 características más importantes para posible filtrado\n",
    "top_features = feature_importance.head(20)['feature'].tolist()\n",
    "\n",
    "# Añadir ingeniería de características polinómicas para las top 5 features\n",
    "print(\"\\nCreando características polinómicas para las top 5 características...\")\n",
    "poly_features = PolynomialFeatures(degree=2, include_bias=False, interaction_only=False)\n",
    "top5_features = feature_importance.head(5)['feature'].tolist()\n",
    "\n",
    "X_train_poly = X_train.copy()\n",
    "X_val_poly = X_val.copy()\n",
    "\n",
    "poly_features_df = pd.DataFrame(\n",
    "    poly_features.fit_transform(X_train[top5_features]),\n",
    "    columns=poly_features.get_feature_names_out(top5_features)\n",
    ")\n",
    "\n",
    "poly_features_df_val = pd.DataFrame(\n",
    "    poly_features.transform(X_val[top5_features]),\n",
    "    columns=poly_features.get_feature_names_out(top5_features)\n",
    ")\n",
    "\n",
    "# Eliminar las columnas originales para evitar duplicados\n",
    "poly_features_df = poly_features_df.drop(columns=top5_features)\n",
    "poly_features_df_val = poly_features_df_val.drop(columns=top5_features)\n",
    "\n",
    "# Unir con el conjunto original\n",
    "X_train_poly = pd.concat([X_train, poly_features_df], axis=1)\n",
    "X_val_poly = pd.concat([X_val, poly_features_df_val], axis=1)\n",
    "\n",
    "print(f\"Tamaño del conjunto de datos con características polinómicas: {X_train_poly.shape}\")\n",
    "\n",
    "# %%\n",
    "# =============================================================================\n",
    "# 4. FUNCIONES AUXILIARES PARA EVALUACIÓN\n",
    "# =============================================================================\n",
    "def evaluate_model(model, X_val, y_val, use_log=False):\n",
    "    \"\"\"Evalúa un modelo en conjunto de validación y devuelve métricas\"\"\"\n",
    "    y_pred = model.predict(X_val)\n",
    "    \n",
    "    if use_log:\n",
    "        y_pred = np.expm1(y_pred)\n",
    "        y_true = np.expm1(y_val)\n",
    "    else:\n",
    "        y_true = y_val\n",
    "        \n",
    "    return {\n",
    "        'MAE': mean_absolute_error(y_true, y_pred),\n",
    "        'RMSE': np.sqrt(mean_squared_error(y_true, y_pred)),\n",
    "        'R2': r2_score(y_true, y_pred)\n",
    "    }\n",
    "\n",
    "def cross_validate_model(model, X, y, cv=5, use_log=False):\n",
    "    \"\"\"Realiza validación cruzada y devuelve métricas promedio\"\"\"\n",
    "    kf = KFold(n_splits=cv, shuffle=True, random_state=RANDOM_SEED)\n",
    "    maes, rmses, r2s = [], [], []\n",
    "    \n",
    "    for train_idx, val_idx in kf.split(X):\n",
    "        X_tr, X_va = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_tr, y_va = y.iloc[train_idx], y.iloc[val_idx]\n",
    "        \n",
    "        model.fit(X_tr, y_tr)\n",
    "        y_pred = model.predict(X_va)\n",
    "        \n",
    "        if use_log:\n",
    "            y_pred = np.expm1(y_pred)\n",
    "            y_va_true = np.expm1(y_va)\n",
    "        else:\n",
    "            y_va_true = y_va\n",
    "            \n",
    "        maes.append(mean_absolute_error(y_va_true, y_pred))\n",
    "        rmses.append(np.sqrt(mean_squared_error(y_va_true, y_pred)))\n",
    "        r2s.append(r2_score(y_va_true, y_pred))\n",
    "        \n",
    "    return {\n",
    "        'MAE_CV': np.mean(maes),\n",
    "        'MAE_STD': np.std(maes),\n",
    "        'RMSE_CV': np.mean(rmses),\n",
    "        'RMSE_STD': np.std(rmses),\n",
    "        'R2_CV': np.mean(r2s),\n",
    "        'R2_STD': np.std(r2s)\n",
    "    }\n",
    "\n",
    "def make_submission(model, test_data, feature_cols, filename='submission.csv', use_log=False, \n",
    "                    poly_features=None, top5_features=None):\n",
    "    \"\"\"Genera archivo de submisión para Kaggle\"\"\"\n",
    "    X_test = test_data[feature_cols]\n",
    "    \n",
    "    # Imputar valores faltantes si existen\n",
    "    if X_test.isnull().sum().sum() > 0:\n",
    "        X_test = X_test.fillna(X_test.median())\n",
    "    \n",
    "    # Aplicar transformación polinómica si corresponde\n",
    "    if poly_features is not None and top5_features is not None:\n",
    "        poly_features_test = pd.DataFrame(\n",
    "            poly_features.transform(X_test[top5_features]),\n",
    "            columns=poly_features.get_feature_names_out(top5_features)\n",
    "        )\n",
    "        # Eliminar las columnas originales\n",
    "        poly_features_test = poly_features_test.drop(columns=top5_features)\n",
    "        # Unir con el conjunto original\n",
    "        X_test = pd.concat([X_test, poly_features_test], axis=1)\n",
    "    \n",
    "    # Predicción\n",
    "    preds = model.predict(X_test)\n",
    "    \n",
    "    # Transformar de vuelta si se usó log\n",
    "    if use_log:\n",
    "        preds = np.expm1(preds)\n",
    "        \n",
    "    # Crear y guardar submisión\n",
    "    submission = pd.DataFrame({'id': test_data['id'], 'prezo_euros': preds})\n",
    "    submission.to_csv(filename, index=False)\n",
    "    print(f\"Submission guardado como {filename}\")\n",
    "    \n",
    "    # Visualizar distribución de predicciones\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.histplot(preds, bins=50, kde=True)\n",
    "    plt.title('Distribución de Predicciones')\n",
    "    plt.xlabel('Precio (euros)')\n",
    "    plt.ylabel('Frecuencia')\n",
    "    plt.savefig('predicciones_distribucion.png')\n",
    "    plt.close()\n",
    "    \n",
    "    return submission\n",
    "\n",
    "# %%\n",
    "# =============================================================================\n",
    "# 5. ENTRENAMIENTO DE MODELOS BÁSICOS MEJORADOS\n",
    "# =============================================================================\n",
    "print(\"\\nEntrenando modelos básicos...\")\n",
    "\n",
    "models = {}\n",
    "\n",
    "def train_and_record(name, model, X_tr=X_train, X_va=X_val):\n",
    "    \"\"\"Entrena, evalúa y registra resultados de un modelo\"\"\"\n",
    "    t0 = time.time()\n",
    "    model.fit(X_tr, y_train)\n",
    "    t = time.time() - t0\n",
    "    \n",
    "    ev = evaluate_model(model, X_va, y_val, use_log)\n",
    "    cv = cross_validate_model(model, X, y, cv=5, use_log=use_log)\n",
    "    \n",
    "    models[name] = {**ev, **cv, 'train_time': t, 'model': model}\n",
    "    \n",
    "    print(f\"{name}: MAE={ev['MAE']:.2f}, RMSE={ev['RMSE']:.2f}, R2={ev['R2']:.4f}, time={t:.1f}s\")\n",
    "\n",
    "# Lista de algoritmos básicos a probar\n",
    "algorithms = [\n",
    "    ('LinearRegression', LinearRegression()),\n",
    "    ('Ridge', Ridge(alpha=1.0, random_state=RANDOM_SEED)),\n",
    "    ('Lasso', Lasso(alpha=0.001, max_iter=10000, random_state=RANDOM_SEED)),\n",
    "    ('ElasticNet', ElasticNet(alpha=0.001, l1_ratio=0.5, max_iter=10000, random_state=RANDOM_SEED)),\n",
    "    ('HuberRegressor', HuberRegressor(epsilon=1.35, max_iter=1000)),\n",
    "    ('RandomForest', RandomForestRegressor(n_estimators=200, n_jobs=-1, random_state=RANDOM_SEED)),\n",
    "    ('ExtraTrees', ExtraTreesRegressor(n_estimators=200, n_jobs=-1, random_state=RANDOM_SEED)),\n",
    "    ('GradientBoosting', GradientBoostingRegressor(n_estimators=200, random_state=RANDOM_SEED)),\n",
    "    ('XGBoost', xgb.XGBRegressor(n_estimators=200, learning_rate=0.05, n_jobs=-1, random_state=RANDOM_SEED)),\n",
    "    ('LightGBM', lgb.LGBMRegressor(n_estimators=200, learning_rate=0.05, n_jobs=-1, random_state=RANDOM_SEED)),\n",
    "    ('CatBoost', cb.CatBoostRegressor(iterations=200, learning_rate=0.05, verbose=0, random_state=RANDOM_SEED))\n",
    "]\n",
    "\n",
    "# Entrenar modelos con datos originales\n",
    "print(\"\\nEntrenando con características originales:\")\n",
    "for n, m in algorithms: \n",
    "    train_and_record(n, m)\n",
    "\n",
    "# Entrenar modelos con datos polinómicos para los mejores algoritmos\n",
    "print(\"\\nEntrenando modelos con características polinómicas:\")\n",
    "boosting_algorithms = [\n",
    "    ('XGB_Poly', xgb.XGBRegressor(n_estimators=200, learning_rate=0.05, n_jobs=-1, random_state=RANDOM_SEED)),\n",
    "    ('LGBM_Poly', lgb.LGBMRegressor(n_estimators=200, learning_rate=0.05, n_jobs=-1, random_state=RANDOM_SEED)),\n",
    "    ('CatBoost_Poly', cb.CatBoostRegressor(iterations=200, learning_rate=0.05, verbose=0, random_state=RANDOM_SEED))\n",
    "]\n",
    "\n",
    "for n, m in boosting_algorithms:\n",
    "    train_and_record(n, m, X_train_poly, X_val_poly)\n",
    "\n",
    "print(\"\\nResumen inicial:\")\n",
    "init_results = pd.DataFrame({m: {\n",
    "    'MAE': models[m]['MAE'],\n",
    "    'RMSE': models[m]['RMSE'],\n",
    "    'R2': models[m]['R2'],\n",
    "    'MAE_CV': models[m]['MAE_CV'],\n",
    "    'R2_CV': models[m]['R2_CV']\n",
    "} for m in models}).T.sort_values('MAE')\n",
    "\n",
    "print(init_results)\n",
    "\n",
    "# Visualizar comparación de modelos\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.barplot(x=init_results.index, y='MAE', data=init_results.reset_index().rename(columns={'index': 'Modelo'}))\n",
    "plt.title('Comparación de MAE por Modelo')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.savefig('model_comparison.png')\n",
    "plt.close()\n",
    "\n",
    "# %%\n",
    "# =============================================================================\n",
    "# 6. BÚSQUEDA EXHAUSTIVA DE HIPERPARÁMETROS PARA TOP MODELOS\n",
    "# =============================================================================\n",
    "# Seleccionar los 5 mejores modelos\n",
    "top5 = init_results.index[:5].tolist()\n",
    "print(f\"\\nSeleccionados para optimización: {top5}\")\n",
    "\n",
    "# Definir grids de búsqueda mucho más extensos\n",
    "param_grids = {\n",
    "    'Ridge': {\n",
    "        'alpha': [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]\n",
    "    },\n",
    "    'Lasso': {\n",
    "        'alpha': [0.0001, 0.0005, 0.001, 0.005, 0.01]\n",
    "    },\n",
    "    'ElasticNet': {\n",
    "        'alpha': [0.0001, 0.0005, 0.001, 0.005, 0.01],\n",
    "        'l1_ratio': [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "    },\n",
    "    'RandomForest': {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [None, 10, 20, 30],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4],\n",
    "        'max_features': ['sqrt', 'log2', None]\n",
    "    },\n",
    "    'ExtraTrees': {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [None, 10, 20, 30],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    },\n",
    "    'GradientBoosting': {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'learning_rate': [0.01, 0.05, 0.1],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'min_samples_split': [2, 5],\n",
    "        'subsample': [0.8, 0.9, 1.0]\n",
    "    },\n",
    "    'XGBoost': {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'learning_rate': [0.01, 0.05, 0.1],\n",
    "        'max_depth': [3, 5, 7, 9],\n",
    "        'subsample': [0.8, 0.9, 1.0],\n",
    "        'colsample_bytree': [0.8, 0.9, 1.0],\n",
    "        'gamma': [0, 0.1, 0.2]\n",
    "    },\n",
    "    'XGB_Poly': {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'learning_rate': [0.01, 0.05, 0.1],\n",
    "        'max_depth': [3, 5, 7],\n",
    "        'subsample': [0.8, 0.9, 1.0],\n",
    "        'colsample_bytree': [0.8, 0.9, 1.0]\n",
    "    },\n",
    "    'LightGBM': {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'learning_rate': [0.01, 0.05, 0.1],\n",
    "        'num_leaves': [31, 50, 70],\n",
    "        'subsample': [0.8, 0.9, 1.0],\n",
    "        'colsample_bytree': [0.8, 0.9, 1.0],\n",
    "        'reg_alpha': [0, 0.1, 0.5],\n",
    "        'reg_lambda': [0, 0.1, 0.5]\n",
    "    },\n",
    "    'LGBM_Poly': {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'learning_rate': [0.01, 0.05, 0.1],\n",
    "        'num_leaves': [31, 50, 70],\n",
    "        'subsample': [0.8, 0.9, 1.0],\n",
    "        'colsample_bytree': [0.8, 0.9, 1.0]\n",
    "    },\n",
    "    'CatBoost': {\n",
    "        'iterations': [100, 200, 300],\n",
    "        'learning_rate': [0.01, 0.05, 0.1],\n",
    "        'depth': [6, 8, 10],\n",
    "        'l2_leaf_reg': [1, 3, 5, 7],\n",
    "        'border_count': [32, 64, 128]\n",
    "    },\n",
    "    'CatBoost_Poly': {\n",
    "        'iterations': [100, 200, 300],\n",
    "        'learning_rate': [0.01, 0.05, 0.1],\n",
    "        'depth': [6, 8, 10],\n",
    "        'l2_leaf_reg': [1, 3, 5, 7]\n",
    "    }\n",
    "}\n",
    "\n",
    "tuned_models = {}\n",
    "\n",
    "for name in top5:\n",
    "    print(f\"\\nOptimizando {name}...\")\n",
    "    base = models[name]['model']\n",
    "    \n",
    "    # Determinar si usamos datos polinómicos\n",
    "    use_poly = name.endswith('_Poly')\n",
    "    X_tune = X_train_poly if use_poly else X_train\n",
    "    X_tune_val = X_val_poly if use_poly else X_val\n",
    "    \n",
    "    # Usar RandomizedSearchCV en lugar de GridSearchCV para búsqueda más eficiente\n",
    "    grid = RandomizedSearchCV(\n",
    "        base.__class__(**{k: v for k, v in base.get_params().items() if k in ['random_state', 'n_jobs', 'verbose']}),\n",
    "        param_grids.get(name, {}),\n",
    "        n_iter=25,  # Número de combinaciones a probar\n",
    "        cv=5,\n",
    "        scoring='neg_mean_absolute_error',\n",
    "        verbose=1,\n",
    "        n_jobs=-1,\n",
    "        random_state=RANDOM_SEED\n",
    "    )\n",
    "    \n",
    "    grid.fit(X_tune, y_train)\n",
    "    best = grid.best_estimator_\n",
    "    \n",
    "    ev = evaluate_model(best, X_tune_val, y_val, use_log)\n",
    "    cv = cross_validate_model(best, X, y, cv=5, use_log=use_log)\n",
    "    \n",
    "    tuned_models[name + '_tuned'] = {**ev, **cv, 'model': best, 'best_params': grid.best_params_}\n",
    "    \n",
    "    print(f\"{name} mejores parámetros: {grid.best_params_}\")\n",
    "    print(f\"{name} MAE: {ev['MAE']:.4f}, RMSE: {ev['RMSE']:.4f}, R2: {ev['R2']:.4f}\")\n",
    "\n",
    "# %%\n",
    "# =============================================================================\n",
    "# 7. ENSAMBLADO DE MODELOS\n",
    "# =============================================================================\n",
    "print(\"\\nCreando modelos de ensamblado...\")\n",
    "\n",
    "# Obtener los 3 mejores modelos tunados para el ensamblado\n",
    "tuned_results = pd.DataFrame({m: {\n",
    "    'MAE': tuned_models[m]['MAE'],\n",
    "    'RMSE': tuned_models[m]['RMSE'],\n",
    "    'R2': tuned_models[m]['R2'],\n",
    "    'MAE_CV': tuned_models[m]['MAE_CV'],\n",
    "    'R2_CV': tuned_models[m]['R2_CV']\n",
    "} for m in tuned_models}).T.sort_values('MAE')\n",
    "\n",
    "print(\"\\nResultados de modelos tunados:\")\n",
    "print(tuned_results)\n",
    "\n",
    "# Seleccionar los 3 mejores modelos\n",
    "top3_models = [tuned_models[name]['model'] for name in tuned_results.index[:3]]\n",
    "top3_names = tuned_results.index[:3].tolist()\n",
    "\n",
    "# Crear un modelo de votación (promedio de predicciones)\n",
    "voting_regressor = VotingRegressor(\n",
    "    estimators=[(name.replace('_tuned', ''), model) for name, model in zip(top3_names, top3_models)],\n",
    "    weights=[0.4, 0.3, 0.3]  # Dar más peso al mejor modelo\n",
    ")\n",
    "\n",
    "# Determinar si alguno de los modelos top usa características polinómicas\n",
    "use_poly_ensemble = any(name.startswith(('XGB_Poly', 'LGBM_Poly', 'CatBoost_Poly')) for name in top3_names)\n",
    "X_ensemble = X_train_poly if use_poly_ensemble else X_train\n",
    "X_val_ensemble = X_val_poly if use_poly_ensemble else X_val\n",
    "\n",
    "print(\"\\nEntrenando modelo de ensamblado (Voting)...\")\n",
    "voting_regressor.fit(X_ensemble, y_train)\n",
    "ev_voting = evaluate_model(voting_regressor, X_val_ensemble, y_val, use_log)\n",
    "cv_voting = cross_validate_model(voting_regressor, X, y, cv=5, use_log=use_log)\n",
    "\n",
    "models['VotingRegressor'] = {**ev_voting, **cv_voting, 'model': voting_regressor}\n",
    "print(f\"VotingRegressor: MAE={ev_voting['MAE']:.4f}, RMSE={ev_voting['RMSE']:.4f}, R2={ev_voting['R2']:.4f}\")\n",
    "\n",
    "# Crear un modelo de Stacking\n",
    "base_models = [(name.replace('_tuned', ''), model) for name, model in zip(top3_names, top3_models)]\n",
    "stacking_regressor = StackingRegressor(\n",
    "    estimators=base_models,\n",
    "    final_estimator=Ridge(alpha=1.0, random_state=RANDOM_SEED)\n",
    ")\n",
    "\n",
    "print(\"\\nEntrenando modelo de ensamblado (Stacking)...\")\n",
    "stacking_regressor.fit(X_ensemble, y_train)\n",
    "ev_stacking = evaluate_model(stacking_regressor, X_val_ensemble, y_val, use_log)\n",
    "cv_stacking = cross_validate_model(stacking_regressor, X, y, cv=5, use_log=use_log)\n",
    "\n",
    "models['StackingRegressor'] = {**ev_stacking, **cv_stacking, 'model': stacking_regressor}\n",
    "print(f\"StackingRegressor: MAE={ev_stacking['MAE']:.4f}, RMSE={ev_stacking['RMSE']:.4f}, R2={ev_stacking['R2']:.4f}\")\n",
    "\n",
    "# %%\n",
    "# =============================================================================\n",
    "# 8. RESUMEN FINAL Y SELECCIÓN DE MEJOR MODELO\n",
    "# =============================================================================\n",
    "# Combinar todos los resultados\n",
    "all_results = {}\n",
    "for name, model_info in models.items():\n",
    "    if name in ['VotingRegressor', 'StackingRegressor'] or name in top5:\n",
    "        all_results[name] = {\n",
    "            'MAE': model_info['MAE'],\n",
    "            'RMSE': model_info['RMSE'],\n",
    "            'R2': model_info['R2'],\n",
    "            'MAE_CV': model_info['MAE_CV'],\n",
    "            'R2_CV': model_info['R2_CV']\n",
    "        }\n",
    "\n",
    "for name, model_info in tuned_models.items():\n",
    "    all_results[name] = {\n",
    "        'MAE': model_info['MAE'],\n",
    "        'RMSE': model_info['RMSE'],\n",
    "        'R2': model_info['R2'],\n",
    "        'MAE_CV': model_info['MAE_CV'],\n",
    "        'R2_CV': model_info['R2_CV']\n",
    "    }\n",
    "\n",
    "final_results = pd.DataFrame(all_results).T.sort_values('MAE')\n",
    "\n",
    "print(\"\\nResumen final de todos los modelos:\")\n",
    "print(final_results)\n",
    "\n",
    "# Guardar resultados\n",
    "final_results.to_csv('model_results.csv')\n",
    "\n",
    "best_name = final_results.index[0]\n",
    "if best_name in tuned_models:\n",
    "    best_model = tuned_models[best_name]['model']\n",
    "else:\n",
    "    best_model = models[best_name]['model']\n",
    "\n",
    "print(f\"\\nMejor modelo final: {best_name} con MAE {final_results.loc[best_name, 'MAE']:.4f}\")\n",
    "\n",
    "# Guardar el mejor modelo\n",
    "joblib.dump(best_model, f\"best_model_{best_name}.pkl\")\n",
    "print(f\"Mejor modelo guardado como best_model_{best_name}.pkl\")\n",
    "\n",
    "# %%\n",
    "# =============================================================================\n",
    "# 9. GENERAR SUBMISSION FINAL\n",
    "# =============================================================================\n",
    "# Determinar si el mejor modelo usa características polinómicas\n",
    "use_poly_final = best_name.startswith(('XGB_Poly', 'LGBM_Poly', 'CatBoost_Poly', 'VotingRegressor', 'StackingRegressor')) and use_poly_ensemble\n",
    "\n",
    "# Generar predicciones finales\n",
    "submission = make_submission(\n",
    "    best_model, \n",
    "    test_data, \n",
    "    feature_cols, \n",
    "    filename=f'submission_{best_name}.csv', \n",
    "    use_log=use_log,\n",
    "    poly_features=poly_features if use_poly_final else None,\n",
    "    top5_features=top5_features if use_poly_final else None\n",
    ")\n",
    "\n",
    "print(\"\\nPreparación de submission completada. Aquí está una muestra:\")\n",
    "print(submission.head())\n",
    "\n",
    "# Visualizar características más importantes del mejor modelo\n",
    "if hasattr(best_model, 'feature_importances_') or (hasattr(best_model, 'estimators_') and hasattr(best_model.estimators_[0], 'feature_importances_')):\n",
    "    try:\n",
    "        if hasattr(best_model, 'feature_importances_'):\n",
    "            importances = best_model.feature_importances_\n",
    "            feature_names = X_ensemble.columns if use_poly_final else X.columns\n",
    "        else:\n",
    "            # Para ensambles, usar el primer estimador\n",
    "            importances = best_model.estimators_[0].feature_importances_\n",
    "            feature_names = X_ensemble.columns if use_poly_final else X.columns\n",
    "        \n",
    "        feature_imp = pd.DataFrame({\n",
    "            'feature': feature_names,\n",
    "            'importance': importances\n",
    "        }).sort_values('importance', ascending=False)\n",
    "        \n",
    "        plt.figure(figsize=(12, 8))\n",
    "        sns.barplot(x='importance', y='feature', data=feature_imp.head(20))\n",
    "        plt.title(f'Importancia de Características ({best_name})')\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'feature_importance_{best_name}.png')\n",
    "        plt.close()\n",
    "        \n",
    "        print(\"\\nCaracterísticas más importantes del mejor modelo guardadas en 'feature_importance_{best_name}.png'\")\n",
    "    except:\n",
    "        print(\"No se pudo generar el gráfico de importancia de características para este modelo.\")\n",
    "\n",
    "print(\"\\nAnalisis y modelado completados exitosamente!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0523ecfdfd03da9535a2cd394fa2b2a2368df119d71b1e2a5e4a2b8711053260"
  },
  "kernelspec": {
   "display_name": "Python 3.7.8 64-bit ('venvP4': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
