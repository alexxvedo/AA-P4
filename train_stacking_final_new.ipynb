{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "from xgboost import XGBRegressor\n",
    "import lightgbm as lgb\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create results directory\n",
    "os.makedirs('results_stacking', exist_ok=True)\n",
    "\n",
    "# Function to load data\n",
    "def load_data(train_path, test_path):\n",
    "    \"\"\"\n",
    "    Load training and test data\n",
    "    \"\"\"\n",
    "    train_df = pd.read_csv(train_path)\n",
    "    test_df = pd.read_csv(test_path)\n",
    "    \n",
    "    # For training data\n",
    "    X_train = train_df.drop(['prezo_euros', 'id'], axis=1, errors='ignore')\n",
    "    y_train = train_df['prezo_euros']\n",
    "    \n",
    "    # For test data\n",
    "    if 'Unnamed: 0' in test_df.columns:\n",
    "        X_test = test_df.drop(['id', 'Unnamed: 0'], axis=1, errors='ignore')\n",
    "    else:\n",
    "        X_test = test_df.drop(['id'], axis=1, errors='ignore')\n",
    "    test_ids = test_df['id']\n",
    "    \n",
    "    print(f\"Training data shape: {X_train.shape}\")\n",
    "    print(f\"Test data shape: {X_test.shape}\")\n",
    "    return X_train, y_train, X_test, test_ids\n",
    "\n",
    "# Function to identify categorical features\n",
    "def get_categorical_features(df):\n",
    "    \"\"\"\n",
    "    Identify categorical features in the dataset\n",
    "    \"\"\"\n",
    "    categorical_features = []\n",
    "    for col in df.columns:\n",
    "        if (df[col].dtype == 'object' or\n",
    "            col in ['tipo_edificacion', 'calidade_materiais',\n",
    "                   'cor_favorita_propietario', 'acceso_transporte_publico',\n",
    "                   'orientacion', 'eficiencia_enerxetica'] or\n",
    "            'tipo_' in col or 'color_' in col):\n",
    "            categorical_features.append(col)\n",
    "    print(f\"Categorical features: {categorical_features}\")\n",
    "    return categorical_features\n",
    "\n",
    "# Function to load pretrained models\n",
    "def load_models(model_paths):\n",
    "    \"\"\"\n",
    "    Load pretrained models from specified paths\n",
    "    \"\"\"\n",
    "    models = {}\n",
    "    \n",
    "    # Load CatBoost model\n",
    "    if 'catboost' in model_paths and os.path.exists(model_paths['catboost']):\n",
    "        print(\"Loading CatBoost model...\")\n",
    "        models['catboost'] = CatBoostRegressor()\n",
    "        models['catboost'].load_model(model_paths['catboost'])\n",
    "    \n",
    "    # Load XGBoost model\n",
    "    if 'xgboost' in model_paths and os.path.exists(model_paths['xgboost']):\n",
    "        print(\"Loading XGBoost model...\")\n",
    "        models['xgboost'] = XGBRegressor()\n",
    "        models['xgboost'].load_model(model_paths['xgboost'])\n",
    "    \n",
    "    # Load LightGBM model\n",
    "    if 'lightgbm' in model_paths and os.path.exists(model_paths['lightgbm']):\n",
    "        print(\"Loading LightGBM model...\")\n",
    "        models['lightgbm'] = lgb.Booster(model_file=model_paths['lightgbm'])\n",
    "    \n",
    "    # Load MLP model (which includes the scaler)\n",
    "    if 'mlp' in model_paths and os.path.exists(model_paths['mlp']):\n",
    "        print(\"Loading MLP model...\")\n",
    "        models['mlp'] = joblib.load(model_paths['mlp'])\n",
    "    \n",
    "    print(f\"Successfully loaded {len(models)} models\")\n",
    "    return models\n",
    "\n",
    "# Function to make predictions with loaded models\n",
    "def predict_with_models(models, X):\n",
    "    \"\"\"\n",
    "    Make predictions using all loaded models\n",
    "    \"\"\"\n",
    "    predictions = {}\n",
    "    \n",
    "    if 'catboost' in models:\n",
    "        print(\"Predicting with CatBoost...\")\n",
    "        predictions['catboost'] = models['catboost'].predict(X)\n",
    "    \n",
    "    if 'xgboost' in models:\n",
    "        print(\"Predicting with XGBoost...\")\n",
    "        predictions['xgboost'] = models['xgboost'].predict(X)\n",
    "    \n",
    "    if 'lightgbm' in models:\n",
    "        print(\"Predicting with LightGBM...\")\n",
    "        predictions['lightgbm'] = models['lightgbm'].predict(X)\n",
    "    \n",
    "    if 'mlp' in models:\n",
    "        print(\"Predicting with MLP...\")\n",
    "        # Extract model and scaler from the MLP model object\n",
    "        scaler = models['mlp']['scaler']\n",
    "        mlp_model = models['mlp']['model']\n",
    "        # Apply scaling before prediction\n",
    "        X_scaled = scaler.transform(X)\n",
    "        predictions['mlp'] = mlp_model.predict(X_scaled)\n",
    "    \n",
    "    return predictions\n",
    "\n",
    "# Function to generate level 1 features\n",
    "def generate_level1_features(models, X_train, X_test, y_train, categorical_features=None):\n",
    "    \"\"\"\n",
    "    Generate level 1 training features for meta-model using cross-validation\n",
    "    \"\"\"\n",
    "    print(\"Generating level 1 features for stacking...\")\n",
    "    \n",
    "    # For training set, use k-fold cross-validation to avoid data leakage\n",
    "    k = 5\n",
    "    train_preds_df = pd.DataFrame(index=range(X_train.shape[0]))\n",
    "    test_preds_all = {}\n",
    "    \n",
    "    # Create folds\n",
    "    n_samples = X_train.shape[0]\n",
    "    fold_size = n_samples // k\n",
    "    indices = np.arange(n_samples)\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    for fold in range(k):\n",
    "        print(f\"Processing fold {fold+1}/{k}\")\n",
    "        \n",
    "        # Get indices for this fold\n",
    "        start_idx = fold * fold_size\n",
    "        end_idx = (fold + 1) * fold_size if fold < k - 1 else n_samples\n",
    "        val_indices = indices[start_idx:end_idx]\n",
    "        train_indices = np.setdiff1d(indices, val_indices)\n",
    "        \n",
    "        # Split data for this fold\n",
    "        X_train_fold = X_train.iloc[train_indices]\n",
    "        y_train_fold = y_train.iloc[train_indices]\n",
    "        X_val_fold = X_train.iloc[val_indices]\n",
    "        \n",
    "        # Generate predictions for this fold\n",
    "        fold_preds = pd.DataFrame(index=val_indices)\n",
    "        test_fold_preds = {}\n",
    "        \n",
    "        # Train and predict with CatBoost\n",
    "        if 'catboost' in models:\n",
    "            print(\"Training fold with CatBoost...\")\n",
    "            if categorical_features:\n",
    "                train_pool = Pool(X_train_fold, y_train_fold, cat_features=categorical_features)\n",
    "                val_pool = Pool(X_val_fold, cat_features=categorical_features)\n",
    "                test_pool = Pool(X_test, cat_features=categorical_features)\n",
    "                \n",
    "                temp_model = CatBoostRegressor()\n",
    "                temp_model.set_params(**models['catboost'].get_params())\n",
    "                temp_model.fit(train_pool, verbose=False)\n",
    "                fold_preds['catboost'] = temp_model.predict(val_pool)\n",
    "                test_fold_preds['catboost'] = temp_model.predict(test_pool)\n",
    "            else:\n",
    "                temp_model = CatBoostRegressor()\n",
    "                temp_model.set_params(**models['catboost'].get_params())\n",
    "                temp_model.fit(X_train_fold, y_train_fold, verbose=False)\n",
    "                fold_preds['catboost'] = temp_model.predict(X_val_fold)\n",
    "                test_fold_preds['catboost'] = temp_model.predict(X_test)\n",
    "        \n",
    "        # Train and predict with XGBoost\n",
    "        if 'xgboost' in models:\n",
    "            print(\"Training fold with XGBoost...\")\n",
    "            temp_model = XGBRegressor()\n",
    "            temp_model.set_params(**models['xgboost'].get_params())\n",
    "            temp_model.fit(X_train_fold, y_train_fold, verbose=False)\n",
    "            fold_preds['xgboost'] = temp_model.predict(X_val_fold)\n",
    "            test_fold_preds['xgboost'] = temp_model.predict(X_test)\n",
    "        \n",
    "        # Train and predict with LightGBM\n",
    "        if 'lightgbm' in models:\n",
    "            print(\"Training fold with LightGBM...\")\n",
    "            train_lgb = lgb.Dataset(X_train_fold, label=y_train_fold)\n",
    "            params = models['lightgbm'].params if hasattr(models['lightgbm'], 'params') else {}\n",
    "            temp_model = lgb.train(params, train_lgb, num_boost_round=100)\n",
    "            fold_preds['lightgbm'] = temp_model.predict(X_val_fold)\n",
    "            test_fold_preds['lightgbm'] = temp_model.predict(X_test)\n",
    "        \n",
    "        # Train and predict with MLP\n",
    "        if 'mlp' in models:\n",
    "            print(\"Training fold with MLP...\")\n",
    "            # Extract parameters from loaded model\n",
    "            mlp_params = models['mlp']['model'].get_params()\n",
    "            \n",
    "            # Create and fit scaler\n",
    "            scaler = StandardScaler()\n",
    "            X_train_fold_scaled = scaler.fit_transform(X_train_fold)\n",
    "            X_val_fold_scaled = scaler.transform(X_val_fold)\n",
    "            X_test_scaled = scaler.transform(X_test)\n",
    "            \n",
    "            # Create and train MLP model\n",
    "            temp_model = MLPRegressor(**mlp_params)\n",
    "            temp_model.fit(X_train_fold_scaled, y_train_fold)\n",
    "            \n",
    "            fold_preds['mlp'] = temp_model.predict(X_val_fold_scaled)\n",
    "            test_fold_preds['mlp'] = temp_model.predict(X_test_scaled)\n",
    "        \n",
    "        # Add fold predictions to overall predictions DataFrame\n",
    "        for model_name in fold_preds.columns:\n",
    "            train_preds_df.loc[val_indices, model_name] = fold_preds[model_name].values\n",
    "            \n",
    "            if model_name not in test_preds_all:\n",
    "                test_preds_all[model_name] = []\n",
    "            test_preds_all[model_name].append(test_fold_preds[model_name])\n",
    "    \n",
    "    # Create test predictions by averaging fold predictions\n",
    "    test_preds_df = pd.DataFrame()\n",
    "    for model_name, preds_list in test_preds_all.items():\n",
    "        test_preds_df[model_name] = np.mean(preds_list, axis=0)\n",
    "    \n",
    "    print(f\"Generated level 1 features with shape: {train_preds_df.shape} (train), {test_preds_df.shape} (test)\")\n",
    "    return train_preds_df, test_preds_df\n",
    "\n",
    "# Function to train meta-model\n",
    "def train_meta_model(level1_train, y_train, level1_test):\n",
    "    \"\"\"\n",
    "    Train meta-model on level 1 features\n",
    "    \"\"\"\n",
    "    print(\"Training meta-model...\")\n",
    "    \n",
    "    # Configure and train meta-model\n",
    "    meta_model = GradientBoostingRegressor(\n",
    "        n_estimators=100,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=4,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Split level 1 data for validation\n",
    "    X_train, X_val, y_train_split, y_val = train_test_split(\n",
    "        level1_train, y_train, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Train meta-model\n",
    "    meta_model.fit(X_train, y_train_split)\n",
    "    \n",
    "    # Evaluate on validation set\n",
    "    val_preds = meta_model.predict(X_val)\n",
    "    val_mae = mean_absolute_error(y_val, val_preds)\n",
    "    val_rmse = np.sqrt(mean_squared_error(y_val, val_preds))\n",
    "    val_r2 = r2_score(y_val, val_preds)\n",
    "    \n",
    "    print(f\"Meta-model validation metrics:\")\n",
    "    print(f\"  MAE: {val_mae:.2f}\")\n",
    "    print(f\"  RMSE: {val_rmse:.2f}\")\n",
    "    print(f\"  RÂ²: {val_r2:.4f}\")\n",
    "    \n",
    "    # Retrain on full dataset\n",
    "    meta_model.fit(level1_train, y_train)\n",
    "    \n",
    "    # Generate predictions for test data\n",
    "    test_preds = meta_model.predict(level1_test)\n",
    "    \n",
    "    return meta_model, test_preds\n",
    "\n",
    "# Function to create submission file\n",
    "def create_submission(test_ids, predictions, output_file):\n",
    "    \"\"\"\n",
    "    Create submission file with predictions\n",
    "    \"\"\"\n",
    "    submission_df = pd.DataFrame({\n",
    "        'id': test_ids,\n",
    "        'prezo_euros': predictions\n",
    "    })\n",
    "    \n",
    "    submission_df.to_csv(output_file, index=False)\n",
    "    print(f\"Submission saved to {output_file}\")\n",
    "    return submission_df\n",
    "\n",
    "# Main execution\n",
    "if __name__ == \"__main__\":\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Load data\n",
    "    train_path = 'train_processed.csv'\n",
    "    test_path = 'test_processed.csv'\n",
    "    X_train, y_train, X_test, test_ids = load_data(train_path, test_path)\n",
    "    \n",
    "    # Get categorical features\n",
    "    categorical_features = get_categorical_features(X_train)\n",
    "    \n",
    "    # Define paths to pretrained models\n",
    "    model_paths = {\n",
    "        'catboost': 'results_initial_hyper/best_model_long.cbm',\n",
    "        'xgboost': 'models/xgboost_model.json',\n",
    "        'lightgbm': 'models/lightgbm_model.txt',\n",
    "        'mlp': 'models/mlp_model.pkl'\n",
    "    }\n",
    "    \n",
    "    # Load models\n",
    "    models = load_models(model_paths)\n",
    "    \n",
    "    # Generate level 1 features\n",
    "    level1_train, level1_test = generate_level1_features(\n",
    "        models, X_train, X_test, y_train, categorical_features\n",
    "    )\n",
    "    \n",
    "    # Train meta-model\n",
    "    meta_model, test_preds = train_meta_model(level1_train, y_train, level1_test)\n",
    "    \n",
    "    # Create submission file\n",
    "    create_submission(test_ids, test_preds, 'submissions_final_stacking.csv')\n",
    "    \n",
    "    # Save meta-model\n",
    "    joblib.dump(meta_model, 'results_stacking/meta_model.pkl')\n",
    "    print(\"Meta-model saved to results_stacking/meta_model.pkl\")\n",
    "    \n",
    "    # End timing\n",
    "    end_time = time.time()\n",
    "    print(f\"Total execution time: {end_time - start_time:.2f} seconds\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
