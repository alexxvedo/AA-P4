{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valores faltantes en el dataset: 0\n",
      "Características categóricas detectadas: []\n",
      "Forma del dataset: (20000, 47)\n",
      "Features incluidas: ['superficie_interior_m2', 'superficie_exterior_m2', 'numero_habitacions', 'numero_banos', 'ano_construccion', 'lonxitude', 'latitude', 'temperatura_media_mes_construccion', 'distancia_centro_km', 'distancia_escola_km', 'indice_criminalidade', 'numero_arboles_xardin', 'edad_vivienda', 'superficie_por_habitacion', 'superficie_total', 'ratio_interior_exterior', 'densidad_banos', 'densidad_habitaciones', 'dist_coruna', 'dist_vigo', 'dist_santiago', 'calidad_edad', 'banos_por_habitacion', 'orientacion_valor', 'eficiencia_valor', 'calidade_valor', 'transporte_valor', 'tipo_Apartamento', 'tipo_Casa', 'tipo_Chalet adosado', 'color_Amarelo', 'color_Azul', 'color_Branco', 'color_Negro', 'color_Verde', 'color_Vermello', 'tipo_Apartamento.1', 'tipo_Casa.1', 'tipo_Chalet adosado.1', 'color_Amarelo.1', 'color_Azul.1', 'color_Branco.1', 'color_Negro.1', 'color_Verde.1', 'color_Vermello.1']\n",
      "\n",
      "=== Entrenando modelo inicial para análisis ===\n",
      "0:\tlearn: 123570.0157799\ttest: 123451.7797439\tbest: 123451.7797439 (0)\ttotal: 50.6ms\tremaining: 8m 25s\n",
      "200:\tlearn: 31985.9232254\ttest: 31951.2941700\tbest: 31951.2941700 (200)\ttotal: 390ms\tremaining: 19s\n",
      "400:\tlearn: 30004.9258015\ttest: 30392.4165038\tbest: 30392.2038988 (399)\ttotal: 717ms\tremaining: 17.2s\n",
      "600:\tlearn: 29275.8416026\ttest: 29984.9706211\tbest: 29984.9706211 (600)\ttotal: 1.04s\tremaining: 16.4s\n",
      "800:\tlearn: 28805.9859193\ttest: 29757.0755102\tbest: 29755.1743329 (797)\ttotal: 1.37s\tremaining: 15.8s\n",
      "1000:\tlearn: 28472.3836569\ttest: 29671.6353440\tbest: 29671.6353440 (1000)\ttotal: 1.71s\tremaining: 15.3s\n",
      "1200:\tlearn: 28200.5381169\ttest: 29616.8895784\tbest: 29614.1513106 (1191)\ttotal: 2.04s\tremaining: 15s\n",
      "1400:\tlearn: 27945.7047077\ttest: 29514.0052286\tbest: 29514.0052286 (1400)\ttotal: 2.37s\tremaining: 14.6s\n",
      "1600:\tlearn: 27732.5416821\ttest: 29454.7089725\tbest: 29453.9222785 (1599)\ttotal: 2.71s\tremaining: 14.2s\n",
      "1800:\tlearn: 27558.6688145\ttest: 29458.8243073\tbest: 29452.1861798 (1643)\ttotal: 3.05s\tremaining: 13.9s\n",
      "2000:\tlearn: 27387.2155809\ttest: 29434.9516542\tbest: 29427.9366904 (1978)\ttotal: 3.38s\tremaining: 13.5s\n",
      "2200:\tlearn: 27238.6330510\ttest: 29421.2796148\tbest: 29412.2879974 (2072)\ttotal: 3.73s\tremaining: 13.2s\n",
      "2400:\tlearn: 27098.7816982\ttest: 29419.2106110\tbest: 29412.2879974 (2072)\ttotal: 4.05s\tremaining: 12.8s\n",
      "2600:\tlearn: 26964.4253689\ttest: 29408.8666618\tbest: 29403.0719305 (2466)\ttotal: 4.38s\tremaining: 12.5s\n",
      "2800:\tlearn: 26837.1588299\ttest: 29399.2190029\tbest: 29399.2190029 (2800)\ttotal: 4.71s\tremaining: 12.1s\n",
      "3000:\tlearn: 26706.2079770\ttest: 29379.1748753\tbest: 29371.7076334 (2900)\ttotal: 5.04s\tremaining: 11.8s\n",
      "3200:\tlearn: 26585.1582619\ttest: 29379.6012641\tbest: 29371.7071819 (3183)\ttotal: 5.37s\tremaining: 11.4s\n",
      "3400:\tlearn: 26468.7846421\ttest: 29358.1168787\tbest: 29355.1749783 (3377)\ttotal: 5.71s\tremaining: 11.1s\n",
      "3600:\tlearn: 26361.7391324\ttest: 29365.4219438\tbest: 29344.4599015 (3472)\ttotal: 6.06s\tremaining: 10.8s\n",
      "3800:\tlearn: 26255.3871706\ttest: 29357.1830121\tbest: 29344.4599015 (3472)\ttotal: 6.42s\tremaining: 10.5s\n",
      "4000:\tlearn: 26160.9729309\ttest: 29366.2114202\tbest: 29344.4599015 (3472)\ttotal: 6.75s\tremaining: 10.1s\n",
      "4200:\tlearn: 26069.1055171\ttest: 29361.4569083\tbest: 29344.4599015 (3472)\ttotal: 7.09s\tremaining: 9.78s\n",
      "4400:\tlearn: 25973.2894819\ttest: 29373.9812575\tbest: 29344.4599015 (3472)\ttotal: 7.46s\tremaining: 9.48s\n",
      "4600:\tlearn: 25897.0516309\ttest: 29382.6358013\tbest: 29344.4599015 (3472)\ttotal: 7.8s\tremaining: 9.15s\n",
      "4800:\tlearn: 25821.6157229\ttest: 29383.7599284\tbest: 29344.4599015 (3472)\ttotal: 8.15s\tremaining: 8.83s\n",
      "5000:\tlearn: 25759.0413911\ttest: 29386.1076901\tbest: 29344.4599015 (3472)\ttotal: 8.48s\tremaining: 8.48s\n",
      "5200:\tlearn: 25695.1660273\ttest: 29398.2490450\tbest: 29344.4599015 (3472)\ttotal: 8.82s\tremaining: 8.14s\n",
      "5400:\tlearn: 25631.2660439\ttest: 29395.4401373\tbest: 29344.4599015 (3472)\ttotal: 9.15s\tremaining: 7.79s\n",
      "5600:\tlearn: 25565.8087840\ttest: 29393.5208956\tbest: 29344.4599015 (3472)\ttotal: 9.49s\tremaining: 7.45s\n",
      "5800:\tlearn: 25499.6380895\ttest: 29387.7456432\tbest: 29344.4599015 (3472)\ttotal: 9.83s\tremaining: 7.12s\n",
      "6000:\tlearn: 25443.1292328\ttest: 29394.3042636\tbest: 29344.4599015 (3472)\ttotal: 10.2s\tremaining: 6.78s\n",
      "6200:\tlearn: 25386.9529497\ttest: 29406.1142800\tbest: 29344.4599015 (3472)\ttotal: 10.5s\tremaining: 6.45s\n",
      "6400:\tlearn: 25329.7950089\ttest: 29403.8691712\tbest: 29344.4599015 (3472)\ttotal: 10.9s\tremaining: 6.12s\n",
      "6600:\tlearn: 25272.5890075\ttest: 29413.5602020\tbest: 29344.4599015 (3472)\ttotal: 11.3s\tremaining: 5.8s\n",
      "6800:\tlearn: 25218.1361609\ttest: 29407.2736073\tbest: 29344.4599015 (3472)\ttotal: 11.6s\tremaining: 5.46s\n",
      "7000:\tlearn: 25177.4691420\ttest: 29409.8193924\tbest: 29344.4599015 (3472)\ttotal: 12s\tremaining: 5.14s\n",
      "7200:\tlearn: 25130.7645093\ttest: 29410.4596528\tbest: 29344.4599015 (3472)\ttotal: 12.4s\tremaining: 4.81s\n",
      "7400:\tlearn: 25083.6885838\ttest: 29405.6645093\tbest: 29344.4599015 (3472)\ttotal: 12.7s\tremaining: 4.46s\n",
      "7600:\tlearn: 25036.1064881\ttest: 29412.0964888\tbest: 29344.4599015 (3472)\ttotal: 13s\tremaining: 4.12s\n",
      "7800:\tlearn: 24994.0117781\ttest: 29407.3067602\tbest: 29344.4599015 (3472)\ttotal: 13.4s\tremaining: 3.77s\n",
      "8000:\tlearn: 24959.6238780\ttest: 29410.0108395\tbest: 29344.4599015 (3472)\ttotal: 13.7s\tremaining: 3.43s\n",
      "8200:\tlearn: 24918.8507018\ttest: 29411.8437292\tbest: 29344.4599015 (3472)\ttotal: 14s\tremaining: 3.08s\n",
      "8400:\tlearn: 24876.2564905\ttest: 29412.7896939\tbest: 29344.4599015 (3472)\ttotal: 14.4s\tremaining: 2.74s\n",
      "8600:\tlearn: 24834.6769169\ttest: 29418.7142600\tbest: 29344.4599015 (3472)\ttotal: 14.8s\tremaining: 2.4s\n",
      "8800:\tlearn: 24793.2660883\ttest: 29423.9727398\tbest: 29344.4599015 (3472)\ttotal: 15.1s\tremaining: 2.06s\n",
      "9000:\tlearn: 24752.3948810\ttest: 29431.2689779\tbest: 29344.4599015 (3472)\ttotal: 15.5s\tremaining: 1.72s\n",
      "9200:\tlearn: 24708.8950645\ttest: 29433.7757598\tbest: 29344.4599015 (3472)\ttotal: 15.8s\tremaining: 1.37s\n",
      "9400:\tlearn: 24672.5640587\ttest: 29435.0740631\tbest: 29344.4599015 (3472)\ttotal: 16.1s\tremaining: 1.03s\n",
      "9600:\tlearn: 24632.7472715\ttest: 29440.1168203\tbest: 29344.4599015 (3472)\ttotal: 16.5s\tremaining: 685ms\n",
      "9800:\tlearn: 24601.5683967\ttest: 29440.6027825\tbest: 29344.4599015 (3472)\ttotal: 16.8s\tremaining: 341ms\n",
      "9999:\tlearn: 24561.2739630\ttest: 29430.2539663\tbest: 29344.4599015 (3472)\ttotal: 17.1s\tremaining: 0us\n",
      "\n",
      "bestTest = 29344.4599\n",
      "bestIteration = 3472\n",
      "\n",
      "Shrink model to first 3473 iterations.\n",
      "Métricas del modelo inicial:\n",
      "  MAE: 29344.46\n",
      "  RMSE: 41309.35\n",
      "  R2: 0.9378\n",
      "\n",
      "Análisis de características completado - Ver 'results_initial_hyper/initial_feature_importance.png'\n",
      "\n",
      "Modelo guardado como 'results_initial_hyper/best_model.cbm'\n",
      "MAE en conjunto de prueba: 29344.46\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Configuración para visualización\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "sns.set_palette(\"viridis\")\n",
    "os.makedirs('results_initial_hyper', exist_ok=True)\n",
    "\n",
    "# Función para cargar y preparar los datos\n",
    "def load_data(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Verificar y gestionar valores faltantes\n",
    "    print(f\"Valores faltantes en el dataset: {df.isnull().sum().sum()}\")\n",
    "    \n",
    "    # Identificar columnas categóricas automáticamente (si es necesario)\n",
    "    categorical_features = []\n",
    "    for col in df.columns:\n",
    "        if df[col].dtype == 'object' or col in ['tipo_edificacion', 'calidade_materiais', \n",
    "                                               'cor_favorita_propietario', 'acceso_transporte_publico',\n",
    "                                               'orientacion', 'eficiencia_enerxetica']:\n",
    "            categorical_features.append(col)\n",
    "    \n",
    "    print(f\"Características categóricas detectadas: {categorical_features}\")\n",
    "    \n",
    "    # Separamos features y target\n",
    "    X = df.drop(['prezo_euros', 'id'], axis=1, errors='ignore')\n",
    "    y = df['prezo_euros']\n",
    "    \n",
    "    print(f\"Forma del dataset: {df.shape}\")\n",
    "    print(f\"Features incluidas: {X.columns.tolist()}\")\n",
    "    \n",
    "    return X, y, categorical_features\n",
    "\n",
    "# FASE 1: Entrenamiento base y análisis\n",
    "def train_initial_model(X, y, cat_features=None, random_state=42):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.1, random_state=random_state\n",
    "    )\n",
    "    \n",
    "    initial_params = {\n",
    "        'iterations': 10000,\n",
    "        'learning_rate': 0.05,\n",
    "        'depth': 3,\n",
    "        'loss_function': 'MAE',\n",
    "        'eval_metric': 'MAE',\n",
    "        'random_seed': random_state,\n",
    "        'verbose': 200\n",
    "    }\n",
    "    \n",
    "    print(\"\\n=== Entrenando modelo inicial para análisis ===\")\n",
    "    \n",
    "    # Crear pool de datos para CatBoost\n",
    "    train_pool = Pool(X_train, y_train, cat_features=cat_features)\n",
    "    test_pool = Pool(X_test, y_test, cat_features=cat_features)\n",
    "    \n",
    "    # Entrenar modelo\n",
    "    model = CatBoostRegressor(**initial_params)\n",
    "    model.fit(train_pool, eval_set=test_pool, use_best_model=True)\n",
    "    \n",
    "    # Evaluar modelo\n",
    "    y_pred = model.predict(X_test)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"Métricas del modelo inicial:\")\n",
    "    print(f\"  MAE: {mae:.2f}\")\n",
    "    print(f\"  RMSE: {rmse:.2f}\")\n",
    "    print(f\"  R2: {r2:.4f}\")\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Analizar importancia de características\n",
    "    feature_importance = model.get_feature_importance()\n",
    "    feature_names = X.columns\n",
    "    importance_df = pd.DataFrame({'Feature': feature_names, 'Importance': feature_importance})\n",
    "    importance_df = importance_df.sort_values('Importance', ascending=False)\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(x='Importance', y='Feature', data=importance_df.head(20))\n",
    "    plt.title('Top 20 características más importantes', fontsize=15)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('results_initial_hyper/initial_feature_importance.png')\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    print(\"\\nAnálisis de características completado - Ver 'results_initial_hyper/initial_feature_importance.png'\")\n",
    "    \n",
    "    # Devolver todo lo necesario para optimización\n",
    "    return model, X_train, X_test, y_train, y_test, cat_features, importance_df\n",
    "\n",
    "# FASE 2: Optimización con Grid Search (solo sobre los parámetros iniciales)\n",
    "def optimize_with_grid_search(X_train, y_train, X_test, y_test, cat_features=None, random_state=42):\n",
    "    print(\"\\n=== Optimización con Grid Search ===\")\n",
    "    \n",
    "    # Grid de parámetros a evaluar (solo los parámetros iniciales)\n",
    "    param_grid = {\n",
    "        'iterations': [5000],\n",
    "        'learning_rate': [0.1,0.08, 0.05, 0.03, 0.01],\n",
    "        'depth': [1,2,3,4,5,6,7],\n",
    "    }\n",
    "    \n",
    "    # Crear pool para entrenamiento\n",
    "    train_pool = Pool(X_train, y_train, cat_features=cat_features)\n",
    "    \n",
    "    # Configurar modelo base para GridSearch\n",
    "    base_model = CatBoostRegressor(\n",
    "        loss_function='MAE',\n",
    "        eval_metric='MAE',\n",
    "        random_seed=random_state,\n",
    "        verbose=100\n",
    "    )\n",
    "    \n",
    "    # Configurar GridSearchCV\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=base_model,\n",
    "        param_grid=param_grid,\n",
    "        cv=3,  # Usar validación cruzada con 3 folds\n",
    "        scoring='neg_mean_absolute_error',\n",
    "        n_jobs=-1,  # Usar todos los núcleos disponibles\n",
    "        verbose=1\n",
    "    )\n",
    "    \n",
    "    # Entrenar Grid Search\n",
    "    start_time = time.time()\n",
    "    grid_search.fit(X_train, y_train, cat_features=cat_features)\n",
    "    gs_time = time.time() - start_time\n",
    "    \n",
    "    # Mejor valor de MAE encontrado\n",
    "    print(f\"Tiempo de Grid Search: {gs_time:.2f} segundos\")\n",
    "    print(f\"Mejor MAE encontrado: {-grid_search.best_score_:.2f}\")\n",
    "    print(f\"Mejores parámetros: {grid_search.best_params_}\")\n",
    "    \n",
    "    # Evaluar mejor modelo en conjunto de prueba\n",
    "    best_gs_model = grid_search.best_estimator_\n",
    "    y_pred = best_gs_model.predict(X_test)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    \n",
    "    print(f\"MAE en conjunto de prueba: {mae:.2f}\")\n",
    "    \n",
    "    # Guardar modelo optimizado en formato CatBoost .cbm\n",
    "    best_gs_model.save_model('results_initial_hyper/best_model_long.cbm')\n",
    "    print(\"\\nModelo guardado como 'results_initial_hyper/best_model.cbm'\")\n",
    "    \n",
    "    # Guardar resultados\n",
    "    gs_results = pd.DataFrame(grid_search.cv_results_)\n",
    "    gs_results.to_csv('results_initial_hyper/grid_search_results.csv', index=False)\n",
    "    \n",
    "    return best_gs_model, grid_search.best_params_\n",
    "\n",
    "# Fase de ejecución del script\n",
    "if __name__ == \"__main__\":\n",
    "    # Cargar datos\n",
    "    X, y, cat_features = load_data('train_processed.csv')\n",
    "    \n",
    "    # Entrenar el modelo inicial\n",
    "    model, X_train, X_test, y_train, y_test, cat_features, importance_df = train_initial_model(X, y, cat_features)\n",
    "    \n",
    "    # Guardar modelo optimizado en formato CatBoost .cbm\n",
    "    model.save_model('results_initial_hyper/best_model_initial.cbm')\n",
    "    print(\"\\nModelo guardado como 'results_initial_hyper/best_model.cbm'\")\n",
    "\n",
    "    # Evaluar mejor modelo en conjunto de prueba\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    \n",
    "    print(f\"MAE en conjunto de prueba: {mae:.2f}\")\n",
    "    \n",
    "    # Guardar resultados\n",
    "    #gs_results = pd.DataFrame(model.cv_results_)\n",
    "    #gs_results.to_csv('results_initial_hyper/grid_search_results.csv', index=False)\n",
    "    \n",
    "    # Optimización de hiperparámetros con GridSearch\n",
    "    #best_model, best_params = optimize_with_grid_search(X_train, y_train, X_test, y_test, cat_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Generando predicciones para el conjunto de prueba ===\n",
      "Cargando datos de prueba...\n",
      "Características categóricas detectadas: []\n",
      "Forma del dataset de prueba: (10000, 45)\n",
      "Cargando el modelo entrenado...\n",
      "Generando predicciones...\n",
      "¡Completado! Archivo de submission generado en submission_catboost_initial_hyper_long.csv\n",
      "Primeras filas del archivo de submission:\n",
      "      id    prezo_euros\n",
      "0   2309  465389.731641\n",
      "1  22405  147166.867508\n",
      "2  23398  422953.382795\n",
      "3  25059  262508.070719\n",
      "4   2665  586974.908665\n",
      "5   8512   78392.320081\n",
      "6   5149  599309.750207\n",
      "7   7791  274498.132714\n",
      "8  11312  109116.843607\n",
      "9  19044  327702.003566\n",
      "\n",
      "Estadísticas de las predicciones:\n",
      "Mínimo: 43170.38\n",
      "Máximo: 829299.20\n",
      "Media: 222292.58\n",
      "Mediana: 151767.43\n",
      "Desviación estándar: 163590.92\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Verificar la existencia del modelo entrenado\n",
    "model_path = 'results_initial_hyper/best_model_initial.cbm'\n",
    "if not os.path.exists(model_path):\n",
    "    print(f\"¡Error! No se encuentra el modelo en {model_path}\")\n",
    "    print(\"Por favor, asegúrate de ejecutar primero el script de entrenamiento\")\n",
    "    exit(1)\n",
    "\n",
    "print(\"=== Generando predicciones para el conjunto de prueba ===\")\n",
    "\n",
    "# Cargar el dataset de prueba\n",
    "print(\"Cargando datos de prueba...\")\n",
    "test_df = pd.read_csv('test_processed.csv')\n",
    "\n",
    "# Extraer los IDs para el submission\n",
    "ids = test_df['id']\n",
    "\n",
    "# Verificar características categóricas (usando las mismas que en entrenamiento)\n",
    "categorical_features = []\n",
    "for col in test_df.columns:\n",
    "    if test_df[col].dtype == 'object' or col in ['tipo_edificacion', 'calidade_materiais', \n",
    "                                              'cor_favorita_propietario', 'acceso_transporte_publico',\n",
    "                                              'orientacion', 'eficiencia_enerxetica']:\n",
    "        categorical_features.append(col)\n",
    "\n",
    "print(f\"Características categóricas detectadas: {categorical_features}\")\n",
    "\n",
    "# Preparar los datos para la predicción (eliminar cualquier columna que no sea una feature)\n",
    "X_test = test_df.drop(['id', 'Unnamed: 0'], axis=1, errors='ignore')\n",
    "if 'prezo_euros' in X_test.columns:\n",
    "    X_test = X_test.drop(['prezo_euros'], axis=1, errors='ignore')\n",
    "\n",
    "print(f\"Forma del dataset de prueba: {X_test.shape}\")\n",
    "\n",
    "# Cargar el modelo entrenado\n",
    "print(\"Cargando el modelo entrenado...\")\n",
    "model = CatBoostRegressor()\n",
    "model.load_model(model_path)\n",
    "\n",
    "# Realizar predicciones\n",
    "print(\"Generando predicciones...\")\n",
    "predictions = model.predict(X_test)\n",
    "\n",
    "# Crear el dataframe de submission\n",
    "submission = pd.DataFrame({\n",
    "    'id': ids,\n",
    "    'prezo_euros': predictions\n",
    "})\n",
    "\n",
    "# Guardar el archivo de submission\n",
    "submission_path = 'submission_catboost_initial_hyper_initial_simple.csv'\n",
    "submission.to_csv(submission_path, index=False)\n",
    "\n",
    "print(f\"¡Completado! Archivo de submission generado en {submission_path}\")\n",
    "print(f\"Primeras filas del archivo de submission:\")\n",
    "print(submission.head(10))\n",
    "\n",
    "# Estadísticas de las predicciones\n",
    "print(\"\\nEstadísticas de las predicciones:\")\n",
    "print(f\"Mínimo: {predictions.min():.2f}\")\n",
    "print(f\"Máximo: {predictions.max():.2f}\")\n",
    "print(f\"Media: {predictions.mean():.2f}\")\n",
    "print(f\"Mediana: {np.median(predictions):.2f}\")\n",
    "print(f\"Desviación estándar: {predictions.std():.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0523ecfdfd03da9535a2cd394fa2b2a2368df119d71b1e2a5e4a2b8711053260"
  },
  "kernelspec": {
   "display_name": "Python 3.7.8 64-bit ('venvP4': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
