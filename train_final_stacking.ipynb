{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import joblib\n",
    "import time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, r2_score, mean_squared_error\n",
    "from sklearn.linear_model import Ridge\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "from xgboost import XGBRegressor\n",
    "import lightgbm as lgbm\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "# Create results directory\n",
    "os.makedirs('results_stacking', exist_ok=True)\n",
    "\n",
    "# Function to load data\n",
    "def load_data(train_path, test_path):\n",
    "    \"\"\"\n",
    "    Load training and test data\n",
    "    \"\"\"\n",
    "    train_df = pd.read_csv(train_path)\n",
    "    test_df = pd.read_csv(test_path)\n",
    "    \n",
    "    # For training data\n",
    "    X_train = train_df.drop(['prezo_euros', 'id'], axis=1, errors='ignore')\n",
    "    y_train = train_df['prezo_euros']\n",
    "    \n",
    "    # For test data\n",
    "    X_test = test_df.drop(['id', 'Unnamed: 0'], axis=1, errors='ignore')\n",
    "    test_ids = test_df['id']\n",
    "    \n",
    "    print(f\"Training data shape: {X_train.shape}\")\n",
    "    print(f\"Test data shape: {X_test.shape}\")\n",
    "    \n",
    "    return X_train, y_train, X_test, test_ids\n",
    "\n",
    "# Function to load pretrained models\n",
    "def load_models(model_paths):\n",
    "    \"\"\"\n",
    "    Load pretrained models from specified paths\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    model_paths : dict\n",
    "        Dictionary with model names as keys and file paths as values\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Dictionary of loaded models\n",
    "    \"\"\"\n",
    "    models = {}\n",
    "    \n",
    "    # Load CatBoost model\n",
    "    if 'catboost' in model_paths:\n",
    "        print(\"Loading CatBoost model...\")\n",
    "        models['catboost'] = CatBoostRegressor()\n",
    "        models['catboost'].load_model(model_paths['catboost'])\n",
    "    \n",
    "    # Load XGBoost model\n",
    "    if 'xgboost' in model_paths:\n",
    "        print(\"Loading XGBoost model...\")\n",
    "        models['xgboost'] = XGBRegressor()\n",
    "        models['xgboost'].load_model(model_paths['xgboost'])\n",
    "    \n",
    "    # Load LightGBM model\n",
    "    if 'lightgbm' in model_paths:\n",
    "        print(\"Loading LightGBM model...\")\n",
    "        models['lightgbm'] = lgbm.Booster(model_file=model_paths['lightgbm'])\n",
    "        \n",
    "    # Load MLP model\n",
    "    if 'mlp' in model_paths:\n",
    "        print(\"Loading MLP model...\")\n",
    "        models['mlp'] = joblib.load(model_paths['mlp'])\n",
    "    \n",
    "    print(f\"Successfully loaded {len(models)} models\")\n",
    "    return models\n",
    "\n",
    "# Function to generate level 1 features\n",
    "def generate_level1_features(models, X_train, X_test, y_train, categorical_features=None):\n",
    "    \"\"\"\n",
    "    Generate level 1 training features for meta-model using cross-validation\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    models : dict\n",
    "        Dictionary of pretrained models\n",
    "    X_train : DataFrame\n",
    "        Training features\n",
    "    X_test : DataFrame\n",
    "        Test features\n",
    "    y_train : Series\n",
    "        Target variable\n",
    "    categorical_features : list, optional\n",
    "        List of categorical feature names\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    DataFrame, DataFrame\n",
    "        Level 1 features for training and test sets\n",
    "    \"\"\"\n",
    "    print(\"Generating level 1 features for stacking...\")\n",
    "    \n",
    "    # For training set, use k-fold cross-validation to avoid data leakage\n",
    "    k = 5\n",
    "    train_preds = pd.DataFrame()\n",
    "    test_preds_all = []\n",
    "    \n",
    "    # Create folds\n",
    "    n_samples = X_train.shape[0]\n",
    "    fold_size = n_samples // k\n",
    "    indices = np.arange(n_samples)\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    for fold in range(k):\n",
    "        print(f\"Processing fold {fold+1}/{k}\")\n",
    "        # Get indices for this fold\n",
    "        start_idx = fold * fold_size\n",
    "        end_idx = (fold + 1) * fold_size if fold < k - 1 else n_samples\n",
    "        val_indices = indices[start_idx:end_idx]\n",
    "        train_indices = np.setdiff1d(indices, val_indices)\n",
    "        \n",
    "        # Split data for this fold\n",
    "        X_train_fold = X_train.iloc[train_indices]\n",
    "        y_train_fold = y_train.iloc[train_indices]\n",
    "        X_val_fold = X_train.iloc[val_indices]\n",
    "        \n",
    "        # Generate predictions for this fold\n",
    "        fold_preds = pd.DataFrame(index=val_indices)\n",
    "        test_fold_preds = {}\n",
    "        \n",
    "        for name, model in models.items():\n",
    "            # Training set predictions\n",
    "            if name == 'catboost':\n",
    "                if categorical_features:\n",
    "                    train_pool = Pool(X_train_fold, y_train_fold, cat_features=categorical_features)\n",
    "                    val_pool = Pool(X_val_fold, cat_features=categorical_features)\n",
    "                    temp_model = CatBoostRegressor()\n",
    "                    temp_model.set_params(**model.get_params())\n",
    "                    temp_model.fit(train_pool, verbose=False)\n",
    "                    fold_preds[name] = temp_model.predict(val_pool)\n",
    "                    test_fold_preds[name] = temp_model.predict(X_test)\n",
    "                else:\n",
    "                    temp_model = CatBoostRegressor()\n",
    "                    temp_model.set_params(**model.get_params())\n",
    "                    temp_model.fit(X_train_fold, y_train_fold, verbose=False)\n",
    "                    fold_preds[name] = temp_model.predict(X_val_fold)\n",
    "                    test_fold_preds[name] = temp_model.predict(X_test)\n",
    "            \n",
    "            elif name == 'xgboost':\n",
    "                temp_model = XGBRegressor()\n",
    "                temp_model.set_params(**model.get_params())\n",
    "                temp_model.fit(X_train_fold, y_train_fold, verbose=False)\n",
    "                fold_preds[name] = temp_model.predict(X_val_fold)\n",
    "                test_fold_preds[name] = temp_model.predict(X_test)\n",
    "            \n",
    "            elif name == 'lightgbm':\n",
    "                # For LightGBM we need to handle differently as it's a Booster object\n",
    "                train_data = lgbm.Dataset(X_train_fold, y_train_fold)\n",
    "                params = {\n",
    "                    'objective': 'regression',\n",
    "                    'metric': 'mae',\n",
    "                    'verbosity': -1\n",
    "                }\n",
    "                temp_model = lgbm.train(params, train_data, num_boost_round=100)\n",
    "                fold_preds[name] = temp_model.predict(X_val_fold)\n",
    "                test_fold_preds[name] = temp_model.predict(X_test)\n",
    "            \n",
    "            elif name == 'mlp':\n",
    "                temp_model = MLPRegressor(random_state=42)\n",
    "                if hasattr(model, 'get_params'):\n",
    "                    temp_model.set_params(**model.get_params())\n",
    "                temp_model.fit(X_train_fold, y_train_fold)\n",
    "                fold_preds[name] = temp_model.predict(X_val_fold)\n",
    "                test_fold_preds[name] = temp_model.predict(X_test)\n",
    "        \n",
    "        # Save fold predictions\n",
    "        fold_preds = fold_preds.sort_index()  # Sort by original indices\n",
    "        train_preds = pd.concat([train_preds, fold_preds])\n",
    "        \n",
    "        # Accumulate test predictions\n",
    "        if not test_preds_all:\n",
    "            test_preds_all = {k: v for k, v in test_fold_preds.items()}\n",
    "        else:\n",
    "            for name in test_fold_preds:\n",
    "                test_preds_all[name] += test_fold_preds[name]\n",
    "    \n",
    "    # Average test predictions across folds\n",
    "    test_preds = pd.DataFrame()\n",
    "    for name in test_preds_all:\n",
    "        test_preds[name] = test_preds_all[name] / k\n",
    "    \n",
    "    # Final predictions for training and test sets\n",
    "    train_preds = train_preds.sort_index()  # Ensure correct order\n",
    "    \n",
    "    print(f\"Level 1 features shape - training: {train_preds.shape}, test: {test_preds.shape}\")\n",
    "    return train_preds, test_preds\n",
    "\n",
    "# Function to train meta-model\n",
    "def train_meta_model(level1_train, y_train, level1_test):\n",
    "    \"\"\"\n",
    "    Train meta-model for final predictions\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    level1_train : DataFrame\n",
    "        Level 1 features for training\n",
    "    y_train : Series\n",
    "        Target variable\n",
    "    level1_test : DataFrame\n",
    "        Level 1 features for test set\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    model, array\n",
    "        Trained meta-model and predictions for test set\n",
    "    \"\"\"\n",
    "    print(\"Training meta-model...\")\n",
    "    \n",
    "    # Split training data for meta-model validation\n",
    "    X_meta_train, X_meta_val, y_meta_train, y_meta_val = train_test_split(\n",
    "        level1_train, y_train, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Initialize and train meta-model (Ridge regression)\n",
    "    meta_model = Ridge(alpha=1.0)\n",
    "    meta_model.fit(X_meta_train, y_meta_train)\n",
    "    \n",
    "    # Validate meta-model\n",
    "    meta_val_preds = meta_model.predict(X_meta_val)\n",
    "    meta_mae = mean_absolute_error(y_meta_val, meta_val_preds)\n",
    "    meta_rmse = np.sqrt(mean_squared_error(y_meta_val, meta_val_preds))\n",
    "    meta_r2 = r2_score(y_meta_val, meta_val_preds)\n",
    "    \n",
    "    print(f\"Meta-model validation metrics:\")\n",
    "    print(f\"  MAE: {meta_mae:.2f}\")\n",
    "    print(f\"  RMSE: {meta_rmse:.2f}\")\n",
    "    print(f\"  R2: {meta_r2:.4f}\")\n",
    "    \n",
    "    # Generate predictions for test set\n",
    "    test_preds = meta_model.predict(level1_test)\n",
    "    \n",
    "    # Check feature importances in meta-model\n",
    "    feature_importance = meta_model.coef_\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': level1_train.columns,\n",
    "        'Importance': feature_importance\n",
    "    })\n",
    "    importance_df = importance_df.sort_values('Importance', ascending=False)\n",
    "    print(\"\\nMeta-model feature importances:\")\n",
    "    print(importance_df)\n",
    "    \n",
    "    return meta_model, test_preds\n",
    "\n",
    "# Function to create submission file\n",
    "def create_submission(test_ids, predictions, output_file):\n",
    "    \"\"\"\n",
    "    Create submission file\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    test_ids : Series\n",
    "        IDs for test set\n",
    "    predictions : array\n",
    "        Predicted values\n",
    "    output_file : str\n",
    "        Path to output file\n",
    "    \"\"\"\n",
    "    submission = pd.DataFrame({\n",
    "        'id': test_ids,\n",
    "        'prezo_euros': predictions\n",
    "    })\n",
    "    submission.to_csv(output_file, index=False)\n",
    "    print(f\"Submission file created: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (20000, 45)\n",
      "Test data shape: (10000, 45)\n",
      "Categorical features: ['tipo_Apartamento', 'tipo_Casa', 'tipo_Chalet adosado', 'color_Amarelo', 'color_Azul', 'color_Branco', 'color_Negro', 'color_Verde', 'color_Vermello', 'tipo_Apartamento.1', 'tipo_Casa.1', 'tipo_Chalet adosado.1', 'color_Amarelo.1', 'color_Azul.1', 'color_Branco.1', 'color_Negro.1', 'color_Verde.1', 'color_Vermello.1']\n",
      "Loading CatBoost model...\n",
      "Successfully loaded 1 models\n",
      "Generating level 1 features for stacking...\n",
      "Processing fold 1/5\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Load data\n",
    "train_path = 'train_processed.csv'\n",
    "test_path = 'test_processed.csv'\n",
    "X_train, y_train, X_test, test_ids = load_data(train_path, test_path)\n",
    "\n",
    "# Get categorical features\n",
    "categorical_features = []\n",
    "for col in X_train.columns:\n",
    "    if (X_train[col].dtype == 'object' or \n",
    "        col in ['tipo_edificacion', 'calidade_materiais', \n",
    "                'cor_favorita_propietario', 'acceso_transporte_publico',\n",
    "                'orientacion', 'eficiencia_enerxetica'] or\n",
    "        'tipo_' in col or 'color_' in col):\n",
    "        categorical_features.append(col)\n",
    "\n",
    "print(f\"Categorical features: {categorical_features}\")\n",
    "\n",
    "# Define paths to pretrained models\n",
    "model_paths = {\n",
    "    'catboost': 'results_initial_hyper/best_model_long.cbm',\n",
    "    #'xgboost': 'models/xgboost_model.json',\n",
    "    #'lightgbm': 'models/lightgbm_model.txt',\n",
    "    #'mlp': 'models/mlp_model.pkl'\n",
    "}\n",
    "\n",
    "# Load models\n",
    "models = load_models(model_paths)\n",
    "\n",
    "# Generate level 1 features\n",
    "level1_train, level1_test = generate_level1_features(\n",
    "    models, X_train, X_test, y_train, categorical_features\n",
    ")\n",
    "\n",
    "# Train meta-model\n",
    "meta_model, test_preds = train_meta_model(level1_train, y_train, level1_test)\n",
    "\n",
    "# Create submission file\n",
    "create_submission(test_ids, test_preds, 'submissions_final_stacking.csv')\n",
    "\n",
    "# Save meta-model\n",
    "joblib.dump(meta_model, 'results_stacking/meta_model.pkl')\n",
    "print(\"Meta-model saved to results_stacking/meta_model.pkl\")\n",
    "\n",
    "# End timing\n",
    "end_time = time.time()\n",
    "print(f\"Total execution time: {end_time - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0523ecfdfd03da9535a2cd394fa2b2a2368df119d71b1e2a5e4a2b8711053260"
  },
  "kernelspec": {
   "display_name": "Python 3.7.8 64-bit ('venvP4': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
