{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/netapp2/Store_uni/home/usc/ci/avs/personal/aprendizaje/p4/venvP4/lib/python3.7/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Codificadores guardados en label_encoders.pkl\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import optuna\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Configuración\n",
    "DEBUG = False    # True para runs rápidos de prueba\n",
    "DATA_PATH = 'train.csv'\n",
    "MODEL_DIR = 'models'\n",
    "XGB_MODEL_FILE = 'xgb_large_optuna.json'\n",
    "STACK_MODEL_FILE = 'stacked_model.pkl'\n",
    "ENCODERS_FILE = 'label_encoders.pkl'\n",
    "SUBMIT_FILE = 'submission_stack.csv'\n",
    "TEST_SIZE = 0.1\n",
    "RANDOM_STATE = 42\n",
    "EVAL_METRIC = 'rmse'\n",
    "CV_FOLDS = 5\n",
    "MAX_ROUNDS = 400\n",
    "EARLY_STOPPING_ROUNDS = 100\n",
    "N_TRIALS = 100\n",
    "# DEBUG params\n",
    "if DEBUG:\n",
    "    CV_FOLDS = 2\n",
    "    MAX_ROUNDS = 50\n",
    "    EARLY_STOPPING_ROUNDS = 10\n",
    "    N_TRIALS = 3\n",
    "    print(f\"DEBUG MODE: folds={CV_FOLDS}, rounds={MAX_ROUNDS}, stop_rounds={EARLY_STOPPING_ROUNDS}, trials={N_TRIALS}\")\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "\n",
    "# 1. Carga de datos\n",
    "df = pd.read_csv(DATA_PATH)\n",
    "FEATURES = [c for c in df.columns if c != 'prezo_euros']\n",
    "y = df['prezo_euros']\n",
    "X = df[FEATURES].copy()\n",
    "\n",
    "dtypes = X.dtypes\n",
    "num_cols = dtypes[dtypes.isin([np.float64, np.int64])].index.tolist()\n",
    "cat_cols = dtypes[dtypes == object].index.tolist()\n",
    "\n",
    "# 2. Imputación y codificación\n",
    "for col in num_cols:\n",
    "    X[col].fillna(X[col].median(), inplace=True)\n",
    "label_encoders = {}\n",
    "for col in cat_cols:\n",
    "    le = LabelEncoder()\n",
    "    X[col] = X[col].fillna('Missing').astype(str)\n",
    "    X[col] = le.fit_transform(X[col])\n",
    "    label_encoders[col] = le\n",
    "joblib.dump(label_encoders, os.path.join(MODEL_DIR, ENCODERS_FILE))\n",
    "print(f\"Codificadores guardados en {ENCODERS_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-29 17:50:53,682] A new study created in memory with name: no-name-99316236-58d7-42c3-b97e-e35f56583522\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iniciando optimización de XGBoost...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-29 17:52:38,160] Trial 0 finished with value: 169731.83239288843 and parameters: {'learning_rate': 0.0013292918943162175, 'max_depth': 12, 'subsample': 0.8659969709057025, 'colsample_bytree': 0.7993292420985183, 'lambda': 0.004207988669606638, 'alpha': 0.004207053950287938, 'gamma': 0.2904180608409973, 'min_child_weight': 9}. Best is trial 0 with value: 169731.83239288843.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 0 en 104.5s (avg 104.5s), rem 10343.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-29 17:53:19,523] Trial 1 finished with value: 51029.332600376096 and parameters: {'learning_rate': 0.006358358856676255, 'max_depth': 10, 'subsample': 0.5102922471479012, 'colsample_bytree': 0.9849549260809971, 'lambda': 2.1368329072358767, 'alpha': 0.0070689749506246055, 'gamma': 0.9091248360355031, 'min_child_weight': 2}. Best is trial 1 with value: 51029.332600376096.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 1 en 145.8s (avg 72.9s), rem 7146.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-29 17:53:39,099] Trial 2 finished with value: 206887.3447078906 and parameters: {'learning_rate': 0.0008179499475211679, 'max_depth': 8, 'subsample': 0.7159725093210578, 'colsample_bytree': 0.645614570099021, 'lambda': 0.2801635158716261, 'alpha': 0.003613894271216527, 'gamma': 1.4607232426760908, 'min_child_weight': 4}. Best is trial 1 with value: 51029.332600376096.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 2 en 165.4s (avg 55.1s), rem 5348.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-29 17:54:55,010] Trial 3 finished with value: 122451.14245605687 and parameters: {'learning_rate': 0.0023345864076016252, 'max_depth': 11, 'subsample': 0.5998368910791798, 'colsample_bytree': 0.7571172192068059, 'lambda': 0.23423849847112907, 'alpha': 0.0015339162591163618, 'gamma': 3.0377242595071916, 'min_child_weight': 2}. Best is trial 1 with value: 51029.332600376096.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 3 en 241.3s (avg 60.3s), rem 5791.9s\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 3. Optimización XGBoost con Optuna\n",
    "print(\"Iniciando optimización de XGBoost...\")\n",
    "dtrain_full = xgb.DMatrix(X, label=y)\n",
    "start_time = time.time()\n",
    "def optuna_callback(study, trial):\n",
    "    completed = len([t for t in study.trials if t.state == optuna.trial.TrialState.COMPLETE])\n",
    "    elapsed = time.time() - start_time\n",
    "    avg = elapsed / completed if completed else 0\n",
    "    rem = avg * (N_TRIALS - completed)\n",
    "    print(f\"Trial {trial.number} en {elapsed:.1f}s (avg {avg:.1f}s), rem {rem:.1f}s\")\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "def objective(trial):\n",
    "    \"\"\"\n",
    "    params = {\n",
    "        'tree_method':'gpu_hist','predictor':'gpu_predictor',\n",
    "        'objective':'reg:squarederror','eval_metric':EVAL_METRIC,\n",
    "        'learning_rate': trial.suggest_float('learning_rate',1e-4,1e-1,log=True),\n",
    "        'max_depth': trial.suggest_int('max_depth',4,12),\n",
    "        'subsample': trial.suggest_float('subsample',0.5,1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree',0.5,1.0),\n",
    "        'lambda': trial.suggest_float('lambda',1e-3,10.0,log=True),\n",
    "        'alpha': trial.suggest_float('alpha',1e-3,10.0,log=True),\n",
    "        'gamma': trial.suggest_float('gamma',0.0,5.0),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight',1,10)\n",
    "    }\n",
    "    \n",
    "    \"\"\"\n",
    "    params = {\n",
    "        'tree_method': 'gpu_hist',\n",
    "        'predictor': 'gpu_predictor',\n",
    "        'objective': 'reg:squarederror',\n",
    "        'eval_metric': EVAL_METRIC,\n",
    "        'learning_rate': trial.suggest_loguniform('learning_rate', 1e-4, 1e-1),\n",
    "        'max_depth': trial.suggest_int('max_depth', 4, 20),\n",
    "        'subsample': trial.suggest_uniform('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_uniform('colsample_bytree', 0.5, 1.0),\n",
    "        'lambda': trial.suggest_loguniform('lambda', 1e-3, 100.0),\n",
    "        'alpha': trial.suggest_loguniform('alpha', 1e-3, 100.0),\n",
    "        'gamma': trial.suggest_uniform('gamma', 0.0, 10.0),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 40)\n",
    "    }\n",
    "    cv = xgb.cv(params, dtrain_full, num_boost_round=MAX_ROUNDS,\n",
    "                nfold=CV_FOLDS, early_stopping_rounds=EARLY_STOPPING_ROUNDS,\n",
    "                seed=RANDOM_STATE, verbose_eval=False)\n",
    "    return cv['test-rmse-mean'].min()\n",
    "\n",
    "study = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler(seed=RANDOM_STATE))\n",
    "study.optimize(objective, n_trials=N_TRIALS, callbacks=[optuna_callback])\n",
    "print(f\"XGB optimize completo en {time.time()-start_time:.1f}s\")\n",
    "print(\"Mejores parámetros:\", study.best_params)\n",
    "print(f\"Mejor RMSE CV: {study.best_value:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generando OOF predictions para stacking...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.046009 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2224\n",
      "[LightGBM] [Info] Number of data points in the train set: 10000, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 224830.855200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 1/2 [01:18<01:18, 78.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.042093 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2227\n",
      "[LightGBM] [Info] Number of data points in the train set: 10000, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 221974.773800\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [02:41<00:00, 80.90s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 4. Predicciones out-of-fold para stacking\n",
    "print(\"Generando OOF predictions para stacking...\")\n",
    "# Detectar GPU LightGBM\n",
    "\"\"\"\n",
    "try:\n",
    "    _ = lgb.LGBMRegressor(device='gpu')\n",
    "    gpu_lgb = True\n",
    "except:\n",
    "    gpu_lgb = False\n",
    "    print(\"LightGBM GPU no disponible: usando CPU\")\n",
    "\"\"\"\n",
    "gpu_lgb = False\n",
    "\n",
    "xgb_oof = np.zeros(len(X))\n",
    "lgb_oof = np.zeros(len(X))\n",
    "for fold, (tr_idx, va_idx) in enumerate(tqdm(KFold(n_splits=CV_FOLDS, shuffle=True, random_state=RANDOM_STATE).split(X), total=CV_FOLDS)):\n",
    "    X_tr, X_va = X.iloc[tr_idx], X.iloc[va_idx]\n",
    "    y_tr, y_va = y.iloc[tr_idx], y.iloc[va_idx]\n",
    "    # XGBoost\n",
    "    dtr = xgb.DMatrix(X_tr, label=y_tr)\n",
    "    dva = xgb.DMatrix(X_va, label=y_va)\n",
    "    xgb_params = study.best_params.copy()\n",
    "    xgb_params.update({'tree_method':'gpu_hist','predictor':'gpu_predictor','objective':'reg:squarederror','eval_metric':EVAL_METRIC})\n",
    "    bst = xgb.train(xgb_params, dtr, num_boost_round=MAX_ROUNDS,\n",
    "                    evals=[(dtr,'train'),(dva,'val')], early_stopping_rounds=EARLY_STOPPING_ROUNDS,\n",
    "                    verbose_eval=False)\n",
    "    xgb_oof[va_idx] = bst.predict(dva)\n",
    "    # LightGBM\n",
    "    lgb_params = {'objective':'regression','metric':'rmse','n_estimators':bst.best_iteration}\n",
    "    if gpu_lgb:\n",
    "        lgb_params['device'] = 'gpu'\n",
    "    else:\n",
    "        lgb_params.pop('device', None)\n",
    "    lgbm = lgb.LGBMRegressor(**lgb_params)\n",
    "    #lgbm.fit(X_tr, y_tr, eval_set=[(X_va,y_va)], early_stopping_rounds=EARLY_STOPPING_ROUNDS, verbose=False)\n",
    "    lgbm.fit(X_tr, y_tr, eval_set=[(X_va,y_va)])\n",
    "    lgb_oof[va_idx] = lgbm.predict(X_va)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacking model guardado en stacked_model.pkl\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 5. Stacking meta-model\n",
    "stack_input = np.vstack([xgb_oof, lgb_oof]).T\n",
    "meta = Ridge()\n",
    "meta.fit(stack_input, y)\n",
    "joblib.dump((study.best_params, meta), os.path.join(MODEL_DIR, STACK_MODEL_FILE))\n",
    "print(f\"Stacking model guardado en {STACK_MODEL_FILE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Modelo XGB final guardado en xgb_large_optuna.json\n"
     ]
    }
   ],
   "source": [
    "# 6. Entrenamiento final XGBoost\n",
    "dtr_full = xgb.DMatrix(X, label=y)\n",
    "final_params = study.best_params.copy()\n",
    "final_params.update({'tree_method':'gpu_hist','predictor':'gpu_predictor','objective':'reg:squarederror','eval_metric':EVAL_METRIC})\n",
    "final_bst = xgb.train(final_params, dtr_full, num_boost_round=int(bst.best_iteration))\n",
    "final_bst.save_model(os.path.join(MODEL_DIR, XGB_MODEL_FILE))\n",
    "print(f\"Modelo XGB final guardado en {XGB_MODEL_FILE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 0.041626 seconds.\n",
      "You can set `force_col_wise=true` to remove the overhead.\n",
      "[LightGBM] [Info] Total Bins 2230\n",
      "[LightGBM] [Info] Number of data points in the train set: 20000, number of used features: 19\n",
      "[LightGBM] [Info] Start training from score 223402.814500\n",
      "Submission guardada en submission.csv\n"
     ]
    }
   ],
   "source": [
    "# 7. Submission si test.csv existe\n",
    "if os.path.exists('test.csv'):\n",
    "    df_test = pd.read_csv('test.csv', index_col=0)\n",
    "    X_test = df_test.reindex(columns=FEATURES).copy()\n",
    "    for c in num_cols:\n",
    "        X_test[c].fillna(X[c].median(), inplace=True)\n",
    "    for c,le in label_encoders.items():\n",
    "        X_test[c] = le.transform(X_test[c].fillna('Missing').astype(str))\n",
    "    dtest = xgb.DMatrix(X_test)\n",
    "    xgb_pred = final_bst.predict(dtest)\n",
    "    # LightGBM final fit\n",
    "    lgb_final = lgb.LGBMRegressor(**lgb_params)\n",
    "    lgb_final.fit(X, y)\n",
    "    lgb_pred = lgb_final.predict(X_test)\n",
    "    stacked = np.vstack([xgb_pred, lgb_pred]).T\n",
    "    final_pred = meta.predict(stacked)\n",
    "    submission = pd.DataFrame({'id': df_test['id'], 'prezo_euros': final_pred})\n",
    "    submission.to_csv(os.path.join(MODEL_DIR, SUBMIT_FILE), index=False)\n",
    "    print(f\"Submission guardada en {SUBMIT_FILE}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0523ecfdfd03da9535a2cd394fa2b2a2368df119d71b1e2a5e4a2b8711053260"
  },
  "kernelspec": {
   "display_name": "Python 3.7.8 64-bit ('venvP4': venv)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
