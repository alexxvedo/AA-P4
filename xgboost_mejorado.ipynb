{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-06 17:39:24,136 - __main__ - INFO - Iniciando pipeline de entrenamiento XGBoost\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>superficie_interior_m2</th>\n",
       "      <th>superficie_exterior_m2</th>\n",
       "      <th>numero_habitacions</th>\n",
       "      <th>numero_banos</th>\n",
       "      <th>ano_construccion</th>\n",
       "      <th>lonxitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>temperatura_media_mes_construccion</th>\n",
       "      <th>tipo_edificacion</th>\n",
       "      <th>...</th>\n",
       "      <th>is_outlier</th>\n",
       "      <th>tipo_Apartamento.1</th>\n",
       "      <th>tipo_Casa.1</th>\n",
       "      <th>tipo_Chalet adosado.1</th>\n",
       "      <th>color_Amarelo.1</th>\n",
       "      <th>color_Azul.1</th>\n",
       "      <th>color_Branco.1</th>\n",
       "      <th>color_Negro.1</th>\n",
       "      <th>color_Verde.1</th>\n",
       "      <th>color_Vermello.1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>25521</td>\n",
       "      <td>-1.214878</td>\n",
       "      <td>-0.600905</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1947</td>\n",
       "      <td>-8.17</td>\n",
       "      <td>43.20</td>\n",
       "      <td>24.75</td>\n",
       "      <td>Apartamento</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4843</td>\n",
       "      <td>-0.590517</td>\n",
       "      <td>-0.493874</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1977</td>\n",
       "      <td>-7.23</td>\n",
       "      <td>43.60</td>\n",
       "      <td>14.06</td>\n",
       "      <td>Apartamento</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>27734</td>\n",
       "      <td>1.723563</td>\n",
       "      <td>-0.371180</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1996</td>\n",
       "      <td>-8.40</td>\n",
       "      <td>42.25</td>\n",
       "      <td>12.27</td>\n",
       "      <td>Chalet adosado</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>22142</td>\n",
       "      <td>0.415219</td>\n",
       "      <td>-0.533578</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1996</td>\n",
       "      <td>-6.81</td>\n",
       "      <td>43.15</td>\n",
       "      <td>11.61</td>\n",
       "      <td>Chalet adosado</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14748</td>\n",
       "      <td>1.203970</td>\n",
       "      <td>2.040511</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1990</td>\n",
       "      <td>-8.76</td>\n",
       "      <td>42.92</td>\n",
       "      <td>10.04</td>\n",
       "      <td>Casa</td>\n",
       "      <td>...</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 54 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      id  superficie_interior_m2  superficie_exterior_m2  numero_habitacions  \\\n",
       "0  25521               -1.214878               -0.600905                   1   \n",
       "1   4843               -0.590517               -0.493874                   2   \n",
       "2  27734                1.723563               -0.371180                   1   \n",
       "3  22142                0.415219               -0.533578                   4   \n",
       "4  14748                1.203970                2.040511                   1   \n",
       "\n",
       "   numero_banos  ano_construccion  lonxitude  latitude  \\\n",
       "0             2              1947      -8.17     43.20   \n",
       "1             2              1977      -7.23     43.60   \n",
       "2             1              1996      -8.40     42.25   \n",
       "3             2              1996      -6.81     43.15   \n",
       "4             1              1990      -8.76     42.92   \n",
       "\n",
       "   temperatura_media_mes_construccion tipo_edificacion  ... is_outlier  \\\n",
       "0                               24.75      Apartamento  ...      False   \n",
       "1                               14.06      Apartamento  ...      False   \n",
       "2                               12.27   Chalet adosado  ...      False   \n",
       "3                               11.61   Chalet adosado  ...      False   \n",
       "4                               10.04             Casa  ...      False   \n",
       "\n",
       "  tipo_Apartamento.1  tipo_Casa.1  tipo_Chalet adosado.1  color_Amarelo.1  \\\n",
       "0                  1            0                      0                0   \n",
       "1                  1            0                      0                0   \n",
       "2                  0            0                      1                0   \n",
       "3                  0            0                      1                0   \n",
       "4                  0            1                      0                0   \n",
       "\n",
       "  color_Azul.1 color_Branco.1 color_Negro.1  color_Verde.1  color_Vermello.1  \n",
       "0            1              0             0              0                 0  \n",
       "1            0              0             0              1                 0  \n",
       "2            0              1             0              0                 0  \n",
       "3            0              0             0              0                 1  \n",
       "4            0              0             0              1                 0  \n",
       "\n",
       "[5 rows x 54 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-06 17:39:24,218 - __main__ - INFO - Datos cargados correctamente: 20000 filas, 54 columnas\n",
      "2025-05-06 17:39:24,218 - __main__ - INFO - \n",
      "Resumen estadístico:\n",
      "2025-05-06 17:39:24,218 - __main__ - INFO - Dimensiones: (20000, 54)\n",
      "2025-05-06 17:39:24,226 - __main__ - INFO - Valores faltantes por columna:\n",
      "Series([], dtype: int64)\n",
      "2025-05-06 17:39:24,228 - __main__ - INFO - Columnas numéricas: 45\n",
      "2025-05-06 17:39:24,228 - __main__ - INFO - Columnas categóricas: 6\n",
      "2025-05-06 17:39:24,234 - __main__ - INFO - Iniciando preprocesamiento de datos\n",
      "2025-05-06 17:39:24,327 - __main__ - INFO - Imputador y scaler guardados en models\n",
      "2025-05-06 17:39:24,344 - __main__ - INFO - Codificadores guardados en models\n",
      "2025-05-06 17:39:24,350 - __main__ - INFO - División de datos: 17000 muestras de entrenamiento, 3000 de validación\n",
      "2025-05-06 17:39:24,351 - __main__ - INFO - Iniciando búsqueda de hiperparámetros con Optuna (100 trials)\n",
      "[I 2025-05-06 17:39:24,351] A new study created in memory with name: no-name-444670f3-e997-4506-b8bf-6172961e48a9\n",
      "[I 2025-05-06 17:39:57,669] Trial 0 finished with value: 30522.80868212891 and parameters: {'learning_rate': 0.008468008575248327, 'max_depth': 12, 'min_child_weight': 37, 'subsample': 0.7993292420985183, 'colsample_bytree': 0.5780093202212182, 'lambda': 0.004207053950287938, 'alpha': 0.0017073967431528124, 'gamma': 4.330880728874676, 'grow_policy': 'lossguide', 'max_leaves': 5, 'max_bin': 505}. Best is trial 0 with value: 30522.80868212891.\n",
      "[I 2025-05-06 17:40:00,214] Trial 1 finished with value: 30325.927751367184 and parameters: {'learning_rate': 0.11536162338241392, 'max_depth': 5, 'min_child_weight': 10, 'subsample': 0.5917022549267169, 'colsample_bytree': 0.6521211214797689, 'lambda': 0.12561043700013558, 'alpha': 0.05342937261279776, 'gamma': 1.4561457009902097, 'grow_policy': 'depthwise', 'max_leaves': 75, 'max_bin': 350}. Best is trial 1 with value: 30325.927751367184.\n",
      "[I 2025-05-06 17:41:54,645] Trial 2 finished with value: 30237.64639511719 and parameters: {'learning_rate': 0.013481575603601416, 'max_depth': 10, 'min_child_weight': 10, 'subsample': 0.7571172192068059, 'colsample_bytree': 0.7962072844310213, 'lambda': 0.0015339162591163618, 'alpha': 0.26926469100861794, 'gamma': 0.8526206184364576, 'grow_policy': 'lossguide', 'max_leaves': 248, 'max_bin': 463}. Best is trial 2 with value: 30237.64639511719.\n",
      "[I 2025-05-06 17:42:28,370] Trial 3 finished with value: 30531.649991308594 and parameters: {'learning_rate': 0.0056828375585122656, 'max_depth': 3, 'min_child_weight': 35, 'subsample': 0.7200762468698007, 'colsample_bytree': 0.5610191174223894, 'lambda': 0.09565499215943825, 'alpha': 0.0013726318898045872, 'gamma': 4.546602010393911, 'grow_policy': 'lossguide', 'max_leaves': 80, 'max_bin': 389}. Best is trial 2 with value: 30237.64639511719.\n",
      "[I 2025-05-06 17:42:47,555] Trial 4 finished with value: 30167.09520722656 and parameters: {'learning_rate': 0.02260828676373495, 'max_depth': 4, 'min_child_weight': 49, 'subsample': 0.8875664116805573, 'colsample_bytree': 0.9697494707820946, 'lambda': 3.7958531426706403, 'alpha': 0.24637685958997463, 'gamma': 4.609371175115584, 'grow_policy': 'lossguide', 'max_leaves': 11, 'max_bin': 339}. Best is trial 4 with value: 30167.09520722656.\n",
      "[I 2025-05-06 17:43:30,800] Trial 5 finished with value: 29991.620129296876 and parameters: {'learning_rate': 0.00917911425163946, 'max_depth': 5, 'min_child_weight': 42, 'subsample': 0.6783766633467947, 'colsample_bytree': 0.6404672548436904, 'lambda': 0.14817820606039092, 'alpha': 0.0036618192203924276, 'gamma': 4.010984903770199, 'grow_policy': 'lossguide', 'max_leaves': 198, 'max_bin': 307}. Best is trial 5 with value: 29991.620129296876.\n",
      "[I 2025-05-06 17:44:44,718] Trial 6 finished with value: 32213.449820898437 and parameters: {'learning_rate': 0.0010319982330247674, 'max_depth': 11, 'min_child_weight': 36, 'subsample': 0.8645035840204937, 'colsample_bytree': 0.8856351733429728, 'lambda': 0.0019777828512462727, 'alpha': 0.02715581955282941, 'gamma': 0.5793452976256486, 'grow_policy': 'depthwise', 'max_leaves': 85, 'max_bin': 272}. Best is trial 5 with value: 29991.620129296876.\n",
      "[I 2025-05-06 17:45:19,940] Trial 7 finished with value: 30025.181326367187 and parameters: {'learning_rate': 0.005893060761114622, 'max_depth': 6, 'min_child_weight': 37, 'subsample': 0.8187787356776066, 'colsample_bytree': 0.9436063712881633, 'lambda': 0.0774211647399625, 'alpha': 0.003008686821445846, 'gamma': 3.566223936114975, 'grow_policy': 'depthwise', 'max_leaves': 198, 'max_bin': 382}. Best is trial 5 with value: 29991.620129296876.\n",
      "[I 2025-05-06 17:45:33,761] Trial 8 finished with value: 29951.70249882813 and parameters: {'learning_rate': 0.019718442220616167, 'max_depth': 7, 'min_child_weight': 2, 'subsample': 0.5539457134966522, 'colsample_bytree': 0.5157145928433671, 'lambda': 0.35127047262708466, 'alpha': 0.018089390092767135, 'gamma': 2.542853455823514, 'grow_policy': 'depthwise', 'max_leaves': 105, 'max_bin': 450}. Best is trial 8 with value: 29951.70249882813.\n",
      "[I 2025-05-06 17:46:03,488] Trial 9 finished with value: 30819.25257763672 and parameters: {'learning_rate': 0.0036877442861551015, 'max_depth': 3, 'min_child_weight': 15, 'subsample': 0.5806106436270022, 'colsample_bytree': 0.9648488261712865, 'lambda': 1.7079750342958235, 'alpha': 0.34167643418329696, 'gamma': 4.357302950938589, 'grow_policy': 'depthwise', 'max_leaves': 229, 'max_bin': 394}. Best is trial 8 with value: 29951.70249882813.\n",
      "[I 2025-05-06 17:46:08,620] Trial 10 finished with value: 30823.23664667969 and parameters: {'learning_rate': 0.08419971167624792, 'max_depth': 8, 'min_child_weight': 1, 'subsample': 0.5089809378074097, 'colsample_bytree': 0.5072552314167731, 'lambda': 0.8529494377627208, 'alpha': 6.903717055434683, 'gamma': 2.694232938321996, 'grow_policy': 'depthwise', 'max_leaves': 141, 'max_bin': 445}. Best is trial 8 with value: 29951.70249882813.\n",
      "[I 2025-05-06 17:46:26,287] Trial 11 finished with value: 30110.673426171874 and parameters: {'learning_rate': 0.033846175254018875, 'max_depth': 7, 'min_child_weight': 24, 'subsample': 0.6642242348394745, 'colsample_bytree': 0.6802639230418349, 'lambda': 0.4088812669376903, 'alpha': 0.007542481944789031, 'gamma': 2.527794140583878, 'grow_policy': 'lossguide', 'max_leaves': 161, 'max_bin': 262}. Best is trial 8 with value: 29951.70249882813.\n",
      "[I 2025-05-06 17:46:41,102] Trial 12 finished with value: 30378.56871152344 and parameters: {'learning_rate': 0.05059805262424324, 'max_depth': 8, 'min_child_weight': 49, 'subsample': 0.9828472877209868, 'colsample_bytree': 0.677346428615168, 'lambda': 0.01598519049727414, 'alpha': 0.011701191748027962, 'gamma': 3.3362280523931753, 'grow_policy': 'lossguide', 'max_leaves': 199, 'max_bin': 310}. Best is trial 8 with value: 29951.70249882813.\n",
      "[I 2025-05-06 17:46:43,055] Trial 13 finished with value: 31368.940635351562 and parameters: {'learning_rate': 0.23131829516555288, 'max_depth': 6, 'min_child_weight': 26, 'subsample': 0.635696404804212, 'colsample_bytree': 0.5036317587811662, 'lambda': 8.031094986587314, 'alpha': 0.006945727049204129, 'gamma': 1.671680435920643, 'grow_policy': 'depthwise', 'max_leaves': 115, 'max_bin': 440}. Best is trial 8 with value: 29951.70249882813.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "XGBoost Modeling Pipeline (MAE Optimized)\n",
    "----------------------------------------\n",
    "Script optimizado para entrenamiento de un modelo XGBoost con validación cruzada,\n",
    "búsqueda de hiperparámetros usando Optuna, y generación de predicciones para test.\n",
    "Este script está específicamente optimizado para minimizar el Error Absoluto Medio (MAE).\n",
    "\n",
    "Características:\n",
    "- Optimización enfocada en MAE en lugar de RMSE\n",
    "- Preprocesamiento robusto con manejo inteligente de valores faltantes\n",
    "- Validación cruzada con múltiples métricas de evaluación\n",
    "- Optimización bayesiana de hiperparámetros con Optuna\n",
    "- Análisis de importancia de características\n",
    "- Serialización de todos los componentes del pipeline\n",
    "- Logging detallado\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import time\n",
    "import logging\n",
    "import warnings\n",
    "import json\n",
    "from typing import Dict, List, Tuple, Union, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "import optuna\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split, KFold, StratifiedKFold\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler, RobustScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Configurar logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"xgboost_training.log\"),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# Ignorar warnings específicos\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Configuración\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "CONFIG = {\n",
    "    # Rutas\n",
    "    'data_path': 'train_processed.csv',\n",
    "    'test_path': 'test_processed.csv',\n",
    "    'output_dir': 'models',\n",
    "    \n",
    "    # Nombres de archivos\n",
    "    'model_file': 'xgboost_model.json',\n",
    "    'encoders_file': 'label_encoders.pkl',\n",
    "    'scaler_file': 'scaler.pkl',\n",
    "    'imputer_file': 'imputer.pkl',\n",
    "    'feature_importance_file': 'feature_importance.png',\n",
    "    'submission_file': 'submission_mae.csv',\n",
    "    'config_file': 'model_config.json',\n",
    "    \n",
    "    # Parámetros de división de datos\n",
    "    'test_size': 0.15,          # Aumentado para una validación más robusta\n",
    "    'random_state': 42,\n",
    "    \n",
    "    # Parámetros XGBoost\n",
    "    'eval_metric': 'mae',       # Cambiado a MAE como métrica principal\n",
    "    'cv_folds': 5,\n",
    "    'max_boost_rounds': 3000,   # Aumentado para permitir más iteraciones\n",
    "    'early_stopping_rounds': 50,\n",
    "    \n",
    "    # Optuna\n",
    "    'n_trials': 100,           # Número de combinaciones de hiperparámetros a probar\n",
    "    'timeout': 3600,           # Tiempo máximo en segundos (1 hora)\n",
    "    \n",
    "    # Target y columna ID\n",
    "    'target_column': 'prezo_euros',\n",
    "    'id_column': 'id',\n",
    "    \n",
    "    # Hardware\n",
    "    'use_gpu': False,          # Cambiar a True si tienes GPU disponible\n",
    "}\n",
    "\n",
    "# Crear directorio de salida\n",
    "os.makedirs(CONFIG['output_dir'], exist_ok=True)\n",
    "\n",
    "# Guardar configuración\n",
    "with open(os.path.join(CONFIG['output_dir'], CONFIG['config_file']), 'w') as f:\n",
    "    json.dump(CONFIG, f, indent=4)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Funciones auxiliares\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def load_data(data_path: str) -> pd.DataFrame:\n",
    "    \"\"\"Carga los datos desde un archivo CSV.\"\"\"\n",
    "    try:\n",
    "        df = pd.read_csv(data_path)\n",
    "        df.drop('log_prezo', axis=1, inplace=True)\n",
    "        display(df.head())\n",
    "\n",
    "        logger.info(f\"Datos cargados correctamente: {df.shape[0]} filas, {df.shape[1]} columnas\")\n",
    "        return df\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error al cargar datos: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "def analyze_data(df: pd.DataFrame) -> Tuple[List[str], List[str], List[str]]:\n",
    "    \"\"\"\n",
    "    Analiza el dataframe y clasifica las columnas.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple con listas de columnas numéricas, categóricas y la columna objetivo\n",
    "    \"\"\"\n",
    "    # Detectar automáticamente tipos de columnas\n",
    "    num_cols = df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "    cat_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    \n",
    "    # Remover columna objetivo y ID de las listas\n",
    "    if CONFIG['target_column'] in num_cols:\n",
    "        num_cols.remove(CONFIG['target_column'])\n",
    "    if CONFIG['id_column'] in num_cols:\n",
    "        num_cols.remove(CONFIG['id_column'])\n",
    "    if CONFIG['id_column'] in cat_cols:\n",
    "        cat_cols.remove(CONFIG['id_column'])\n",
    "        \n",
    "    logger.info(f\"Columnas numéricas: {len(num_cols)}\")\n",
    "    logger.info(f\"Columnas categóricas: {len(cat_cols)}\")\n",
    "    \n",
    "    return num_cols, cat_cols, [CONFIG['target_column']]\n",
    "\n",
    "def preprocess_data(df: pd.DataFrame, num_cols: List[str], cat_cols: List[str], \n",
    "                   is_training: bool = True) -> Tuple[pd.DataFrame, Dict, Dict]:\n",
    "    \"\"\"\n",
    "    Preprocesa los datos aplicando imputación, codificación y escalado.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame original\n",
    "        num_cols: Lista de columnas numéricas\n",
    "        cat_cols: Lista de columnas categóricas\n",
    "        is_training: Si es True, entrena los transformadores, si no, usa los guardados\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame preprocesado, diccionarios con transformadores\n",
    "    \"\"\"\n",
    "    X = df.copy()\n",
    "    \n",
    "    # Inicializar diccionarios para almacenar transformadores\n",
    "    label_encoders = {}\n",
    "    \n",
    "    # Procesar columnas numéricas\n",
    "    if is_training:\n",
    "        # Imputar valores faltantes con la mediana\n",
    "        num_imputer = SimpleImputer(strategy='median')\n",
    "        X[num_cols] = num_imputer.fit_transform(X[num_cols])\n",
    "        \n",
    "        # Escalar características numéricas\n",
    "        scaler = RobustScaler()  # Más robusto a outliers que StandardScaler\n",
    "        X[num_cols] = scaler.fit_transform(X[num_cols])\n",
    "        \n",
    "        # Guardar imputer y scaler\n",
    "        joblib.dump(num_imputer, os.path.join(CONFIG['output_dir'], CONFIG['imputer_file']))\n",
    "        joblib.dump(scaler, os.path.join(CONFIG['output_dir'], CONFIG['scaler_file']))\n",
    "        logger.info(f\"Imputador y scaler guardados en {CONFIG['output_dir']}\")\n",
    "    else:\n",
    "        # Cargar imputer y scaler\n",
    "        num_imputer = joblib.load(os.path.join(CONFIG['output_dir'], CONFIG['imputer_file']))\n",
    "        scaler = joblib.load(os.path.join(CONFIG['output_dir'], CONFIG['scaler_file']))\n",
    "        \n",
    "        # Aplicar transformaciones\n",
    "        X[num_cols] = num_imputer.transform(X[num_cols])\n",
    "        X[num_cols] = scaler.transform(X[num_cols])\n",
    "    \n",
    "    # Procesar columnas categóricas\n",
    "    if is_training:\n",
    "        for col in cat_cols:\n",
    "            # Primero rellenar valores faltantes\n",
    "            X[col] = X[col].fillna('Missing').astype(str)\n",
    "            \n",
    "            # Codificar categorías\n",
    "            le = LabelEncoder()\n",
    "            X[col] = le.fit_transform(X[col])\n",
    "            label_encoders[col] = le\n",
    "        \n",
    "        # Guardar encoders\n",
    "        joblib.dump(label_encoders, os.path.join(CONFIG['output_dir'], CONFIG['encoders_file']))\n",
    "        logger.info(f\"Codificadores guardados en {CONFIG['output_dir']}\")\n",
    "    else:\n",
    "        # Cargar encoders\n",
    "        label_encoders = joblib.load(os.path.join(CONFIG['output_dir'], CONFIG['encoders_file']))\n",
    "        \n",
    "        # Aplicar encoders a cada columna categórica\n",
    "        for col in cat_cols:\n",
    "            if col in label_encoders:\n",
    "                # Manejar valores nuevos no vistos durante el entrenamiento\n",
    "                X[col] = X[col].fillna('Missing').astype(str)\n",
    "                # Reemplazar categorías desconocidas con 'Missing'\n",
    "                X[col] = X[col].map(lambda x: x if x in label_encoders[col].classes_ else 'Missing')\n",
    "                X[col] = label_encoders[col].transform(X[col])\n",
    "    \n",
    "    return X, num_imputer, label_encoders\n",
    "\n",
    "def evaluate_model(model, X_val, y_val):\n",
    "    \"\"\"Evalúa el modelo en el conjunto de validación.\"\"\"\n",
    "    dval = xgb.DMatrix(X_val)\n",
    "    preds = model.predict(dval)\n",
    "    \n",
    "    # Calcular múltiples métricas de evaluación\n",
    "    mae = mean_absolute_error(y_val, preds)\n",
    "    rmse = np.sqrt(mean_squared_error(y_val, preds))\n",
    "    r2 = r2_score(y_val, preds)\n",
    "    \n",
    "    # Calcular métricas adicionales específicas para regresión\n",
    "    # Error porcentual absoluto medio (MAPE)\n",
    "    mape = np.mean(np.abs((y_val - preds) / y_val)) * 100\n",
    "    # Error absoluto mediano (MedianAE)\n",
    "    median_ae = np.median(np.abs(y_val - preds))\n",
    "    \n",
    "    logger.info(f\"Evaluación en validación:\")\n",
    "    logger.info(f\"  MAE: {mae:.4f} (métrica principal)\")\n",
    "    logger.info(f\"  RMSE: {rmse:.4f}\")\n",
    "    logger.info(f\"  R²: {r2:.4f}\")\n",
    "    logger.info(f\"  MAPE: {mape:.2f}%\")\n",
    "    logger.info(f\"  MedianAE: {median_ae:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'mae': mae,\n",
    "        'rmse': rmse,\n",
    "        'r2': r2,\n",
    "        'mape': mape,\n",
    "        'median_ae': median_ae\n",
    "    }\n",
    "\n",
    "def plot_feature_importance(model, features: List[str]):\n",
    "    \"\"\"Visualiza la importancia de características.\"\"\"\n",
    "    importance = model.get_score(importance_type='gain')\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': list(importance.keys()),\n",
    "        'Importance': list(importance.values())\n",
    "    })\n",
    "    importance_df = importance_df.sort_values('Importance', ascending=False)\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sns.barplot(x='Importance', y='Feature', data=importance_df.head(20))\n",
    "    plt.title('XGBoost - Top 20 características más importantes (gain)')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Guardar figura\n",
    "    output_path = os.path.join(CONFIG['output_dir'], CONFIG['feature_importance_file'])\n",
    "    plt.savefig(output_path, dpi=300, bbox_inches='tight')\n",
    "    logger.info(f\"Gráfico de importancia guardado en {output_path}\")\n",
    "\n",
    "def objective(trial, X, y, cv_folds=5):\n",
    "    \"\"\"\n",
    "    Función objetivo para Optuna que optimiza los hiperparámetros de XGBoost.\n",
    "    \n",
    "    Args:\n",
    "        trial: Objeto de Optuna para sugerir hiperparámetros\n",
    "        X: Features de entrenamiento\n",
    "        y: Target de entrenamiento\n",
    "        cv_folds: Número de folds para validación cruzada\n",
    "        \n",
    "    Returns:\n",
    "        Métrica de error promedio (MAE) en validación cruzada\n",
    "    \"\"\"\n",
    "    # Espacio de búsqueda de hiperparámetros\n",
    "    params = {\n",
    "        'objective': 'reg:squarederror',  # Mantenemos objetivo cuadrático para el entrenamiento\n",
    "        'eval_metric': CONFIG['eval_metric'],  # MAE para evaluación\n",
    "        \n",
    "        # Hiperparámetros principales\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 1e-3, 0.3, log=True),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 12),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 50),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),\n",
    "        \n",
    "        # Regularización\n",
    "        'lambda': trial.suggest_float('lambda', 1e-3, 10.0, log=True),\n",
    "        'alpha': trial.suggest_float('alpha', 1e-3, 10.0, log=True),\n",
    "        'gamma': trial.suggest_float('gamma', 0.0, 5.0),\n",
    "        \n",
    "        # Configuración adicional\n",
    "        'grow_policy': trial.suggest_categorical('grow_policy', ['depthwise', 'lossguide']),\n",
    "        'max_leaves': trial.suggest_int('max_leaves', 0, 256),\n",
    "        'max_bin': trial.suggest_int('max_bin', 256, 512),\n",
    "    }\n",
    "    \n",
    "    # Si se usa GPU, añadir tree_method y predictor\n",
    "    if CONFIG['use_gpu']:\n",
    "        params.update({\n",
    "            'tree_method': 'gpu_hist',\n",
    "            'predictor': 'gpu_predictor',\n",
    "        })\n",
    "    else:\n",
    "        params.update({\n",
    "            'tree_method': 'hist',\n",
    "        })\n",
    "    \n",
    "    # Definir folds para validación cruzada\n",
    "    # Si el target es continuo, usar KFold\n",
    "    # Si es categórico o discreto, considerar StratifiedKFold\n",
    "    kf = KFold(n_splits=cv_folds, shuffle=True, random_state=CONFIG['random_state'])\n",
    "    \n",
    "    # Lista para almacenar resultados de cada fold\n",
    "    mae_scores = []\n",
    "    \n",
    "    # Realizar validación cruzada manualmente\n",
    "    for train_idx, val_idx in kf.split(X):\n",
    "        X_fold_train, X_fold_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "        y_fold_train, y_fold_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "        \n",
    "        # Convertir a DMatrix\n",
    "        dtrain = xgb.DMatrix(X_fold_train, label=y_fold_train)\n",
    "        dval = xgb.DMatrix(X_fold_val, label=y_fold_val)\n",
    "        \n",
    "        # Entrenar modelo\n",
    "        model = xgb.train(\n",
    "            params,\n",
    "            dtrain,\n",
    "            num_boost_round=CONFIG['max_boost_rounds'],\n",
    "            evals=[(dval, 'val')],\n",
    "            early_stopping_rounds=CONFIG['early_stopping_rounds'],\n",
    "            verbose_eval=False\n",
    "        )\n",
    "        \n",
    "        # Evaluar usando MAE\n",
    "        preds = model.predict(dval)\n",
    "        mae = mean_absolute_error(y_fold_val, preds)\n",
    "        mae_scores.append(mae)\n",
    "    \n",
    "    # Retornar la media de los scores MAE\n",
    "    return np.mean(mae_scores)\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Flujo principal\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "def main():\n",
    "    \"\"\"Función principal que ejecuta todo el pipeline de modelado.\"\"\"\n",
    "    start_time = time.time()\n",
    "    logger.info(\"Iniciando pipeline de entrenamiento XGBoost\")\n",
    "    \n",
    "    # 1. Cargar datos\n",
    "    df = load_data(CONFIG['data_path'])\n",
    "    \n",
    "    # 2. Análisis exploratorio básico\n",
    "    logger.info(\"\\nResumen estadístico:\")\n",
    "    logger.info(f\"Dimensiones: {df.shape}\")\n",
    "    logger.info(f\"Valores faltantes por columna:\\n{df.isnull().sum()[df.isnull().sum() > 0]}\")\n",
    "    \n",
    "    # 3. Separar target y features\n",
    "    num_cols, cat_cols, target_cols = analyze_data(df)\n",
    "    y = df[CONFIG['target_column']].copy()\n",
    "    X = df.drop(columns=[CONFIG['target_column']]).copy()\n",
    "    \n",
    "    if CONFIG['id_column'] in X.columns:\n",
    "        X = X.drop(columns=[CONFIG['id_column']])\n",
    "    \n",
    "    # 4. Preprocesamiento\n",
    "    logger.info(\"Iniciando preprocesamiento de datos\")\n",
    "    X_processed, imputer, label_encoders = preprocess_data(X, num_cols, cat_cols)\n",
    "    \n",
    "    # 5. División train/validation\n",
    "    X_train, X_val, y_train, y_val = train_test_split(\n",
    "        X_processed, y, \n",
    "        test_size=CONFIG['test_size'], \n",
    "        random_state=CONFIG['random_state']\n",
    "    )\n",
    "    \n",
    "    logger.info(f\"División de datos: {X_train.shape[0]} muestras de entrenamiento, {X_val.shape[0]} de validación\")\n",
    "    \n",
    "    # 6. Optimización de hiperparámetros con Optuna\n",
    "    logger.info(f\"Iniciando búsqueda de hiperparámetros con Optuna ({CONFIG['n_trials']} trials)\")\n",
    "    \n",
    "    study = optuna.create_study(\n",
    "        direction='minimize',\n",
    "        sampler=optuna.samplers.TPESampler(seed=CONFIG['random_state'])\n",
    "    )\n",
    "    \n",
    "    study.optimize(\n",
    "        lambda trial: objective(trial, X_processed, y, CONFIG['cv_folds']),\n",
    "        n_trials=CONFIG['n_trials'],\n",
    "        timeout=CONFIG['timeout']\n",
    "    )\n",
    "    \n",
    "    logger.info(\"Mejores hiperparámetros encontrados:\")\n",
    "    for param, value in study.best_params.items():\n",
    "        logger.info(f\"  {param}: {value}\")\n",
    "    logger.info(f\"Mejor RMSE CV: {study.best_value:.4f}\")\n",
    "    \n",
    "    # 7. Entrenamiento del modelo final con los mejores hiperparámetros\n",
    "    logger.info(\"Entrenando modelo final con los mejores hiperparámetros\")\n",
    "    \n",
    "    # Preparar DMatrix\n",
    "    dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "    dval = xgb.DMatrix(X_val, label=y_val)\n",
    "    \n",
    "    # Configurar parámetros finales\n",
    "    final_params = study.best_params.copy()\n",
    "    if CONFIG['use_gpu']:\n",
    "        final_params.update({\n",
    "            'tree_method': 'gpu_hist',\n",
    "            'predictor': 'gpu_predictor',\n",
    "            'objective': 'reg:absoluteerror',  # Cambiado a error absoluto para optimizar directamente MAE\n",
    "            'eval_metric': CONFIG['eval_metric']\n",
    "        })\n",
    "    else:\n",
    "        final_params.update({\n",
    "            'tree_method': 'hist',\n",
    "            'objective': 'reg:absoluteerror',  # Cambiado a error absoluto para optimizar directamente MAE\n",
    "            'eval_metric': CONFIG['eval_metric']\n",
    "        })\n",
    "    \n",
    "    # Entrenar modelo final\n",
    "    final_model = xgb.train(\n",
    "        final_params,\n",
    "        dtrain,\n",
    "        num_boost_round=CONFIG['max_boost_rounds'],\n",
    "        evals=[(dtrain, 'train'), (dval, 'validation')],\n",
    "        early_stopping_rounds=CONFIG['early_stopping_rounds'],\n",
    "        verbose_eval=100\n",
    "    )\n",
    "    \n",
    "    # 8. Evaluar modelo final\n",
    "    metrics = evaluate_model(final_model, X_val, y_val)\n",
    "    \n",
    "    # 9. Visualizar importancia de características\n",
    "    plot_feature_importance(final_model, X.columns.tolist())\n",
    "    \n",
    "    # 10. Guardar modelo final\n",
    "    model_path = os.path.join(CONFIG['output_dir'], CONFIG['model_file'])\n",
    "    final_model.save_model(model_path)\n",
    "    logger.info(f\"Modelo final guardado en {model_path}\")\n",
    "    \n",
    "    # 11. Generar predicciones para test si existe\n",
    "    test_path = CONFIG['test_path']\n",
    "    if os.path.exists(test_path):\n",
    "        logger.info(f\"Generando predicciones para {test_path}\")\n",
    "        df_test = pd.read_csv(test_path)\n",
    "        \n",
    "        # Guardar ID antes de preprocesar\n",
    "        test_ids = df_test[CONFIG['id_column']].copy()\n",
    "        \n",
    "        # Preprocesamiento de test con los mismos transformadores\n",
    "        if CONFIG['id_column'] in df_test.columns:\n",
    "            X_test = df_test.drop(columns=[CONFIG['id_column']])\n",
    "        else:\n",
    "            X_test = df_test.copy()\n",
    "        \n",
    "        # Aplicar el mismo preprocesamiento\n",
    "        X_test_processed, _, _ = preprocess_data(\n",
    "            X_test, num_cols, cat_cols, is_training=False\n",
    "        )\n",
    "        \n",
    "        # Predecir\n",
    "        dtest = xgb.DMatrix(X_test_processed)\n",
    "        test_preds = final_model.predict(dtest)\n",
    "        \n",
    "        # Crear submission\n",
    "        submission = pd.DataFrame({\n",
    "            CONFIG['id_column']: test_ids, \n",
    "            CONFIG['target_column']: test_preds\n",
    "        })\n",
    "        \n",
    "        # Guardar submission\n",
    "        submission_path = os.path.join(CONFIG['output_dir'], CONFIG['submission_file'])\n",
    "        submission.to_csv(submission_path, index=False)\n",
    "        logger.info(f\"Predicciones guardadas en {submission_path}\")\n",
    "    \n",
    "    # 12. Resumen final\n",
    "    elapsed_time = time.time() - start_time\n",
    "    logger.info(f\"Pipeline completado en {elapsed_time/60:.2f} minutos\")\n",
    "    logger.info(f\"MAE final: {metrics['mae']:.4f} (métrica principal)\")\n",
    "    logger.info(f\"RMSE final: {metrics['rmse']:.4f}\")\n",
    "    logger.info(f\"MAPE final: {metrics['mape']:.2f}%\")\n",
    "    logger.info(f\"MedianAE final: {metrics['median_ae']:.4f}\")\n",
    "    logger.info(f\"R² final: {metrics['r2']:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        main()\n",
    "    except Exception as e:\n",
    "        logger.exception(f\"Error en el pipeline: {str(e)}\")\n",
    "        raise"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Practica4",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
